<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="../feed.xsl" type="text/xsl"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">

<channel>
<title>Susam's Mathematics Pages</title>
<link>https://susam.net/tag/mathematics.html</link>
<atom:link rel="self" type="application/rss+xml" href="https://susam.net/tag/mathematics-full.xml"/>
<description>Feed for Susam's Mathematics Pages</description>

<item>
<title>Triangle-Free Cayley Graph</title>
<link>https://susam.net/triangle-free-cayley-graph.html</link>
<guid isPermaLink="false">cgwnt</guid>
<pubDate>Wed, 03 Dec 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  In this note I elaborate the proof of a claim regarding Cayley
  graphs of symmetric groups with transpositions as generators that I
  found in the book <em>Algebraic Graph Theory</em> by Chris Godsil
  and Gordon Royle.  This claim appears as commentary in Section 3.10
  about <em>Transpositions</em>.  Here I present it in the form of a
  theorem along with a complete proof.
</p>
<p>
  <strong>Theorem.</strong>
  <em>
    If \( \mathcal{T} \) is a set of transpositions, then the Cayley
    graph \( X(\operatorname{Sym}(n), \mathcal{T}) \) has no triangles.
  </em>
</p>
<p>
  <em>Proof.</em>  Suppose the vertices \( a, b, c \in
  \operatorname{Sym}(n) \) form a triangle in the Cayley graph \(
  X(\operatorname{Sym}(n), \mathcal{T}).  \)  Since multiplication by
  \( a^{-1} \) is an automorphism of the Cayley graph (by the proof of
  Theorem 3.1.2 that comes earlier), the vertices \( e, ba^{-1},
  ca^{-1} \) form a triangle too.  Let us label them as \( e, b', c'
  \) respectively.
</p>
<p>
  Now by the definition of a Cayley graph, for any two vertices \( a,
  b \in \operatorname{Sym}(n), \) we have

  \begin{align*}
    a \sim b
    &amp; \iff ba^{-1} \in \mathcal{T} \\
    &amp; \iff ba^{-1} = g \\
    &amp; \iff b = ga
  \end{align*}

  for some \( g \in \mathcal{T}.  \)  Therefore

  \begin{align*}
    e \sim b'  &amp; \iff b' = ge = g, \\
    e \sim c'  &amp; \iff c' = he = h, \\
    b' \sim c' &amp; \iff c' = lb'
  \end{align*}

  for some \( g, h, l \in \mathcal{T}.  \)  Note that the last equality gives

  \[
    l =c'b'^{-1} = hg^{-1} = hg \in \mathcal{T}
  \]

  Therefore \( g, h, hg \in \mathcal{T}.  \)  However, this is
  impossible since the product of two transpositions is \( e, \) a \(
  3 \)-cycle or a product of two disjoint transpositions.  For
  example, \( (12)(12) = e, \) \( (12)(13) = (123) \) and \( (12)(24)
  = (12)(24).  \)  Therefore \( hg \) cannot be a transposition,
  i.e. \( hg \notin \mathcal{T}.  \)  This is a contradiction.
  Therefore the vertices \( a, b, c \) cannot form a triangle.  We
  conclude that the Cayley graph \( X(\operatorname{Sym}(n),
  \mathcal{T}) \) has no triangles.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/triangle-free-cayley-graph.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Fizz Buzz with Cosines</title>
<link>https://susam.net/fizz-buzz-with-cosines.html</link>
<guid isPermaLink="false">fzbzz</guid>
<pubDate>Thu, 20 Nov 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Fizz Buzz is a counting game that has become oddly popular in the
  world of computer programming as a simple test of basic programming
  skills.  The rules of the game are straightforward.  Players say the
  numbers aloud in order beginning with one.  Whenever a number is
  divisible by 3, they say 'Fizz' instead.  If it is divisible by 5,
  they say 'Buzz'.  If it is divisible by both 3 and 5, the player
  says both 'Fizz' and 'Buzz'.  Here is a typical Python program that
  prints this sequence:
</p>
<pre><code>for n in range(1, 101):
    if n % 15 == 0:
        print('FizzBuzz')
    elif n % 3 == 0:
        print('Fizz')
    elif n % 5 == 0:
        print('Buzz')
    else:
        print(n)</code>
</pre>
<p>
  Here is the output:
  <a href="files/blog/fizz-buzz.txt">fizz-buzz.txt</a>.  Can we make
  the program more complicated?  The words 'Fizz', 'Buzz' and
  'FizzBuzz' repeat in a periodic manner throughout the sequence.
  What else is periodic?  Trigonometric functions!  Perhaps we can use
  trigonometric functions to encode all four rules of the sequence in
  a single closed-form expression.  That is what we are going to
  explore in this article, for fun and no profit.
</p>
<p>
  By the end, we will obtain a discrete Fourier series that can take
  any integer \( n \) and select the corresponding text to be printed.
  In fact, we will derive it using two different methods.  First, we
  will follow a long-winded but hopefully enjoyable approach that
  relies on a basic understanding of complex exponentiation, geometric
  series and trigonometric functions.  Then, we will obtain the same
  result through a direct application of the discrete Fourier
  transform.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#definitions">Definitions</a>
    <ul>
      <li><a href="#symbol-functions">Symbol Functions</a></li>
      <li><a href="#index-function">Index Function</a></li>
      <li><a href="#fizz-buzz-sequence">Fizz Buzz Sequence</a></li>
    </ul>
  </li>
  <li><a href="#from-indicator-functions-to-cosines">From Indicator Functions to Cosines</a>
    <ul>
      <li><a href="#indicator-functions">Indicator Functions</a></li>
      <li><a href="#complex-exponentials">Complex Exponentials</a></li>
      <li><a href="#cosines">Cosines</a></li>
    </ul>
  </li>
  <li><a href="#dft">Discrete Fourier Transform</a>
    <ul>
      <li><a href="#one-period-of-fizz-buzz">One Period of Fizz Buzz</a></li>
      <li><a href="#fourier-coefficients">Fourier Coefficients</a></li>
      <li><a href="#inverse-transform">Inverse Transform</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="definitions">Definitions<a href="#definitions"></a></h2>
<p>
  Before going any further, we establish a precise mathematical
  definition for the Fizz Buzz sequence.  We begin by introducing a
  few functions that will help us define the Fizz Buzz sequence later.
</p>
<h3 id="symbol-functions">Symbol Functions<a href="#symbol-functions"></a></h3>
<p>
  We define a set of four functions \( \{ s_0, s_1, s_2, s_3 \} \) for
  integers \( n \) by:

  \begin{align*}
    s_0(n) &amp;= n, \\
    s_1(n) &amp;= \mathtt{Fizz}, \\
    s_2(n) &amp;= \mathtt{Buzz}, \\
    s_3(n) &amp;= \mathtt{FizzBuzz}.
  \end{align*}

  We call these the symbol functions because they produce every term
  that appears in the Fizz Buzz sequence.  The symbol function \( s_0
  \) returns \( n \) itself.  The functions \( s_1, \) \( s_2 \) and
  \( s_3 \) are constant functions that always return the literal
  words \( \mathtt{Fizz}, \) \( \mathtt{Buzz} \) and \(
  \mathtt{FizzBuzz} \) respectively, no matter what the value of \( n
  \) is.
</p>
<h3 id="index-function">Index Function<a href="#index-function"></a></h3>
<p>
  We define a function \( f(n) \) for integer \( n \) by

  \[
    f(n) = \begin{cases}
      1 &amp; \text{if } 3 \mid n \text{ and } 5 \nmid n, \\
      2 &amp; \text{if } 3 \nmid n \text{ and } 5 \mid n, \\
      3 &amp; \text{if } 3 \mid n \text{ and } 5 \mid n, \\
      0 &amp; \text{otherwise}.
    \end{cases}
  \]

  The notation \( m \mid n \) means that the integer \( m \) divides
  the integer \( n, \) i.e. \( n \) is a multiple of \( m.  \)
  Equivalently, there exists an integer \( c \) such that \( n = cm
 .  \)  Similarly, \( m \nmid n \) means that \( m \) does not divide
  \( n, \) i.e. \( n \) is not a multiple of \( m.  \)
</p>
<p>
  This function covers all four conditions involved in choosing the \(
  n \)th item of the Fizz Buzz sequence.  As we will soon see, this
  function tells us which of the four symbol functions produces the \(
  n \)th item of the Fizz Buzz sequence.  For this reason, we call \(
  f(n) \) the index function.
</p>
<h3 id="fizz-buzz-sequence">Fizz Buzz Sequence<a href="#fizz-buzz-sequence"></a></h3>
<p>
  We now define the Fizz Buzz sequence as the sequence

  \[
    (s_{f(n)}(n))_{n = 1}^{\infty}
  \]

  We can expand the first few terms of the sequence explicitly as
  follows:

  \begin{align*}
    (s_{f(n)}(n))_{n = 1}^{\infty}
    &amp;= (s_{f(1)}(1), \; s_{f(2)}(2), \; s_{f(3)}(3), \; s_{f(4)}(4), \;
            s_{f(5)}(5), \; s_{f(6)}(6), \; s_{f(7)}(7), \; \dots) \\
    &amp;= (s_0(1), \; s_0(2), \; s_1(3), \; s_0(4),
            s_2(5), \; s_1(6), \; s_0(7), \; \dots) \\
    &amp;= (1, \; 2, \; \mathtt{Fizz}, \; 4, \;
            \mathtt{Buzz}, \; \mathtt{Fizz}, \; 7, \; \dots).
  \end{align*}

  Note how the function \( f(n) \) produces an index \( i \) which we
  then use to select the symbol function \( s_i(n) \) to produce the
  \( n \)th term of the sequence.  This is precisely why we decided to
  call \( f(n) \) the index function while defining it in the previous
  section.
</p>
<h2 id="from-indicator-functions-to-cosines">From Indicator Functions to Cosines<a href="#from-indicator-functions-to-cosines"></a></h2>
<p>
  Here we discuss the first method of deriving our closed form
  expression, starting with indicator functions and rewriting them
  using complex exponentials and cosines.
</p>
<h3 id="indicator-functions">Indicator Functions<a href="#indicator-functions"></a></h3>
<p>
  Here is the index function \( f(n) \) from the previous section with
  its cases and conditions rearranged to make it easier to spot
  interesting patterns:

  \[
    f(n) = \begin{cases}
      0 &amp; \text{if } 5 \nmid n \text{ and } 3 \nmid n, \\
      1 &amp; \text{if } 5 \nmid n \text{ and } 3 \mid n, \\
      2 &amp; \text{if } 5 \mid n \text{ and } 3 \nmid n, \\
      3 &amp; \text{if } 5 \mid n \text{ and } 3 \mid n.
    \end{cases}
  \]

  This function helps us select another function \( s_{f(n)}(n) \)
  which in turn determines the \( n \)th term of the Fizz Buzz
  sequence.  Our goal now is to replace this piecewise formula with a
  single closed-form expression.  To do so, we first define indicator
  functions \( I_m(n) \) as follows:

  \[
    I_m(n) = \begin{cases}
      1 &amp; \text{if } m \mid n, \\
      0 &amp; \text{if } m \nmid n.
    \end{cases}
  \]

  The formula for \( f(n) \) can now be written as:

  \[
    f(n) = \begin{cases}
      0 &amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 0, \\
      1 &amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 1, \\
      2 &amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 0, \\
      3 &amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 1.
    \end{cases}
  \]

  Do you see a pattern?  Here is the same function written as a table:
</p>
<table class="grid center textcenter">
  <tr>
    <th>\( I_5(n) \)</th>
    <th>\( I_3(n) \)</th>
    <th>\( f(n) \)</th>
  </tr>
  <tr>
    <td>\( 0 \)</td>
    <td>\( 0 \)</td>
    <td>\( 0 \)</td>
  </tr>
  <tr>
    <td>\( 0 \)</td>
    <td>\( 1 \)</td>
    <td>\( 1 \)</td>
  </tr>
  <tr>
    <td>\( 1 \)</td>
    <td>\( 0 \)</td>
    <td>\( 2 \)</td>
  </tr>
  <tr>
    <td>\( 1 \)</td>
    <td>\( 1 \)</td>
    <td>\( 3 \)</td>
  </tr>
</table>
<p>
  Do you see it now?  If we treat the values in the first two columns
  as binary digits and the values in the third column as decimal
  numbers, then in each row the first two columns give the binary
  representation of the number in the third column.  For example, \(
  3_{10} = 11_2 \) and indeed in the last row of the table, we see the
  bits \( 1 \) and \( 1 \) in the first two columns and the number \(
  3 \) in the last column.  In other words, writing the binary digits
  \( I_5(n) \) and \( I_3(n) \) side by side gives us the binary
  representation of \( f(n).  \)  Therefore

  \[
    f(n) = 2 \, I_5(n) + I_3(n).
  \]

  We can now write a small program to demonstrate this formula:
</p>
<pre>
<code>for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = (n % 3 == 0) + 2 * (n % 5 == 0)
    print(s[i])</code>
</pre>
<p>
  We can make it even shorter at the cost of some clarity:
</p>
<pre>
<code>for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][(n % 3 == 0) + 2 * (n % 5 == 0)])</code>
</pre>
<p>
  What we have obtained so far is pretty good.  While there is no
  universal definition of a closed-form expression, I think most
  people would agree that the indicator functions as defined above are
  simple enough to be permitted in a closed-form expression.
</p>
<h3 id="complex-exponentials">Complex Exponentials<a href="#complex-exponentials"></a></h3>
<p>
  In the previous section, we obtained the formula

  \[
    f(n) = I_3(n) + 2 \, I_5(n)
  \]

  which we then used as an index to look up the text to be printed.
  We also argued that this is a pretty good closed-form expression
  already.
</p>
<p>
  However, in the interest of making things more complicated, we must
  ask ourselves: What if we are not allowed to use the indicator
  functions?  What if we must adhere to the commonly accepted meaning
  of a closed-form expression which allows only finite combinations of
  basic operations such as addition, subtraction, multiplication,
  division, integer exponents and roots with integer index as well as
  functions such as exponentials, logarithms and trigonometric
  functions.  It turns out that the above formula can be rewritten
  using only addition, multiplication, division and the cosine
  function.  Let us begin the translation.  Consider the sum

  \[
    S_m(n) = \sum_{k = 0}^{m - 1} e^{i 2 \pi k n / m},
  \]

  where \( i \) is the imaginary unit and \( n \) and \( m \) are
  integers.  This is a geometric series in the complex plane with
  ratio \( r = e^{i 2 \pi n / m}.  \)  If \( n \) is a multiple of \( m
 , \) then \( n = cm \) for some integer \( c \) and we get

  \[
    r
    = e^{i 2 \pi n / m}
    = e^{i 2 \pi c}
    = 1.
  \]

  Therefore, when \( n \) is a multiple of \( m, \) we get

  \[
    S_m(n)
    = \sum_{k = 0}^{m - 1} e^{i 2 \pi k n / m}
    = \sum_{k = 0}^{m - 1} 1^k
    = m.
  \]

  If \( n \) is not a multiple of \( m, \) then \( r \ne 1 \) and the
  geometric series becomes

  \[
    S_m(n)
    = \frac{r^m - 1}{r - 1}
    = \frac{e^{i 2 \pi n} - 1}{e^{i 2 \pi n / m} - 1}
    = 0.
  \]

  Therefore,

  \[
    S_m(n) = \begin{cases}
      m &amp; \text{if } m \mid n, \\
      0 &amp; \text{if } m \nmid n.
    \end{cases}
  \]

  Dividing both sides by \( m, \) we get

  \[
    \frac{S_m(n)}{m} = \begin{cases}
      1 &amp; \text{if } m \mid n, \\
      0 &amp; \text{if } m \nmid n.
    \end{cases}
  \]

  But the right-hand side is \( I_m(n).  \)  Therefore

  \[
    I_m(n)
    = \frac{S_m(n)}{m}
    = \frac{1}{m} \sum_{k = 0}^{m - 1} e^{i 2 \pi k n / m}.
  \]
</p>
<h3 id="cosines">Cosines<a href="#cosines"></a></h3>
<p>
  We begin with Euler's formula

  \[
    e^{i x} = \cos x + i \sin x
  \]

  where \( x \) is a real number.  From this formula, we get

  \[
    e^{i x} + e^{-i x} = 2 \cos x.
  \]

  Therefore

  \begin{align*}
    I_3(n)
    &amp;= \frac{1}{3} \sum_{k = 0}^2 e^{i 2 \pi k n / 3} \\
    &amp;= \frac{1}{3} \left( 1 + e^{i 2 \pi n / 3} +
                                  e^{i 4 \pi n / 3} \right) \\
    &amp;= \frac{1}{3} \left( 1 + e^{i 2 \pi n / 3} +
                                  e^{-i 2 \pi n / 3} \right) \\
    &amp;= \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right).
  \end{align*}

  The third equality above follows from the fact that \( e^{i 4 \pi n
  / 3} = e^{i 6 \pi n / 3} e^{-i 2 \pi n / 3} = e^{i 2 \pi n} e^{-i 2
  \pi n/3} = e^{-i 2 \pi n / 3} \) when \( n \) is an integer.
</p>
<p>
  The function above is defined for integer values of \( n \) but we
  can extend its formula to real \( x \) and plot it to observe its
  shape between integers.  As expected, the function takes the value
  \( 1 \) whenever \( x \) is an integer multiple of \( 3 \) and \( 0
  \) whenever \( x \) is an integer not divisible by \( 3.  \)
</p>
<figure class="soft">
  <img src="files/blog/fizz-buzz-i3.png" alt="Graph">
  <figcaption>
    Graph of \( \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi x}{3} \right) \)
  </figcaption>
</figure>
<p>
  Similarly,

  \begin{align*}
    I_5(n)
    &amp;= \frac{1}{5} \sum_{k = 0}^4 e^{i 2 \pi k n / 5} \\
    &amp;= \frac{1}{5} \left( 1 + e^{i 2 \pi n / 5}
                                + e^{i 4 \pi n / 5}
                                + e^{i 6 \pi n / 5}
                                + e^{i 8 \pi n / 5} \right) \\
    &amp;= \frac{1}{5} \left( 1 + e^{i 2 \pi n / 5}
                                + e^{i 4 \pi n / 5}
                                + e^{-i 4 \pi n / 5}
                                + e^{-i 2 \pi n / 5} \right) \\
    &amp;= \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right)
                       + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right).
  \end{align*}

  Extending this expression to real values of \( x \) allows us to
  plot its shape as well.  Once again, the function takes the value \(
  1 \) at integer multiples of \( 5 \) and \( 0 \) at integers not
  divisible by \( 5.  \)
</p>
<figure class="soft">
  <img src="files/blog/fizz-buzz-i5.png" alt="Graph">
  <figcaption>
    Graph of \(
      \frac{1}{5}
      + \frac{2}{5} \cos \left( \frac{2 \pi x}{5} \right)
      + \frac{2}{5} \cos \left( \frac{4 \pi x}{5} \right)
    \)
  </figcaption>
</figure>
<p>
  Recall that we expressed \( f(n) \) as

  \[
    f(n) = I_3(n) + 2 \, I_5(n).
  \]

  Substituting these trigonometric expressions yields

  \[
    f(n)
    = \frac{1}{3}
      + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right)
      + 2 \cdot \left(
        \frac{1}{5}
        + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right)
        + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right)
      \right).
  \]

  A straightforward simplification gives

  \[
    f(n)
    = \frac{11}{15}
      + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right)
      + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right)
      + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right).
  \]

  We can extend this expression to real \( x \) and plot it as well.
  The resulting curve takes the values \( 0, 1, 2 \) and \( 3 \) at
  integer points, as desired.
</p>
<figure class="soft">
  <img src="files/blog/fizz-buzz-f.png" alt="Graph">
  <figcaption>
    Graph of \(
      \frac{11}{15} +
      \frac{2}{3} \cos \left( \frac{2 \pi x}{3} \right) +
      \frac{4}{5} \cos \left( \frac{2 \pi x}{5} \right) +
      \frac{4}{5} \cos \left( \frac{4 \pi x}{5} \right)
    \)
  </figcaption>
</figure>
<p>
  Now we can write our Python program as follows:
</p>
<pre>
<code>from math import cos, pi
for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3)
                      + (4 / 5) * cos(2 * pi * n / 5)
                      + (4 / 5) * cos(4 * pi * n / 5))
    print(s[i])</code>
</pre>
<h2 id="dft">Discrete Fourier Transform<a href="#dft"></a></h2>
<p>
  The keen-eyed might notice that the expression we obtained for \(
  f(n) \) is a discrete Fourier series.  This is not surprising, since
  the output of a Fizz Buzz program depends only on \( n \bmod 15.  \)
  Any function on a finite cyclic group can be written exactly as a
  finite Fourier expansion.  In this section, we obtain \( f(n) \)
  using the discrete Fourier transform.  It is worth mentioning that
  the calculations presented here are quite tedious to do by hand.
  Nevertheless, this section offers a glimpse of how such calculations
  are performed.  By the end, we will arrive at exactly the same \(
  f(n) \) as before.  There is nothing new to discover here.  We
  simply obtain the same result by a more direct but more laborious
  method.  If this doesn't sound interesting, you may safely skip the
  subsections that follow.
</p>
<h3 id="one-period-of-fizz-buzz">One Period of Fizz Buzz<a href="#one-period-of-fizz-buzz"></a></h3>
<div style="display: none">\( \gdef\arraystretch{1.2} \)</div>
<p>
  We know that \( f(n) \) is a periodic function with period \( 15.  \)
  To apply the discrete Fourier transform, we look at one complete
  period of the function using the values \( n = 0, 1, \dots, 14.  \)
  Over this period, we have:

  \begin{array}{c|ccccccccccccccc}
      n &amp;  0 &amp;  1 &amp;  2 &amp;  3 &amp;  4
        &amp;  5 &amp;  6 &amp;  7 &amp;  8 &amp;  9
        &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 \\
    \hline
    f(n) &amp; 3 &amp;  0 &amp;  0 &amp;  1 &amp;  0
         &amp; 2 &amp;  1 &amp;  0 &amp;  0 &amp;  1
         &amp; 2 &amp;  0 &amp;  1 &amp;  0 &amp;  0
  \end{array}

  The discrete Fourier series of \( f(n) \) is

  \[
    f(n) = \sum_{k = 0}^{14} c_k \, e^{i 2 \pi k n / 15}
  \]

  where the Fourier coefficients \( c_k \) are given by the discrete
  Fourier transform

  \[
    c_k = \frac{1}{15} \sum_{n = 0}^{14} f(n) e^{-i 2 \pi k n / 15}.
  \]

  for \( k = 0, 1, \dots, 14.  \)  The formula for \( c_k \) is called
  the discrete Fourier transform (DFT).  The formula for \( f(n) \) is
  called the inverse discrete Fourier transform (IDFT).
</p>
<h3 id="fourier-coefficients">Fourier Coefficients<a href="#fourier-coefficients"></a></h3>
<p>
  Let \( \omega = e^{-i 2 \pi / 15}.  \)  Then using the values of \(
  f(n) \) from the table above, the DFT becomes:

  \[
    c_k = \frac{3 + \omega^{3k} + 2 \omega^{5k} + \omega^{6k}
                  + \omega^{9k} + 2 \omega^{10k} + \omega^{12k}}{15}.
  \]

  Substituting \( k = 0, 1, 2, \dots, 14 \) into the above equation
  gives us the following Fourier coefficients:

  \begin{align*}
    c_{0}  &amp;= \frac{11}{15}, \\
    c_{3}  &amp;= c_{6} = c_{9} = c_{12} = \frac{2}{5}, \\
    c_{5}  &amp;= c_{10} = \frac{1}{3}, \\
    c_{1}  &amp;= c_{2} = c_{4} = c_{7} = c_{8} = c_{11} = c_{13} = c_{14} = 0.
  \end{align*}

  Calculating these Fourier coefficients by hand can be rather
  tedious.  In practice they are almost always calculated using
  numerical software, computer algebra systems or even simple code
  such as the example here:
  <a href="code/fizz-buzz-fourier/fizz-buzz-fourier.py">fizz-buzz-fourier.py</a>.
</p>
<h3 id="inverse-transform">Inverse Transform<a href="#inverse-transform"></a></h3>
<p>
  Once the coefficients are known, we can substitute them into the
  inverse transform introduced earlier to obtain

  \begin{align*}
    f(n)
    &amp;= \sum_{k = 0}^{14} c_k \, e^{i 2 \pi k n / 15} \\[1.5em]
    &amp;= \frac{11}{15}
           + \frac{2}{5} \left(
             e^{i 2 \pi \cdot 3n / 15}
             + e^{i 2 \pi \cdot 6n / 15}
             + e^{i 2 \pi \cdot 9n / 15}
             + e^{i 2 \pi \cdot 12n / 15}
           \right) \\
           &amp; \phantom{=\frac{11}{15}}
           + \frac{1}{3} \left(
             e^{i 2 \pi \cdot 5n / 15}
             + e^{i 2 \pi \cdot 10n / 15}
           \right) \\[1em]
    &amp;= \frac{11}{15}
           + \frac{2}{5} \left(
             e^{i 2 \pi \cdot 3n / 15}
             + e^{i 2 \pi \cdot 6n / 15}
             + e^{-i 2 \pi \cdot 6n / 15}
             + e^{-i 2 \pi \cdot 3n / 15}
           \right) \\
           &amp; \phantom{=\frac{11}{15}}
           + \frac{1}{3} \left(
             e^{i 2 \pi \cdot 5n / 15}
             + e^{-i 2 \pi \cdot 5n / 15}
           \right) \\[1em]
    &amp;= \frac{11}{15}
       + \frac{2}{5} \left(
         2 \cos \left( \frac{2 \pi n}{5} \right)
         + 2 \cos \left( \frac{4 \pi n}{5} \right)
       \right) \\
       &amp; \phantom{=\frac{11}{15}}
       + \frac{1}{3} \left(
         2 \cos \left( \frac{2 \pi n}{3} \right)
       \right) \\[1em]
    &amp;= \frac{11}{15} +
       \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) +
       \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right) +
       \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right).
  \end{align*}

  This is exactly the same expression for \( f(n) \) we obtained in
  the previous section.  We see that the Fizz Buzz index function \(
  f(n) \) can be expressed precisely using the machinery of Fourier
  analysis.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  To summarise, we have defined the Fizz Buzz sequence as

  \[
    (s_{f(n)}(n))_{n = 1}^{\infty}
  \]

  where

  \[
    f(n)
    = \frac{11}{15} +
      \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) +
      \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) +
      \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right).
  \]

  and \( s_0(n) = n, \) \( s_1(n) = \mathtt{Fizz}, \) \( s_2(n) =
  \mathtt{Buzz} \) and \( s_3(n) = \mathtt{FizzBuzz}.  \)  A Python
  program to print the Fizz Buzz sequence based on this definition was
  presented earlier.  That program can be written more succinctly as
  follows:
</p>
<pre>
<code>from math import cos, pi
for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3) + (4 / 5) * (cos(2 * pi * n / 5) + cos(4 * pi * n / 5)))])</code>
</pre>
<p>
  We can also wrap this up nicely in a shell one-liner, in case you
  want to share it with your friends and family and surprise them:
</p>
<pre><code>python3 -c 'from math import cos, pi; [print([n, "Fizz", "Buzz", "FizzBuzz"][round(11/15 + (2/3) * cos(2*pi*n/3) + (4/5) * (cos(2*pi*n/5) + cos(4*pi*n/5)))]) for n in range(1, 101)]'</code></pre>
<p>
  We have taken a simple counting game and turned it into a
  trigonometric construction consisting of a discrete Fourier series
  with three cosine terms and four coefficients.  None of this makes
  Fizz Buzz any easier.  Quite the contrary.  But it does show that
  every \( \mathtt{Fizz} \) and \( \mathtt{Buzz} \) now owes its
  existence to a particular set of Fourier coefficients.  We began
  with the modest goal of making this simple problem more complicated.
  I think it is safe to say that we did not fall short.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/fizz-buzz-with-cosines.html">Read on website</a> |
  <a href="https://susam.net/tag/absurd.html">#absurd</a> |
  <a href="https://susam.net/tag/python.html">#python</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>My Lobsters Interview</title>
<link>https://susam.net/my-lobsters-interview.html</link>
<guid isPermaLink="false">lbstr</guid>
<pubDate>Fri, 12 Sep 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  I recently had an engaging conversation with Alex
  (<a href="https://lobste.rs/~veqq">@veqq</a>) from the
  <a href="https://lobste.rs/">Lobsters</a> community about computing,
  mathematics and a range of related topics.  Our conversation was
  later published on the community website as
  <a href="https://lobste.rs/s/kltoas">Lobsters Interview with
  Susam</a>.
</p>
<p>
  I should mention the sections presented in that post are not in the
  same order in which we originally discussed them.  The sections were
  edited and rearranged by Alex to improve the flow and avoid
  repetition of similar topics too close to each other.
</p>
<p>
  This page preserves a copy of our discussion as edited by Alex, so I
  can keep an archived version on my website.  In my copy, I have
  added a table of contents to make it easier to navigate to specific
  sections.  The interview itself follows the table of contents.  I
  hope you enjoy reading it.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ol>
  <li><a href="#lisp-and-other-things">Lisp and Other Things</a></li>
  <li><a href="#lisp-emacs-and-mathematics">Lisp, Emacs and Mathematics</a></li>
  <li><a href="#interests-and-exploration">Interests and Exploration</a></li>
  <li><a href="#computing-for-fun">Computing for Fun</a></li>
  <li><a href="#computing-activities">Computing Activities</a></li>
  <li><a href="#programming-vs-domains">Programming vs Domains</a></li>
  <li><a href="#old-functionality-and-new-problems">Old Functionality and New Problems</a></li>
  <li><a href="#designing-for-composability">Designing for Composability</a></li>
  <li><a href="#small-vs-large-functions">Small vs Large Functions</a></li>
  <li><a href="#domains-and-projects">Domains and Projects</a></li>
  <li><a href="#double-spacing-and-touch-typing">Double Spacing and Touch Typing</a></li>
  <li><a href="#approach-to-learning">Approach to Learning</a></li>
  <li><a href="#managing-time-and-distractions">Managing Time and Distractions</a></li>
  <li><a href="#blogging">Blogging</a></li>
  <li><a href="#forums">Forums</a></li>
  <li><a href="#mathb-moderation-problems">MathB Moderation Problems</a></li>
  <li><a href="#favourite-mathematics-textbooks">Favourite Mathematics Textbooks</a></li>
  <li><a href="#mathematics-and-computing">Mathematics and Computing</a></li>
</ol>
<h2 id="conversation">Our Conversation<a href="#conversation"></a></h2>
<!-- Lisp and other things -->
<p class="question" id="lisp-and-other-things">
  Hi <a href="https://lobste.rs/~susam">@susam</a>, I primarily know
  you as a Lisper, what other things do you use?
</p>
<p>
  Yes, I use Lisp extensively for my personal projects and much of
  what I do in my leisure is built on it.  I ran
  a <a href="https://github.com/susam/mathb">mathematics pastebin</a>
  for close to thirteen years.  It was quite popular on some IRC
  channels.  The pastebin was written in Common Lisp.
  My <a href="https://susam.net/">personal website</a> and blog are
  generated using a tiny static site generator written in Common Lisp.
  Over the years I have built several other personal tools in it as
  well.
</p>
<p>
  I am an active Emacs Lisp programmer too.  Many of my software tools
  are in fact Emacs Lisp functions that I invoke with convenient key
  sequences.  They help me automate repetitive tasks as well as
  improve my text editing and task management experience.
</p>
<p>
  I use plenty of other tools as well.  In my early adulthood, I spent
  many years working with C, C++, Java and PHP.  My
  <a href="https://issues.apache.org/jira/browse/NUTCH-559">first
  substantial open source contribution</a> was to the Apache Nutch
  project which was in Java and one of my early original open source
  projects was <a href="https://github.com/susam/uncap">Uncap</a>, a C
  program to remap keys on Windows.
</p>
<p>
  These days I use a lot of Python, along with some Go and Rust, but
  Lisp remains important to my personal work.  I also enjoy writing
  small standalone tools directly in HTML and JavaScript, often with
  all the code in a single file in a readable, unminified form.
</p>
<!-- Lisp, Emacs and mathematics -->
<p class="question" id="lisp-emacs-and-mathematics">
  How did you first discover computing, then end up with Lisp, Emacs
  and mathematics?
</p>
<p>
  I got introduced to computers through the Logo programming language
  as a kid.  Using simple arithmetic, geometry, logic and code to
  manipulate a two-dimensional world had a lasting effect on me.
</p>
<p>
  I still vividly remember how I ended up with Lisp.  It was at an
  airport during a long layover in 2007.  I wanted to use the time to
  learn something, so I booted my laptop
  running <a href="https://www.debian.org/">Debian</a> GNU/Linux 4.0
  (Etch) and then started
  <a href="https://www.gnu.org/software/clisp/">GNU CLISP</a> 2.41.
  In those days, Wi-Fi in airports was uncommon.  Smartphones and
  mobile data were also uncommon.  So it was fortunate that I had
  CLISP already installed on my system and my laptop was ready for
  learning Common Lisp.  I had it installed because I had wanted to
  learn Common Lisp for some time.  I was especially attracted by its
  simplicity, by the fact that the entire language can be built up
  from a very small set of special forms.  I
  use <a href="https://www.sbcl.org/">SBCL</a> these days, by the way.
</p>
<p>
  I discovered Emacs through Common Lisp.  Several sources recommended
  using the <a href="https://slime.common-lisp.dev/">Superior Lisp
  Interaction Mode for Emacs (SLIME)</a> for Common Lisp programming,
  so that's where I began.  For many years I continued to use Vim as
  my primary editor, while relying on Emacs and SLIME for Lisp
  development.  Over time, as I learnt more about Emacs itself, I grew
  fond of Emacs Lisp and eventually made Emacs my primary editor and
  computing environment.
</p>
<p>
  I have loved mathematics since my childhood days.  What has always
  fascinated me is how we can prove deep and complex facts using first
  principles and clear logical steps.  That feeling of certainty and
  rigour is unlike anything else.
</p>
<p>
  Over the years, my love for the subject has been rekindled many
  times.  As a specific example, let me share how I got into number
  theory.  One day I decided to learn the RSA cryptosystem.  As I was
  working through the
  <a href="https://people.csail.mit.edu/rivest/Rsapaper.pdf">RSA
  paper</a>, I stumbled upon the Euler totient function
  \( \varphi(n) \) which gives the number of positive integers not
  exceeding n that are relatively prime to n.  The paper first states
  that

  \[
    \varphi(p) = p - 1
  \]

  for prime numbers \( p.  \)  That was obvious since \( p \) has no
  factors other than \( 1 \) and itself, so every integer from \( 1 \)
  up to \( p - 1 \) must be relatively prime to it.  But then it
  presents

  \[
    \varphi(pq) = \varphi(p) \cdot \varphi(q) = (p - 1)(q - 1)
  \]

  for primes \( p \) and \( q.  \)  That was not immediately obvious to
  me back then.  After a few minutes of thinking, I managed to prove
  it from scratch.  By the inclusion-exclusion principle, we count how
  many integers from \( 1 \) up to \( pq \) are not divisible by
  \(p \) or \( q.  \)  There are \( pq \) integers in total.  Among
  them, there are \( q \) integers divisible by \( p \) and \( p \)
  integers divisible by \( q.  \)  So we need to subtract \( p + q \)
  from \(pq.  \)  But since one integer (\( pq \) itself) is counted in
  both groups, we add \( 1 \) back.  Therefore

  \[
    \varphi(pq) = pq - (p + q) + 1 = (p - 1)(q - 1).
  \]

  Next I could also obtain the general formula for \( \varphi(n) \)
  for an arbitrary positive integer \( n \) using the same idea.
  There are several other proofs too, but that is how I derived the
  general formula for \( \varphi(n) \) when I first encountered it.
  And just like that, I had begun to learn number theory!
</p>
<!-- Computing for fun -->
<p class="question" id="computing-for-fun">
  You've said you prefer computing for fun.  What is fun to you?  Do
  you have an idea of what makes something fun or not?
</p>
<p>
  For me, fun in computing began when I first learnt IBM/LCSI PC Logo
  when I was nine years old.  I had very limited access to computers
  back then, perhaps only about two hours per <em>month</em> in the
  computer laboratory at my primary school.  Most of my Logo
  programming happened with pen and paper at home.  I would "test" my
  programs by tracing the results on graph paper.  Eventually I would
  get about thirty minutes of actual computer time in the lab to run
  them for real.
</p>
<p>
  So back then, most of my computing happened without an actual
  computer.  But even with that limited access to computers, a whole
  new world opened up for me: one that showed me the joy of computing
  and more importantly, the joy of sharing my little programs with my
  friends and teachers.  One particular Logo program I still remember
  very well drew a house with animated dashed lines, where the dashes
  moved around the outline of the house.  Everyone around me loved it,
  copied it and tweaked it to change the colours, alter the details
  and add their own little touches.
</p>
<p>
  For me, fun in computing comes from such exploration and sharing.  I
  enjoy asking "what happens if" and then seeing where it leads me.
  My Emacs package
  <a href="https://elpa.nongnu.org/nongnu/devil.html">devil-mode</a>
  comes from such exploration.  It came from asking, "What happens if
  we avoid using the <kbd>ctrl</kbd> and <kbd>meta</kbd> modifier keys
  and use <kbd>,</kbd> (the comma key) or another suitable key as a
  leader key instead?  And can we still have a non-modal editing
  experience?"
</p>
<p>
  Sometimes computing for fun may mean crafting a minimal esoteric
  drawing language, making a small game or building a tool that solves
  an interesting problem elegantly.  It is a bonus if the exploration
  results in something working well enough that I can share with
  others on the World Wide Web and others find it fun too.
</p>
<!-- Pursuits -->
<p class="question" id="interests-and-exploration">
  How do you choose what to investigate?  Which most interest you,
  with what commonalities?
</p>
<p>
  For me, it has always been one exploration leading to another.
</p>
<p>
  For example, I originally built
  <a href="https://github.com/susam/mathb">MathB</a> for my friends
  and myself who were going through a phase in our lives when we used
  to challenge each other with mathematical puzzles.  This tool became
  a nice way to share solutions with each other.  Its use spread from
  my friends to their friends and colleagues, then to schools and
  universities and eventually to IRC channels.
</p>
<p>
  Similarly, I built <a href="https://github.com/susam/texme">TeXMe</a>
  when I was learning neural networks and taking a lot of notes on the
  subject.  I was not ready to share the notes online, but I did want
  to share them with my friends and colleagues who were also learning
  the same topic.  Normally I would write my notes in LaTeX, compile
  them to PDF and share the PDF, but in this case, I wondered, what if
  I took some of the code from MathB and created a tool that would let
  me write plain Markdown
  (<a href="https://github.github.com/gfm/">GFM</a>) + LaTeX
  (<a href="https://www.mathjax.org/">MathJax</a>) in
  a <code>.html</code> file and have the tool render the file as soon
  as it was opened in a web browser?  That resulted in TeXMe, which
  has surprisingly become one of my most popular projects, receiving
  millions of hits in some months according to the CDN statistics.
</p>
<p>
  Another example is <a href="https://susam.github.io/muboard/">Muboard</a>,
  which is a bit like an interactive mathematics chalkboard.  I built
  this when I was hosting an
  <a href="journey-to-prime-number-theorem.html">analytic number
  theory book club</a> and I needed a way to type LaTeX snippets live
  on screen and see them immediately rendered.  That made me wonder:
  what if I took TeXMe, made it interactive and gave it a chalkboard
  look-and-feel?  That led to Muboard.
</p>
<p>
  So we can see that sharing mathematical notes and snippets has been
  a recurring theme in several of my projects.  But that is only a
  small fraction of my interests.  I have a wide variety of interests
  in computing.  I also engage in random explorations, like writing
  IRC clients
  (<a href="https://github.com/susam/nimb">NIMB</a>,
  <a href="https://github.com/susam/tzero">Tzero</a>),
  ray tracing
  (<a href="https://github.com/susam/pov25">POV-Ray</a>,
  <a href="https://github.com/spxy/java-ray-tracing">Java ray tracer</a>),
  writing Emacs guides
  (<a href="https://github.com/susam/emacs4cl">Emacs4CL</a>,
  <a href="https://github.com/susam/emfy">Emfy</a>),
  developing small single-file HTML games
  (<a href="invaders.html">Andromeda Invaders</a>,
  <a href="myrgb.html">Guess My RGB</a>),
  purely recreational programming
  (<a href="fxyt.html">FXYT</a>,
  <a href="https://github.com/susam/may4">may4.fs</a>,
  <a href="self-printing-machine-code.html">self-printing machine code</a>,
  <a href="primegrid.html">prime number grid explorer</a>)
  and so on.  The list goes on.  When it comes to hobby computing, I
  don't think I can pick just one domain and say it interests me the
  most.  I have a lot of interests.
</p>
<!-- What is computing?  -->
<p class="question" id="computing-activities">
  What is computing, to you?
</p>
<p>
  Computing, to me, covers a wide range of activities: programming a
  computer, using a computer, understanding how it works, even
  building one.  For example, I once built a tiny 16-bit CPU along
  with a small main memory that could hold only eight 16-bit
  instructions, using VHDL and a Xilinx CPLD kit.  The design was
  based on the Mano CPU introduced in the book <em>Computer System
  Architecture</em> (3rd ed.) by M. Morris Mano.  It was incredibly
  fun to enter instructions into the main memory, one at a time, by
  pushing DIP switches up and down and then watch the CPU I had built
  execute an entire program.  For someone like me, who usually works
  with software at higher levels of abstraction, that was a thrilling
  experience!
</p>
<p>
  Beyond such experiments, computing also includes more practical and
  concrete activities, such as installing and using my favourite Linux
  distribution (Debian), writing software tools in languages like
  Common Lisp, Emacs Lisp, Python and the shell command language or
  customising my Emacs environment to automate repetitive tasks.
</p>
<p>
  To me, computing also includes the abstract stuff like spending time
  with abstract algebra and number theory and getting a deeper
  understanding of the results pertaining to groups, rings and fields,
  as well as numerous number-theoretic results.  Browsing the
  <a href="https://oeis.org/">On-Line Encyclopedia of Integer
  Sequences</a> (OEIS), writing small programs to explore interesting
  sequences or just thinking about them is computing too.  I think
  many of the interesting results in computer science have deep
  mathematical foundations.  I believe much of computer science is
  really discrete mathematics in action.
</p>
<p>
  And if we dive all the way down from the CPU to the level of
  transistors, we encounter continuous mathematics as well, with
  non-linear voltage-current relationships and analogue behaviour that
  make digital computing possible.  It is fascinating how, as a
  relatively new species on this planet, we have managed to take sand
  and find a way to use continuous voltages and currents in electronic
  circuits built with silicon and convert them into the discrete
  operations of digital logic.  We have machines that can simulate
  themselves!
</p>
<p>
  To me, all of this is fun.  To study and learn about these things,
  to think about them, to understand them better and to accomplish
  useful or amusing results with this knowledge is all part of the
  fun.
</p>
<!-- Programming vs domains -->
<p class="question" id="programming-vs-domains">
  How do you view programming vs. domains?
</p>
<p>
  I focus more on the domain than the tool.  Most of the time it is a
  problem that catches my attention and then I explore it to
  understand the domain and arrive at a solution.  The problem itself
  usually points me to one of the tools I already know.
</p>
<p>
  For example, if it is about working with text files, I might write
  an Emacs Lisp function.  If it involves checking large sets of
  numbers rapidly for patterns, I might choose C++ or Rust.  But if I
  want to share interactive visualisations of those patterns with
  others, I might rewrite the solution in HTML and JavaScript,
  possibly with the use of the Canvas API, so that I can share the
  work as a self-contained file that others can execute easily within
  their web browsers.  When I do that, I prefer to keep the HTML neat
  and readable, rather than bundled or minified, so that people who
  like to 'View Source' can copy, edit and customise the code
  themselves to immediately see their changes take effect.
</p>
<p>
  Let me share a specific example.  While working on a web-based game, I first
  used <code>CanvasRenderingContext2D</code>'s <code>fillText()</code>
  to display text on the game canvas.  However, dissatisfied with the
  text rendering quality, I began looking for IBM PC OEM fonts and
  similar retro fonts online.  After downloading a few font packs, I
  wrote a little Python script to convert them to bitmaps (arrays of
  integers) and then used the bitmaps to draw text on the canvas using
  JavaScript, one cell at a time, to get pixel-perfect results!  These
  tiny Python and JavaScript tools were good enough that I felt
  comfortable sharing them together as a tiny toolkit called
  <a href="https://susam.github.io/pcface/src/demo.html">PCFace</a>.
  This toolkit offers JavaScript bitmap arrays and tiny JavaScript
  rendering functions, so that someone else who wants to display text
  on their game canvas using PC fonts and nothing but plain HTML and
  JavaScript can do so without having to solve the problem from
  scratch!
</p>
<!-- Applicability of old functionality for new problems -->
<p class="question" id="old-functionality-and-new-problems">
  Has the rate of your making new Emacs functions has diminished over
  time (as if everything's covered) or do the widening domains lead to
  more?  I'm curious how applicable old functionality is for new
  problems and how that impacts the APIs!
</p>
<p>
  My rate of making new Emacs functions has definitely decreased.
  There are two reasons.  One is that over the years my computing
  environment has converged into a comfortable, stable setup I am very
  happy with.  The other is that at this stage of life I simply cannot
  afford the time to endlessly tinker with Emacs as I did in my
  younger days.
</p>
<p>
  More generally, when it comes to APIs, I find that well-designed
  functionality tends to remain useful even when new problems appear.
  In Emacs, for example, many of my older functions continue to serve
  me well because they were written in a composable way.  New problems
  can often be solved with small wrappers or combinations of existing
  functions.  I think APIs that consist of functions that are simple,
  orthogonal and flexible age well.  If each function in an API does
  one thing and does it well (the Unix philosophy), it will have
  long-lasting utility.
</p>
<p>
  Of course, new domains and problems do require new functions and
  extensions to an API, but I think it is very important to not give
  in to the temptation of enhancing the existing functions by making
  them more complicated with optional parameters, keyword arguments,
  nested branches and so on.  Personally, I have found that it is much
  better to implement new functions that are small, orthogonal and
  flexible, each doing one thing and doing it well.
</p>
<p class="question" id="designing-for-composability">
  What design methods or tips do you have, to increase composability?
</p>
<p>
  For me, good design starts with good vocabulary.  Clear vocabulary
  makes abstract notions concrete and gives collaborators a shared
  language to work with.  For example, while working on a network
  events database many years ago, we collected data minute by minute
  from network devices.  We decided to call each minute of data from a
  single device a "nugget".  So if we had 15 minutes of data from 10
  devices, that meant 150 nuggets.
</p>
<p>
  Why "nugget"?  Because it was shorter and more convenient than
  repeatedly saying "a minute of data from one device".  Why not
  something less fancy like "chunk"?  Because we reserved "chunk" for
  subdivisions within a nugget.  Perhaps there were better choices,
  but "nugget" was the term we settled on and it quickly became shared
  terminology between the collaborators.  Good terminology naturally
  carries over into code.  With this vocabulary in place, function
  names like <code>collect_nugget()</code>,
  <code>open_nugget()</code>, <code>parse_chunk()</code>,
  <code>index_chunk()</code>, <code>skip_chunk()</code>,
  etc. immediately become meaningful to everyone involved.
</p>
<p>
  Thinking about the vocabulary also ensures that we are thinking
  about the data, concepts and notions we are working with in a
  deliberate manner and that kind of thinking also helps when we
  design the architecture of software.
</p>
<p>
  Too often I see collaborators on software projects jump straight
  into writing functions that take some input and produce some desired
  effect, with variable names and function names decided on the fly.
  To me, this feels backwards.  I prefer the opposite approach.
  Define the terms first and let the code follow from them.
</p>
<p>
  I also prefer developing software in a layered manner, where complex
  functionality is built from simpler, well-named building blocks.  It
  is especially important to avoid <em>layer violations</em>, where
  one complex function invokes another complex function.  That creates
  tight coupling between two complex functions.  If one function
  changes in the future, we have to reason carefully about how it
  affects the other.  Since both are already complex, the cognitive
  burden is high.  A better approach, I think, is to identify the
  common functionality they share and factor that out into smaller,
  simpler functions.
</p>
<p>
  To summarise, I like to develop software with a clear vocabulary,
  consistent use of that vocabulary, a layered design where complex
  functions are built from simpler ones and by avoiding layer
  violations.  I am sure none of this is new to the Lobsters
  community.  Some of these ideas also occur
  in <a href="https://en.wikipedia.org/wiki/Domain-driven_design">domain-driven
  design</a> (DDD).  DDD defines the term <em>ubiquitous language</em>
  to mean, "A language structured around the domain model and used by
  all team members within a bounded context to connect all the
  activities of the team with the software."  If I could call this
  approach of software development something, I would simply call it
  "vocabulary-driven development" (VDD), though of course DDD is the
  more comprehensive concept.
</p>
<p>
  Like I said, none of this is likely new to the Lobsters community.
  In particular, I suspect Forth programmers would find it too
  obvious.  In Forth, it is very difficult to begin with a long,
  poorly thought-out monolithic word and then break it down into
  smaller ones later.  The stack effects quickly become too hard to
  track mentally with that approach.  The only viable way to develop
  software in Forth is to start with a small set of words that
  represent the important notions of the problem domain, test them
  immediately and then compose higher-level words from the lower-level
  ones.  Forth naturally encourages a layered style of development,
  where the programmer thinks carefully about the domain, invents
  vocabulary and expresses complex ideas in terms of simpler ones,
  almost in a mathematical fashion.  In my experience, this kind of
  deliberate design produces software that remains easy to understand
  and reason about even years after it was written.
</p>
<!-- Small vs large functions -->
<p class="question" id="small-vs-large-functions">
  Not enhancing existing functions but adding new small ones seems
  quite lovely, but how do you come back to such a codebase later with
  many tiny functions?  At points, I've advocated for very large
  functions, particularly traumatized by Java-esque 1000 functions in
  1000 files approaches.  When you had time, would you often
  rearchitecture the conceptual space of all of those functions?
</p>
<p>
  The famous quote from Alan J. Perlis comes to mind:
</p>
<blockquote>
  <p>
    It is better to have 100 functions operate on one data structure
    than 10 functions on 10 data structures.
  </p>
</blockquote>
<p>
  Personally, I enjoy working with a codebase that has thousands of
  functions, provided most of them are small, well-scoped and do one
  thing well.  That said, I am not dogmatically opposed to large
  functions.  It is always a matter of taste and judgement.  Sometimes
  one large, cohesive function is clearer than a pile of tiny ones.
</p>
<p>
  For example, when I worked on parser generators, I often found that
  lexers and finite state machines benefited from a single top-level
  function containing the full tokenisation logic or the full state
  transition logic in one place.  That function could call smaller
  helpers for specific tasks, but we still need the overall
  <code>switch</code>-<code>case</code> or
  <code>if</code>-<code>else</code> or <code>cond</code> ladder
  somewhere.  I think trying to split that ladder into smaller
  functions would only make the code harder to follow.
</p>
<p>
  So while I lean towards small, composable functions, the real goal
  is to strike a balance that keeps code maintainable in the long run.
  Each function should be as small as it can reasonably be and no
  smaller.
</p>
<!-- Domains -->
<p class="question" id="domains-and-projects">
  Like you, I program as a tool to explore domains.  Which do you know
  the most about?
</p>
<p>
  For me too, the appeal of computer programming lies especially in
  how it lets me explore different domains.  There are two kinds of
  domains in which I think I have gained good expertise.  The first
  comes from years of developing software for businesses, which has
  included solving problems such as network events parsing, indexing
  and querying, packet decoding, developing parser generators,
  database session management and TLS certificate lifecycle
  management.  The second comes from areas I pursue purely out of
  curiosity or for hobby computing.  This is the kind I am going to
  focus on in our conversation.
</p>
<p>
  Although computing and software are serious business today, for me,
  as for many others, computing is also a hobby.
</p>
<p>
  Personal hobby projects often lead me down various rabbit holes and
  I end up learning new domains along the way.  For example, although
  I am not a web developer, I learnt to build small, interactive
  single-page tools in plain HTML, CSS and JavaScript simply because I
  needed them for my hobby projects over and over again.  An early
  example is <a href="quickqwerty.html">QuickQWERTY</a>, which I built
  to teach myself and my friends touch-typing on QWERTY keyboards.
  Another example is <a href="cfrs.html">CFRS[]</a>, which I created
  because I wanted to make a total (non-Turing complete) drawing
  language that has turtle graphics like Logo but is absolutely
  minimal like P&prime;&prime;.
</p>
<!-- Double spacing -->
<p class="question" id="double-spacing-and-touch-typing">
  You use double spaces after periods which I'd only experienced from
  people who learned touch typing on typewriters, unexpected!
</p>
<p>
  Yes, I do separate sentences by double spaces.  It is interesting
  that you noticed this.
</p>
<p>
  I once briefly learnt touch typing on typewriters as a kid, but
  those lessons did not stick with me.  It was much later, when I used
  a Java applet-based touch typing tutor that I found online about two
  decades ago, that the lessons really stayed with me.  Surprisingly,
  that application taught me to type with a single space between
  sentences.  By the way, I disliked installing Java plugins into the
  web browser, so I wrote <a href="quickqwerty.html">QuickQWERTY</a>
  as a similar touch typing tutor in plain HTML and JavaScript for
  myself and my friends.
</p>
<p>
  I learnt to use double spaces between sentences first with Vim and
  then later again with Emacs.  For example, in Vim,
  the <code>joinspaces</code> option is on by default, so when we join
  sentences with the normal mode command <code>J</code> or format
  paragraphs with <code>gqap</code>, Vim inserts two spaces after full
  stops.  We need to disable that behaviour with <code>:set
  nojoinspaces</code> if we want single spacing.
</p>
<p>
  It is similar in Emacs.  In Emacs, the
  <code>delete-indentation</code> command (<code>M-^</code>) and
  the <code>fill-paragraph</code> command (<code>M-q</code>) both
  insert two spaces between sentences by default.  Single spacing can
  be enabled with <code>(setq sentence-end-double-space nil)</code>.
</p>
<p>
  Incidentally, I spend a good portion of the README for my Emacs
  quick-start DIY kit named
  <a href="https://github.com/susam/emfy">Emfy</a> discussing sentence
  spacing conventions under the section
  <a href="https://github.com/susam/emfy#single-space-for-sentence-spacing">Single
  Space for Sentence Spacing</a>.  There I explain how to configure
  Emacs to use single spaces, although I use double spaces myself.
  That's because many new Emacs users prefer single spacing.
</p>
<p>
  The defaults in Vim and Emacs made me adopt double spacing.  The
  double spacing convention is also widespread across open source
  software.  If we look at the Vim help pages, Emacs built-in
  documentation or the Unix and Linux man pages, double spacing is the
  norm.  Even inline comments in traditional open source projects
  often use it.  For example, see Vim's
  <a href="https://github.com/vim/vim/blob/v9.1.1752/runtime/doc/usr_01.txt">:h usr_01.txt</a>,
  Emacs's
  <a href="https://cgit.git.savannah.gnu.org/cgit/emacs.git/tree/doc/emacs/emacs.texi?h=emacs-30.2#n1556">(info "(emacs) Intro")</a>
  or the comments in the <a href="https://gcc.gnu.org/git/?p=gcc.git;f=gcc/cfg.cc;hb=releases/gcc-15.2.0">GCC source code</a>.
</p>
<!-- Learning -->
<p class="question" id="approach-to-learning">
  How do you approach learning a new domain?
</p>
<p>
  When I take on a new domain, there is of course a lot of reading
  involved from articles, books and documentation.  But as I read, I
  constantly try to test what I learn.  Whenever I see a claim, I ask
  myself, "If this claim were wrong, how could I demonstrate it?"
  Then I design a little experiment, perhaps write a snippet of code
  or run a command or work through a concrete example, with the goal
  of checking the claim in practice.
</p>
<p>
  Now I am not genuinely hoping to prove a claim wrong.  It is just a
  way to engage with the material.  To illustrate, let me share an
  extremely simple and generic example without going into any
  particular domain.  Suppose I learn that Boolean operations in
  Python short-circuit.  I might write out several experimental
  snippets like the following:
</p>
<pre><code class="language-python">def t(): print('t'); return True
def f(): print('f'); return False
f() or t() or f()
</code></pre>
<p>
  And then confirm that the results do indeed confirm short-circuit
  evaluation (<code>f</code> followed by <code>t</code> in this case).
</p>
<p>
  At this point, one could say, "Well, you just confirmed what the
  documentation already told you."  And that's true.  But for me, the
  value lies in trying to test it for myself.  Even if the claim
  holds, the act of checking forces me to see the idea in action.
  That not only reinforces the concept but also helps me build a much
  deeper intuition for it.
</p>
<p>
  Sometimes these experiments also expose gaps in my own
  understanding.  Suppose I didn't properly know what "short-circuit"
  means.  Then the results might contradict my expectations.  That
  contradiction would push me to correct my misconception and that's
  where the real learning happens.
</p>
<p>
  Occasionally, this process even uncovers subtleties I didn't expect.
  For example, while learning socket programming, I discovered that a
  client can successfully receive data using <code>recv()</code> even
  after calling <code>shutdown()</code>, contrary to what I had first
  inferred from the specifications.  See my Stack Overflow post
  <a href="https://stackoverflow.com/q/39698037/303363">Why can recv()
  receive messages after the client has invoked shutdown()?</a> for
  more details if you are curious.
</p>
<p>
  Now this method cannot always be applied, especially if it is very
  expensive or unwieldy to do so.  For example, if I am learning
  something in the finance domain, it is not always possible to
  perform an actual transaction.  One can sometimes use simulation
  software, mock environments or sandbox systems to explore ideas
  safely.  Still, it is worth noting that this method has its
  limitations.
</p>
<p>
  In mathematics, though, I find this method highly effective.  When I
  study a new branch of mathematics, I try to come up with examples
  and counterexamples to test what I am learning.  Often, failing to
  find a counterexample helps me appreciate more deeply why a claim
  holds and why no counterexamples exist.
</p>
<!-- Distraction -->
<p class="question" id="managing-time-and-distractions">
  Do you have trouble not getting distracted with so much on your
  plate?  I'm curious how you balance the time commitments of
  everything!
</p>
<p>
  Indeed, it is very easy to get distracted.  One thing that has
  helped over the years is the increase in responsibilities in other
  areas of my life.  These days I also spend some of my free time
  studying mathematics textbooks.  With growing responsibilities and
  the time I devote to mathematics, I now get at most a few hours each
  week for hobby computing.  This automatically narrows down my
  options.  I can explore perhaps one or at most two ideas in a month
  and that constraint makes me very deliberate about choosing my
  pursuits.
</p>
<p>
  Many of the explorations do not evolve into something solid that I
  can share.  They remain as little experimental code snippets or
  notes archived in a private repository.  But once in a while, an
  exploration grows into something concrete and feels worth sharing on
  the Web.  That becomes a short-term hobby project.  I might work on
  it over a weekend if it is small or for a few weeks if it is more
  complex.  When that happens, the goal of sharing the project helps
  me focus.
</p>
<p>
  I try not to worry too much about making time.  After all, this is
  just a hobby.  Other areas of my life have higher priority.  I also
  want to devote a good portion of my free time to learning more
  mathematics, which is another hobby I am passionate about.  Whatever
  little spare time remains after attending to the higher-priority
  aspects of my life goes into my computing projects, usually a couple
  of hours a week, most of it on weekends.
</p>
<!-- Blogging -->
<p class="question" id="blogging">
  How does blogging mix in?  What's the development like of a single
  piece of curiosity through wrestling with the domain, learning and
  sharing it etc.?
</p>
<p>
  Maintaining my personal website is another aspect of computing that
  I find very enjoyable.  My website began as a loose collection of
  pages on a LAN site during my university days.  Since then I have
  been adding pages to it to write about various topics that I find
  interesting.  It acquired its blog shape and form much later when
  blogging became fashionable.
</p>
<p>
  I usually write a new blog post when I feel like there is some piece
  of knowledge or some exploration that I want to archive in a
  persistent format.  Now what the development of a post looks like
  depends very much on the post.  So let me share two opposite
  examples to describe what the development of a single piece looks
  like.
</p>
<p>
  One of my most frequently visited posts
  is <a href="lisp-in-vim.html">Lisp in Vim</a>.  It started when I
  was hosting a Common Lisp programming club for beginners.  Although
  I have always used Emacs and SLIME for Common Lisp programming
  myself, many in the club used Vim, so I decided to write a short
  guide on setting up something SLIME-like there.  As a former
  long-time Vim user myself, I wanted to make the Lisp journey easier
  for Vim users too.  I thought it would be a 30-minute exercise where
  I write up a README that explains how to install
  <a href="https://github.com/kovisoft/slimv">Slimv</a> and how to set
  it up in Vim.  But then I discovered a newer plugin called
  <a href="https://github.com/vlime/vlime">Vlime</a> that also offered
  SLIME-like features in Vim!  That detail sent me down a very deep
  rabbit hole.  Now I needed to know how the two packages were
  different, what their strengths and weaknesses were, how routine
  operations were performed in both and so on.  What was meant to be a
  short note turned into a nearly 10,000-word article.  As I was
  comparing the two SLIME-like packages for Vim, I also found a few
  bugs in Slimv and contributed fixes for them
  (<a href="https://github.com/kovisoft/slimv/pull/87">#87</a>,
  <a href="https://github.com/kovisoft/slimv/pull/88">#88</a>,
  <a href="https://github.com/kovisoft/slimv/pull/89">#89</a>,
  <a href="https://github.com/kovisoft/slimv/pull/90">#90</a>).
  Writing this blog post turned into a month-long project!
</p>
<p>
  At the opposite extreme is a post like
  <a href="elliptical-python-programming.html">Elliptical
  Python Programming</a>.  I stumbled upon Python's
  <a href="https://docs.python.org/3/library/constants.html#Ellipsis">Ellipsis</a>
  while reviewing someone's code.  It immediately caught my attention.
  I wondered if, combined with some standard obfuscation techniques,
  one could write arbitrary Python programs that looked almost like
  Morse code.  A few minutes of experimentation showed that a
  genuinely Morse code-like appearance was not possible, but something
  close could be achieved.  So I wrote what I hope is a humorous post
  demonstrating that arbitrary Python programs can be written using a
  very restricted set of symbols, one of which is the ellipsis.  It
  took me less than an hour to write this post.  The final result
  doesn't look quite like Morse code as I had imagined, but it is
  quite amusing nevertheless!
</p>
<!-- Forums -->
<p class="question" id="forums">
  What draws you to post and read online forums?  How do you balance
  or allot time for reading technical articles, blogs etc.?
</p>
<p>
  The exchange of ideas!  Just as I enjoy sharing my own
  computing-related thoughts, ideas and projects, I also find joy in
  reading what others have to share.
</p>
<p>
  Other areas of my life take precedence over hobby projects and hobby
  projects take precedence over technical forums.
</p>
<p>
  After I've given time to the higher-priority parts of my life and to
  my own technical explorations, I use whatever spare time remains to
  read articles, follow technical discussions and occasionally add
  comments.
</p>
<!-- MathB.in -->
<p class="question" id="mathb-moderation-problems">
  When you decided to stop with MathB due to moderation burdens, I
  offered to take over/help and you mentioned others had too.  Did
  anyone end up forking it, to your knowledge?
</p>
<p>
  I first thought of shutting down the
  <a href="https://github.com/susam/mathb">MathB</a>-based pastebin
  website in November 2019.  The website had been running for seven
  years at that time.  When I announced my thoughts to the IRC
  communities that would be affected, I received a lot of support and
  encouragement.  A few members even volunteered to help me out with
  moderation.  That support and encouragement kept me going for
  another six years.  However, the volunteers eventually became busy
  with their own lives and moved on.  After all, moderating user
  content for an open pastebin that anyone in the world can post to is
  a thankless and tiring activity.  So most of the moderation activity
  fell back on me.  Finally, in February 2025, I realised that I no
  longer want to spend time on this kind of work.
</p>
<p>
  I developed MathB with a lot of passion for myself and my friends.
  I had no idea at the time that this little project would keep a
  corner of my mind occupied even during weekends and holidays.  There
  was always a nagging worry.  What if someone posted content that
  triggered compliance concerns and my server was taken offline while
  I was away?  I no longer wanted that kind of burden in my life.  So
  I finally decided to shut it down.  I've written more about this
  in <a href="mathbin-is-shutting-down.html">MathB.in Is Shutting
  Down</a>.
</p>
<p>
  To my knowledge, no one has forked it, but others have developed
  alternatives.  Further, the
  <a href="https://wiki.archiveteam.org/">Archive Team</a> has
  <a href="https://web.archive.org/web/*/https://mathb.in/">archived</a>
  all posts from the now-defunct MathB-based website.  A member of the
  Archive Team reached out to me over IRC and we worked together for
  about a week to get everything successfully archived.
</p>
<!-- Textbooks -->
<p class="question" id="favourite-mathematics-textbooks">
  What're your favorite math textbooks?
</p>
<p>
  I have several favourite mathematics books, but let me share three I
  remember especially fondly.
</p>
<p>
  The first is <em>Advanced Engineering Mathematics</em> by Erwin
  Kreyszig.  I don't often see this book recommended online, but for
  me it played a major role in broadening my horizons.  I think I
  studied the 8th edition back in the early 2000s.  It is a hefty book
  with over a thousand pages and I remember reading it cover to cover,
  solving every exercise problem along the way.  It gave me a solid
  foundation in routine areas like differential equations, linear
  algebra, vector calculus and complex analysis.  It also introduced
  me to Fourier transforms and Laplace transforms, which I found
  fascinating.
</p>
<p>
  Of course, the Fourier transform has a wide range of applications in
  signal processing, communications, spectroscopy and more.  But I
  want to focus on the fun and playful part.  In the early 2000s, I
  was also learning to play the piano as a hobby.  I used to record my
  amateur music compositions with
  <a href="https://github.com/audacity/audacity">Audacity</a> by
  connecting my digital piano to my laptop with a line-in cable.  It
  was great fun to plot the spectrum of my music on Audacity, apply
  high-pass and low-pass filters and observe how the Fourier transform
  of the audio changed and then hear the effect on the music.  That
  kind of hands-on tinkering made Fourier analysis intuitive for me
  and I highly recommend it to anyone who enjoys both music and
  mathematics.
</p>
<p>
  The second book is <em>Introduction to Analytic Number Theory</em>
  by Tom M.  Apostol.  As a child I was intrigued by the prime number
  theorem but lacked the mathematical maturity to understand its
  proof.  Years later, as an adult, I finally taught myself the proof
  from Apostol's book.  It was a fantastic journey that began with
  simple concepts like the Mbius function and Dirichlet products and
  ended with quite clever contour integrals that proved the theorem.
  The complex analysis I had learnt from Kreyszig turned out to be
  crucial for understanding those integrals.  Along the way I gained a
  deeper understanding of the Riemann zeta function \( \zeta(s).  \)
  The book discusses zero-free regions where \( \zeta(s) \) does not
  vanish, which I found especially fascinating.  Results like \(
  \zeta(-1) = -1/12, \) which once seemed mysterious, became obvious
  after studying this book.
</p>
<p>
  The third is <em>Galois Theory</em> by Ian Stewart.  It introduced
  me to field extensions, field homomorphisms and solubility by
  radicals.  I had long known that not all quintic equations are
  soluble by radicals, but I didn't know why.  Stewart's book taught
  me exactly why.  In particular, it demonstrated that the polynomial
  \( t^5 - 6t + 3 \) over the field of rational numbers is not soluble
  by radicals.  This particular result, although fascinating, is just
  a small part of a much larger body of work, which is even more
  remarkable.  To arrive at this result, the book takes us through a
  wonderful journey that includes the theory of polynomial rings,
  algebraic and transcendental field extensions, impossibility proofs
  for ruler-and-compass constructions, the Galois correspondence and
  much more.
</p>
<p>
  One of the most rewarding aspects of reading books like these is how
  they open doors to new knowledge, including things I didn't even
  know that I didn't know.
</p>
<!-- Mathematics and computing -->
<p class="question" id="mathematics-and-computing">
  How does the newer math jell with or inform past or present
  computing, compared to much older stuff?
</p>
<p>
  I don't always think explicitly about how mathematics informs
  computing, past or present.  Often the textbooks I pick feel very
  challenging to me, so much so that all my energy goes into simply
  mastering the material.  It is arduous but enjoyable.  I do it
  purely for the fun of learning without worrying about applications.
</p>
<p>
  Of course, a good portion of pure mathematics probably has no
  real-world applications.  As G. H. Hardy famously wrote in <em>A
  Mathematician's Apology</em>:
</p>
<blockquote>
  <p>
    I have never done anything 'useful'.  No discovery of mine has
    made or is likely to make, directly or indirectly, for good or
    ill, the least difference to the amenity of the world.
  </p>
</blockquote>
<p>
  But there is no denying that some of it does find applications.
  Were Hardy alive today, he might be disappointed that number theory,
  his favourite field of "useless" mathematics, is now a crucial part
  of modern cryptography.  Electronic commerce wouldn't likely exist
  without it.
</p>
<p>
  Similarly, it is amusing how something as abstract as abstract
  algebra finds very concrete applications in coding theory.  Concepts
  such as polynomial rings, finite fields and cosets of subspaces in
  vector spaces over finite fields play a crucial role in
  error-correcting codes, without which modern data transmission and
  storage would not be possible.
</p>
<p>
  On a more personal note, some simpler areas of mathematics have been
  directly useful in my own work.  While solving problems for
  businesses, information entropy, combinatorics and probability
  theory were crucial when I worked on gesture-based authentication
  about one and a half decades ago.
</p>
<p>
  Similarly, when I was developing Bloom filter-based indexing and
  querying for a network events database, again, probability theory
  was crucial in determining the parameters of the Bloom filters (such
  as the number of hash functions, bits per filter and elements per
  filter) to ensure that the false positive rate remained below a
  certain threshold.  Subsequent testing with randomly sampled network
  events confirmed that the observed false positive rate matched the
  theoretical estimate quite well.  It was very satisfying to see
  probability theory and the real world agreeing so closely.
</p>
<p>
  Beyond these specific examples, studying mathematics also influences
  the way I think about problems.  Embarking on journeys like analytic
  number theory or Galois theory is humbling.  There are times when I
  struggle to understand a small paragraph of the book and it takes me
  several hours (or even days) to work out the arguments in detail
  with pen and paper (lots of it) before I really grok them.  That
  experience of grappling with dense reasoning teaches humility and
  also makes me sceptical of complex, hand-wavy logic in day-to-day
  programming.
</p>
<p>
  Several times I have seen code that bundles too many decisions into
  one block of logic, where it is not obvious whether it would behave
  correctly in all circumstances.  Explanations may sometimes be
  offered about why it works for reasonable inputs, but the reasoning
  is often not watertight.  The experience of working through
  mathematical proofs, writing my own, making mistakes and then
  correcting them has taught me that if the reasoning for correctness
  is not clear and rigorous, something could be wrong.  In my
  experience, once such code sees real-world usage, a bug is nearly
  always found.
</p>
<p>
  That's why I usually insist either on simplifying the logic or on
  demonstrating correctness in a clear, rigorous way.  Sometimes this
  means doing a case-by-case analysis for different types of inputs or
  conditions and showing that the code behaves correctly in each case.
  There is also a bit of an art to reducing what seem like numerous or
  even infinitely many cases to a small, manageable set of cases by
  spotting structure, such as symmetries, invariants or natural
  partitions of the input space.  Alternatively, one can look for a
  simpler argument that covers all cases.  These are techniques we
  employ routinely in mathematics and I think that kind of thinking
  and reasoning is quite valuable in software development too.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/my-lobsters-interview.html">Read on website</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Prime Number Grid Explorer</title>
<link>https://susam.net/primegrid.html</link>
<guid isPermaLink="false">pghtm</guid>
<pubDate>Tue, 19 Aug 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  A simple single-page HTML application to explore the distribution of
  prime numbers in a grid.  Choose a starting number along with the
  number of rows and columns and the page generates the corresponding
  grid.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/primegrid.html">Read on website</a> |
  <a href="https://susam.net/tag/web.html">#web</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a>
</p>
]]>
</description>
</item>
<item>
<title>Miller-Rabin Speed Test</title>
<link>https://susam.net/code/web/miller-rabin-speed-test.html</link>
<guid isPermaLink="false">mrpst</guid>
<pubDate>Sat, 16 Aug 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  A demo page that implements the Miller-Rabin primality test to
  accurately detect primes for all numbers less than
  318665857834031151167461 and compare its speed against a simple
  division based primality test algorithm.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/code/web/miller-rabin-speed-test.html">Read on website</a> |
  <a href="https://susam.net/tag/web.html">#web</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a>
</p>
]]>
</description>
</item>
<item>
<title>Mutually Attacking Knights</title>
<link>https://susam.net/mutually-attacking-knights.html</link>
<guid isPermaLink="false">makcf</guid>
<pubDate>Mon, 11 Aug 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  How many different ways can we place two identical knights on an \(
  n \times n \) chessboard so that they attack each other?  Can we
  find a closed-form expression that gives this number?  This is the
  problem we explore in this article.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#counting-placements-as-the-board-grows">Counting Placements as the Board Grows</a>
    <ul>
      <li><a href="#type-a-squares">Type A Squares</a></li>
      <li><a href="#type-b-squares">Type B Squares</a></li>
      <li><a href="#type-c-squares">Type C Squares</a></li>
      <li><a href="#type-d-squares">Type D Squares</a></li>
      <li><a href="#closed-form-expression-1">Closed Form Expression</a></li>
    </ul>
  </li>
  <li><a href="#counting-placements-for-each-square">Counting Placements for Each Square</a>
    <ul>
      <li><a href="#attacking-degrees-of-squares">Attacking Degrees of Squares</a></li>
      <li><a href="#from-attacking-degrees-to-counting-placements">From Attacking Degrees to Counting Placements</a></li>
      <li><a href="#closed-form-expression-2">Closed Form Expression</a></li>
    </ul>
  </li>
  <li><a href="#counting-placements-from-minimal-attack-sections">Counting Placements From Minimal Attack Sections</a>
    <ul>
      <li><a href="#minimal-attack-sections">Minimal Attack Sections</a></li>
      <li><a href="#closed-form-expression-3">Closed Form Expression</a></li>
    </ul>
  </li>
  <li><a href="#reference">References</a></li>
</ul>
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  A knight moves two squares in one direction, then one square
  perpendicular to it, forming an L-shaped path.  If a piece occupies
  the destination square, the knight captures it.  If two knights are
  placed such that each can capture the other in a single move, then
  we say the knights attack each other.  We want to determine the
  number of ways to place two identical knights on an \( n \times n \)
  chessboard so that they attack each other.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td class="black knight"></td>
      <td></td>
    </tr>
  </table>
  <figcaption>
    Two knights attacking each other
  </figcaption>
</figure>
<p>
  The above illustration shows just one of several ways two knights
  can attack each other on a \( 3 \times 3 \) board.  There are, in
  fact, a total of eight such placements, shown below.
</p>
<figure style="text-align: center">
  <!-- 1 -->
  <table class="chess odd inline">
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td class="black knight"></td>
      <td></td>
    </tr>
  </table>
  <!-- 2 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td class="black knight"></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <!-- 3 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td class="black knight"></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
  </table>
  <!-- 4 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td class="black knight"></td>
      <td></td>
    </tr>
  </table>
  <!-- 5 -->
  <table class="chess odd inline">
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <!-- 6 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <!-- 7 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
  </table>
  <!-- 8 -->
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <figcaption>
    All \( 8 \) ways two identical knights can attack each other on a
    \( 3 \times 3 \) board.
  </figcaption>
</figure>
<p>
  Let \( f(n) \) denote the number of ways we can place two identical
  knights on an \( n \times n \) chessboard such that they attack each
  other, where \( n \ge 1.  \)
</p>
<p>
  A \( 1 \times 1 \) board has room for only one knight, so we define
  \( f(1) = 0.  \)  On a \( 2 \times 2 \) board, a knight cannot move
  two squares in any direction and therefore cannot attack.
  Therefore, \( f(2) = 0.  \)  To summarise,

  \[
    f(1) = f(2) = 0.
  \]

  From the illustration above, we see that \( f(3) = 8.  \)  We want to
  find a closed-form expression for \( f(n).  \)
</p>
<p>
  We will analyse this problem from various perspectives.  We begin
  with a couple of needlessly complicated approaches, followed by a
  simple and elegant solution.  While I personally enjoy these
  long-winded explorations, if you prefer a more direct solution,
  please skip ahead
  to <a href="#counting-placements-from-minimal-attack-sections">Counting
  Placements From Minimal Attack Sections</a>.
</p>
<p>
  Before we proceed, let us introduce the term <em>mutually attacking
  knight placement</em> to mean a placement of two knights on the
  chessboard such that they attack each other.  Unless stated
  otherwise, the two knights are identical.  This term will serve as a
  convenient shorthand for referring to such placements.
</p>
<h2 id="counting-placements-as-the-board-grows">Counting Placements as the Board Grows<a href="#counting-placements-as-the-board-grows"></a></h2>
<p>
  We now turn to the needlessly complicated solution promised in the
  previous section.  We analyse the <em>new</em> mutually attacking
  knight placements introduced when an existing board is enlarged by
  adding a row and a column.
</p>
<p>
  Let us define

  \[
    \Delta f(n) = f(n) - f(n - 1)
  \]

  for \( n \ge 2, \) so that \( \Delta f(n) \) denotes the new
  mutually attacking knight placements introduced when an \( (n - 1)
  \times (n - 1) \) board is expanded to size \( n \times n \) by
  adding one row and one column.
</p>
<p>
  For brevity, we will avoid restating the process of enlarging an \(
  (n - 1) \times (n - 1) \) board to an \( n \times n \) board by
  adding one row and one column whenever we refer to new placements.
  Instead, we use the term <em>new placements</em> on an
  \( n \times n \) board to refer to \( \Delta f(n).  \)  It is to be
  understood that these new placements are the mutually attacking
  knight placements introduced by enlarging the board from size \( (n
  - 1) \times (n - 1) \) to \( n \times n.  \)
</p>
<p>
  Without loss of generality, suppose the new row and column are added
  to the bottom and right respectively.  We already know that

  \begin{align*}
    \Delta f(2) &amp; = f(2) - f(1) = 0 - 0 = 0, \\
    \Delta f(3) &amp; = f(3) - f(2) = 8 - 0 = 8.  \\
  \end{align*}

  We will now find \( \Delta f(n) \) for \( n \ge 4.  \)  To do this,
  we first categorise the newly added squares due to board expansion,
  into four types, as illustrated below.
</p>
<figure>
  <table class="chess">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">A</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">B</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">C</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">C</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">D</td>
    </tr>
    <tr>
      <td class="em">A</td>
      <td class="em">B</td>
      <td class="em">C</td>
      <td class="em">C</td>
      <td class="em">D</td>
      <td class="em">A</td>
    </tr>
  </table>
  <figcaption>
    New squares, labelled by type, as the board size increases from \(
    5 \times 5 \) to \( 6 \times 6 \)
  </figcaption>
</figure>
<p>
  Here is a brief description of each square type:
</p>
<ul>
  <li>
    Type A squares are the three new corner squares.
  </li>
  <li>
    Type B squares are the two new squares adjacent to type A squares
    at the top and left edges.
  </li>
  <li>
    Type C squares are the new squares that are <em>not</em> adjacent
    to any type A square.  If the new board has dimensions \( n \times
    n, \) where \( n \ge 4, \) then there are exactly \( 2n - 8 \)
    squares of type C.
  </li>
  <li>
    Type D squares are the two new squares adjacent to the
    bottom-right type A square.
  </li>
</ul>
<p>
  We now calculate how many new mutually attacking knight placements
  are introduced by these additional squares as the board expands.  We
  proceed with a case-by-case analysis for each square type.
</p>
<h3 id="type-a-squares">Type A Squares<a href="#type-a-squares"></a></h3>
<p>
  There are three squares of type A.  If we place one knight on a type
  A square, there are two positions for the second knight such that
  the two knights attack each other.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td class="black knight em"></td>
      <td class="em"></td>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
    </tr>
  </table>
  <figcaption>
    Knights on type A squares, with squares attacked by the top knight
    marked with crosses
  </figcaption>
</figure>
<p>
  Since there are three such squares, we get a total of \( 3 \times 2
  = 6 \) new mutually attacking knight placements.
</p>
<h3 id="type-b-squares">Type B Squares<a href="#type-b-squares"></a></h3>
<p>
  There are two squares of type B.  If we place one knight on a type B
  square, there are three positions for the second knight such that
  the two knights attack each other.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td class="em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
      <td class="em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Knights on type B squares, with squares attacked by the top knight
    marked with crosses
  </figcaption>
</figure>
<p>
  Since there are two such squares, we get a total of \( 2 \times 3 =
  6 \) new mutually attacking knight placements.
</p>
<h3 id="type-c-squares">Type C Squares<a href="#type-c-squares"></a></h3>
<p>
  The number of type C squares depends on the board size.  When we
  increase the size of a board from \( (n - 1) \times (n - 1) \) to
  \(n \times n, \) where \( n \ge 4, \) we add \( n^2 - (n - 1)^2 = 2n
  - 1 \) new squares.  Among these, \( 3 \) are of type A, \( 2 \) are
  of type B and \( 2 \) are of type D.  That gives us a total of \(
  7 \) squares of type A, B or D.  The remaining \( 2n - 1 - 7 = 2n -
  8 \) squares are therefore of type C.  Note that when the board size
  increases from \( 3 \times 3 \) to \( 4 \times 4, \) there are \( 2
  \times 4 - 8 = 0 \) squares of type C.
</p>
<figure>
  <table class="chess">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">A</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">B</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">D</td>
    </tr>
    <tr>
      <td class="em">A</td>
      <td class="em">B</td>
      <td class="em">D</td>
      <td class="em">A</td>
    </tr>
  </table>
  <figcaption>
    A \( 4 \times 4 \) board has no type C squares.
  </figcaption>
</figure>
<p>
  However, for a board of size \( 5 \times 5 \) or greater, there is a
  positive number of type C squares since \( 2n - 8 \gt 0 \) if and
  only if \( n \gt 4.  \)
</p>
<figure>
  <table class="chess">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">A</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">B</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">C</td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em">D</td>
    </tr>
    <tr>
      <td class="em">A</td>
      <td class="em">B</td>
      <td class="em">C</td>
      <td class="em">D</td>
      <td class="em">A</td>
    </tr>
  </table>
  <figcaption>
    A \( 5 \times 5 \) board has one type C square.
  </figcaption>
</figure>
<p>
  If we place one knight on a type C square, there are four positions
  for the second knight such that the two knights attack each other.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em">&cross;</td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Knights on type C squares, with squares attacked by the top knight
    marked with crosses
  </figcaption>
</figure>
<p>
  Since there are \( 2n - 8 \) such squares, we get a total of \( 4(2n
  - 8) = 8(n - 4) \) new mutually attacking knight placements.
</p>
<h3 id="type-d-squares">Type D Squares<a href="#type-d-squares"></a></h3>
<p>
  There are two squares of type D.  As with type B squares, placing
  one knight on a type D square yields three positions for the second
  knight such that the two knights attack each other.  This gives \( 2
  \times 3 = 6 \) <em>potentially</em> new placements.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="em">&cross;</td>
      <td class="black knight em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Knights on type D squares, with squares attacked by the top knight
    marked with crosses
  </figcaption>
</figure>
<p>
  However, unlike type B squares, not all of these placements are
  <em>new</em>.  The two placements where one knight is on the right
  edge and the other on the bottom edge were already counted in a
  previous subsection.
</p>
<figure>
  <table class="chess inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <table class="chess inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Placements already counted while analysing placements involving a
    knight on a type B square of the \( 4 \times 4 \) board
  </figcaption>
</figure>
<p>
  For example, when we increase the board size from \( 3 \times 3 \)
  to \( 4 \times 4, \) both the placements described in the previous
  paragraph appear while analysing the placements with a knight on a
  type B square.  More generally, for any board of size
  \( n \times n \) with \( n \ge 5, \) these placements occur while
  analysing the placements with a knight on a type C square.
  Therefore the total number of new mutually attacking knight
  placements is \( 2 \times 3 - 2 = 4.  \)
</p>
<figure>
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Placements already counted while analysing placements involving a
    knight on a type C square of an \( n \times n \) board, where \( n
    \ge 5 \)
  </figcaption>
</figure>
<p>
  Another way to describe this result is to observe that when one
  knight is placed on a type D square, only two positions for the
  second knight yield <em>new</em> mutually attacking knight
  placements.  Since there are two type \( D \) squares, we get a
  total of \( 2 \times 2 = 4 \) new mutually attacking knight
  placements.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td class="em"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td class="black knight em"></td>
    </tr>
    <tr>
      <td class="em"></td>
      <td class="em"></td>
      <td class="em"></td>
      <td class="black knight em"></td>
      <td class="em"></td>
    </tr>
  </table>
  <figcaption>
    Knights on type D squares, with squares attacked by the top knight
    that yield <em>new</em> mutually attacking knight placements
    marked with crosses
  </figcaption>
</figure>
<h3 id="closed-form-expression-1">Closed Form Expression<a href="#closed-form-expression-1"></a></h3>
<p>
  If we add the number of new mutually attacking knight placements
  found in each of the cases above, we get

  \[
    \Delta f(n) = 6 + 6 + 8(n - 4) + 4 = 8(n - 2)
  \]

  new mutually attacking knight placements as the board size increases
  from \( (n - 1) \times (n - 1) \) to \( n \times n, \) where \( n
  \ge 4.  \)  We already know that \( \Delta f(2) = 0 \) and \( \Delta
  f(3) = 8.  \)  Surprisingly, the above formula produces the correct
  values for those cases as well.  Therefore, we can generalise this
  result as

  \[
    \Delta f(n) = 8(n - 2)
  \]

  for all \( n \ge 2.  \)  We can now calculate \( f(n) \) for \( n \ge
  1 \) as follows:

  \begin{align*}
    f(n)
    &amp; = \sum_{k = 1}^n f(k) - \sum_{k = 1}^{n - 1} f(k) \\
    &amp; = \sum_{k = 1}^n f(k) - \sum_{k = 2}^n f(k - 1) \\
    &amp; = f(1) + \sum_{k = 2}^n (f(k) - f(k - 1)) \\
    &amp; = f(1) + \sum_{k = 2}^n \Delta f(k) \\
    &amp; = 0 + \sum_{k = 2}^n 8(k - 2) \\
    &amp; = 8 \sum_{k = 0}^{n - 2} k \\
    &amp; = \frac{8(n - 2)(n - 1)}{2} \\
    &amp; = 4(n - 1)(n - 2).
  \end{align*}

  To summarise, we now have a closed form expression for \( f(n).  \)
  For all \( n \ge 1, \) we have

  \[
    f(n) = 4(n - 1)(n - 2).
  \]
</p>
<h2 id="counting-placements-for-each-square">Counting Placements for Each Square<a href="#counting-placements-for-each-square"></a></h2>
<p>
  The previous section took a long-winded path to arrive at a closed
  form expression for \( f(n).  \)  In this section, we will reach the
  same result that is still a bit drawn out, but not quite as much as
  before.
</p>
<p>
  This time, instead of looking only at the new squares created when
  the board grows, we consider <em>every</em> square on the board.  To
  make the counting easier, we no longer treat the knights as
  identical.  We first work with two distinct knights, count the
  mutually attacking knight placements and then divide the total by \(
  2 \) to get the result for identical knights.
</p>
<h3 id="attacking-degrees-of-squares">Attacking Degrees of Squares<a href="#attacking-degrees-of-squares"></a></h3>
<p>
  Here, we introduce the term <em>attacking degree of a square</em> to
  mean the number of squares a knight can move to from that square in
  a single move.  In other words, the attacking degree of a square is
  the number of squares that would be attacked if a knight were placed
  on it.  For example, the corner squares have an attacking degree of
  \( 2.  \)
</p>
<figure>
  <table class="chess">
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td>&cross;</td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td>&cross;</td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <figcaption>
    The attacking degree of a corner square is \( 2 \) since a knight
    can attack two squares from it
  </figcaption>
</figure>
<p>
  Let us now label each square with its attacking degree.  A \( 1
  \times 1 \) board has only one square of attacking degree \( 0 \)
  since a knight placed on it has nothing to attack.  Similarly, each
  square of a \( 2 \times 2 \) board has attacking degree \( 0 \) too.
</p>
<figure>
  <table class="chess odd inline">
    <tr>
      <td>0</td>
    </tr>
  </table>
  <table class="chess inline">
    <tr>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>0</td>
    </tr>
  </table>
  <figcaption>
    Attacking degrees of all squares are zero on \( 1 \times 1 \) and
    \( 2 \times 2 \) boards
  </figcaption>
</figure>
<p>
  On a \( 3 \times 3 \) board, all squares have attacking degree
  \( 2 \) except the centre square, whose attacking degree is \( 0.  \)
  In other words, placing a knight on any square other than the middle
  one gives exactly two possible positions for the other knight so
  that they attack each other.
</p>
<figure>
  <table class="chess odd">
    <tr>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <td>2</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </table>
  <figcaption>
    Attacking degrees of all squares on a \( 3 \times 3 \) board
  </figcaption>
</figure>
<p>
  With eight such squares, we get \( 8 \times 2 = 16 \) mutually
  attacking knight placements when the two knights are distinct.  If
  we divide this number by \( 2, \) we get \( 8 \) which is indeed the
  number of mutually attacking knight placements on a \( 3 \times 3 \)
  board when the two knights are identical.  This matches the earlier
  result \( f(3) = 8.  \)
</p>
<h3 id="from-attacking-degrees-to-counting-placements">From Attacking Degrees to Counting Placements<a href="#from-attacking-degrees-to-counting-placements"></a></h3>
<p>
  Let \( g(n) \) be the number of mutually attacking knight placements
  on an \( n \times n \) board when the knights are distinct.  Then \(
  g(n) \) is simply the sum of the attacking degrees of all squares on
  the board.  As before, let \( f(n) \) denote the number of mutually
  attacking knight placements on an \( n \times n \) board when the
  two knights are identical.  We will now show that
  \( f(n) = g(n)/2.  \)
</p>
<p>
  Label all squares of the \( n \times n \) board as \( S_1, S_2,
  \dots, S_{n^2} \) in any fixed order.  Label the two distinct
  knights as \( N_1 \) and \( N_2.  \)  We represent each mutually
  attacking knight placement as an ordered pair \( (S_i, S_j) \) if \(
  N_1 \) is on \( S_i \) and \( N_2 \) is on \( S_j, \) with the two
  knights attacking each other.  Here \( 1 \le i, j \le n^2 \) and \(
  i \ne j.  \)
</p>
<p>
  Let \( M \) be the set of all mutually attacking knight placements
  for distinct knights on an \( n \times n \) board.  Then

  \[
    g(n) = \lvert M \rvert.
  \]

  If \( (S_i, S_j) \) is a mutually attacking knight placement of the
  distinct knights \( N_1 \) and \( N_2 \) for some \( i \) and
  \( j \) with \( 1 \le i, j \le n^2 \) and \( i \ne j, \) then \(
  (S_j, S_i) \) is also a mutually attacking knight placement, since
  swapping the positions of the two mutually attacking knights still
  yields a valid mutually attacking placement.  Therefore

  \[
    (S_i, S_j) \in M \iff (S_j, S_i) \in M.
  \]

  Each ordered placement \( (S_i, S_j) \) in \( M \) is thus paired
  with the ordered placement \( (S_j, S_i).  \)  When the knights are
  identical, the two arrangements are indistinguishable and count as
  one placement.  Hence, the number of mutually attacking placements
  for identical knights is exactly half of the number for distinct
  knights, i.e.

  \[
    f(n) = \frac{g(n)}{2}.
  \]

  The next subsection focuses on calculating \( g(n), \) from which \(
  f(n) \) follows immediately by the above formula.
</p>
<h3 id="closed-form-expression-2">Closed Form Expression<a href="#closed-form-expression-2"></a></h3>
<p>
  As noted in the previous section, the number of mutually attacking
  knight placements for two distinct knights on an \( n \times n \)
  board is simply the sum of attacking degrees of all squares on the
  board.  If we label each square as discussed in the previous section
  and use the notation \( \deg(S_i) \) for the attacking degree of the
  square labelled \( S_i, \) where \( 1 \le i \le n^2, \) then

  \[
    g(n) = \sum_{i=1}^{n^2} \deg(S_i).
  \]

  Recall that the attacking degree of a square is the number of
  squares a knight could attack if it were placed there.  Earlier, we
  saw that on a \( 3 \times 3 \) board, all squares except the centre
  one have attacking degree \( 2, \) which gives \( g(3) = 8 \times 2
  = 16 \) and \( f(3) = g(3)/2 = 8.  \)  Let us now write down the
  attacking degrees of all squares on a \( 4 \times 4 \) board.
</p>
<figure>
  <table class="chess">
    <tr>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
    </tr>
  </table>
  <figcaption>
    Attacking degrees of all squares on a \( 4 \times 4 \) board
  </figcaption>
</figure>
<p>
  From the above illustration we get

  \begin{align*}
    g(4) &amp; = 4 \times 2 + 8 \times 3 + 4 \times 4 = 48, \\
    f(4) &amp; = g(4)/2 = 24.
  \end{align*}

  A more general pattern emerges if we consider a larger board, such
  as a \( 6 \times 6 \) board.
</p>
<figure>
  <table class="chess">
    <tr>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>6</td>
      <td>6</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>4</td>
      <td>6</td>
      <td>8</td>
      <td>8</td>
      <td>6</td>
      <td>4</td>
    </tr>
    <tr>
      <td>4</td>
      <td>6</td>
      <td>8</td>
      <td>8</td>
      <td>6</td>
      <td>4</td>
    </tr>
    <tr>
      <td>3</td>
      <td>4</td>
      <td>6</td>
      <td>6</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>3</td>
      <td>2</td>
    </tr>
  </table>
  <figcaption>
    Attacking degrees of all squares on a \( 6 \times 6 \) board
  </figcaption>
</figure>
<p>
  From this illustration, we get

  \begin{align*}
    g(6) &amp; = 4 \times 2 + 8 \times 3 + 12 \times 4 + 8 \times 6 + 4 \times 8 = 160.  \\
    f(6) &amp; = g(6)/2 = 80.
  \end{align*}

  Let us find a general formula now for \( n \ge 4.  \)  We introduce
  one more notation.  Let \( D_k(n) \) denote the sum of the attacking
  degrees of all squares of attacking degree \( k \) on an \( n \times
  n \) board, i.e.

  \[
    D_k(n) = \sum_{\mathclap{\deg(S_i) = k}} \deg(S_i).
  \]

  Since the only attacking degrees the squares can have are \( 2, 3,
  4, 6 \) and \( 8, \) the sum of the attacking degrees of all squares
  can be written as

  \[
    g(n) = D_2(n) + D_3(n) + D_4(n) + D_6(n) + D_8(n).
  \]

  There are exactly four squares of attacking degree \( 2.  \)  These
  are the corner ones.  Therefore,

  \[
    D_2(n) = 4 \times 2 = 8.
  \]

  The eight squares adjacent to the corner squares have attacking
  degree \( 3.  \)  Therefore,

  \[
    D_3(n) = 8 \times 3 = 24.
  \]

  Let us define an <em>inner corner square</em> as one that shares a
  corner with a corner square but not an edge with it.  There are four
  inner corner squares and each has attacking degree \( 4.  \)
  Further, each row and column on the outer edge contains \( n - 4 \)
  additional squares with attacking degree \( 4.  \)  Therefore,

  \[
    D_4(n) = (4 + 4(n - 4))(4) = 16(n - 3).
  \]

  Consider a row or column that contains two inner corner squares of
  attacking degree \( 4.  \)  All \( n - 4 \) squares between the inner
  corner squares have attacking degree \( 6.  \)  There are two such
  rows and two such columns.  Therefore,

  \[
    D_6(n) = 4(n - 4)(6) = 24(n - 4).
  \]

  We have counted the attacking degrees of all squares in the first
  two columns and rows as well as the last two columns and rows.  We
  are left with \( (n - 4)^2 \) squares in the middle and they all
  have attacking degree \( 8.  \)  Therefore,

  \[
    D_8(n) = 8(n - 4)^2.
  \]

  Therefore,

  \begin{align*}
    g(n)
    &amp; = D_2(n) + D_3(n) + D_4(n) + D_6(n) + D_8(n) \\
    &amp; = 8 + 24 + 16(n - 3) + 24(n - 4) + 8(n - 4)^2 \\
    &amp; = 8(n - 1)(n - 2).
  \end{align*}

  Even though we assumed \( n \ge 4 \) while obtaining the above
  formula, remarkably, it gives us the correct values for \( n = 1,
  2 \) and \( 3.  \)  The number of mutually attacking knight
  placements for distinct knights on an \( n \times n \) board is \(
  0 \) if \( n = 1 \) or \( 2.  \)  It is \( 16 \) if \( n = 3.  \)
  Indeed the above formula gives us

  \[
    g(1) = g(2) = 0, \quad g(3) = 16.
  \]

  Therefore, we can now generalise the above result as

  \[
    g(n) = 8(n - 1)(n - 2)
  \]

  for all \( n \ge 1.  \)  Therefore, for all \( n \ge 1, \)

  \[
    f(n) = \frac{g(n)}{2} = 4(n - 1)(n - 2).
  \]
</p>
<h2 id="counting-placements-from-minimal-attack-sections">Counting Placements From Minimal Attack Sections<a href="#counting-placements-from-minimal-attack-sections"></a></h2>
<p>
  Finally, in this section, we take a look at a simple and elegant
  solution that arrives at the closed-form solution in a more direct
  manner.  The analysis begins by looking at the smallest section of
  the board where two knights can attack each other.
</p>
<h3 id="minimal-attack-sections">Minimal Attack Sections<a href="#minimal-attack-sections"></a></h3>
<p>
  Consider a \( 2 \times 3 \) section of a board of size
  \( 3 \times 3 \) or larger.  Such a section has exactly two mutually
  attacking knight placements.
</p>
<figure>
  <table class="chess inline">
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
  </table>
  <table class="chess inline">
    <tr>
      <td></td>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
      <td></td>
    </tr>
  </table>
  <figcaption>
    Two mutually attacking knight placements on a \( 2 \times 3 \)
    section of a board
  </figcaption>
</figure>
<p>
  Similarly, a \( 3 \times 2 \) section of a board also has exactly
  two mutually attacking knight placements.
</p>
<figure>
  <table class="chess odd inline">
    <tr>
      <td class="black knight"></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td></td>
      <td class="black knight"></td>
    </tr>
  </table>
  <table class="chess odd inline">
    <tr>
      <td></td>
      <td class="black knight"></td>
    </tr>
    <tr>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td class="black knight"></td>
      <td></td>
    </tr>
  </table>
  <figcaption>
    Two mutually attacking knight placements on a \( 3 \times 2 \)
    section of a board
  </figcaption>
</figure>
<p>
  We call these \( 2 \times 3 \) and \( 3 \times 2 \) sections
  the <em>minimal attack sections</em> of a board, since no smaller
  section can contain a mutually attacking knight placement.
</p>
<p>
  Two distinct \( 2 \times 3 \) sections can share at most a \( 1
  \times 3 \) section, which is smaller than a minimal attack section.
  Consequently, no mutually attacking knight placement can be common
  to two distinct \( 2 \times 3 \) sections of a board.
</p>
<p>
  Similarly, two distinct \( 3 \times 2 \) sections can share at most
  a \( 3 \times 1 \) section, again too small to contain a minimal
  attack section.  Therefore, they share no mutually attacking knight
  placement.
</p>
<p>
  A \( 2 \times 3 \) section and a \( 3 \times 2 \) section can share
  at most a \( 2 \times 2 \) section, which is still smaller than a
  minimal attack section, so they share no mutually attacking knight
  placement either.
</p>
<p>
  To summarise, any two minimal attack sections of the board yield
  distinct pairs of mutually attacking knight placements.  The total
  number of such placements is therefore exactly twice the number of
  minimal attack sections on the board.
</p>
<h3 id="closed-form-expression-3">Closed Form Expression<a href="#closed-form-expression-3"></a></h3>
<p>
  In an \( n \times n \) board where \( n \ge 3, \) the left edge of a
  \( 2 \times 3 \) section can be placed in any one of the first \( n
  - 2 \) columns of the board.  Similarly, the top edge of such a
  section can be placed in any one of the first \( n - 1 \) rows of
  the board.  Therefore, the total number of distinct \( 2 \times 3 \)
  sections on the board is \( (n - 2)(n - 1).  \)
</p>
<p>
  By similar reasoning, the number of distinct \( 3 \times 2 \)
  sections on an \( n \times n \) board, where \( n \ge 3, \) is also
  \( (n - 1)(n - 2).  \)
</p>
<p>
  Let \( h(n) \) be the total number of minimal attack sections we can
  find on an \( n \times n \) board where \( n \ge 1.  \)  From the
  discussion in the previous two paragraphs, we know that \( h(n) =
  2(n - 1)(n - 2) \) for \( n \ge 3.  \)  Further, this formula for \(
  h(n) \) works for \( n = 1 \) and \( n = 2 \) as well since \( h(1)
  = h(2) = 0 \) and indeed a \( 1 \times 1 \) board or a
  \( 2 \times 2 \) board is too small to contain any minimal attack
  sections.  Therefore, for all \( n \ge 1, \) we get

  \[
    h(n) = 2(n - 1)(n - 2).
  \]

  Since each minimal attack section yields two mutually attacking
  knight placements, the total number of mutually attacking knight
  placements on an \( n \times n \) board is

  \[
    f(n) = 2h(n) = 4(n - 1)(n - 2)
  \]

  for all \( n \ge 1.  \)
</p>
<h2 id="reference">References<a href="#reference"></a></h2>
<ul>
  <li>
    <a href="https://cses.fi/problemset/task/1072">Two Knights</a>
    from the CSES Problem Set
  </li>
  <li>
    <a href="https://mathworld.wolfram.com/KnightGraph.html">Knight Graph</a>
    by Eric W. Weisstein
  </li>
  <li>
    <a href="https://oeis.org/A033996">OEIS Entry A033996</a>
    by N. J. A. Sloane
  </li>
  <li>
    <a href="https://oeis.org/A172132">OEIS Entry A172132</a>
    by Vaclav Kotesovec
  </li>
</ul>
<!-- ### -->
<p>
  <a href="https://susam.net/mutually-attacking-knights.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Zigzag Number Spiral</title>
<link>https://susam.net/zigzag-number-spiral.html</link>
<guid isPermaLink="false">znscf</guid>
<pubDate>Sun, 27 Jul 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<div style="display: none">
  \[
    \gdef\lf{\hspace{-5mm}\leftarrow\hspace{-5mm}}
    \gdef\rt{\hspace{-5mm}\rightarrow\hspace{-5mm}}
    \gdef\up{\uparrow}
    \gdef\dn{\downarrow}
    \gdef\sp{}
    \gdef\cd{\cdots}
    \gdef\vd{\vdots}
    \gdef\dd{\ddots}
    \gdef\arraystretch{1.2}
    \gdef\hl{\small\blacktriangleright}
  \]
</div>
<p>
  Consider the following infinite grid of numbers, where the numbers
  are arranged in a spiral-like manner, but the spiral reverses
  direction each time it reaches the edge of the grid:

  \begin{array}{rcrcrcrcrl}
      1 &amp; \rt &amp;   2 &amp; \sp &amp;   9 &amp; \rt &amp;  10 &amp; \sp &amp;  25 &amp; \cd \\
    \sp &amp; \sp &amp; \dn &amp; \sp &amp; \up &amp; \sp &amp; \dn &amp; \sp &amp; \up &amp; \sp \\
      4 &amp; \lf &amp;   3 &amp; \sp &amp;   8 &amp; \sp &amp;  11 &amp; \sp &amp;  24 &amp; \cd \\
    \dn &amp; \sp &amp; \sp &amp; \sp &amp; \up &amp; \sp &amp; \dn &amp; \sp &amp; \up &amp; \sp \\
      5 &amp; \rt &amp;   6 &amp; \rt &amp;   7 &amp; \sp &amp;  12 &amp; \sp &amp;  23 &amp; \cd \\
    \sp &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \dn &amp; \sp &amp; \up &amp; \sp \\
     16 &amp; \lf &amp;  15 &amp; \lf &amp;  14 &amp; \lf &amp;  13 &amp; \sp &amp;  22 &amp; \cd \\
    \dn &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \sp &amp; \up &amp; \sp \\
     17 &amp; \rt &amp;  18 &amp; \rt &amp;  19 &amp; \rt &amp;  20 &amp; \rt &amp;  21 &amp; \cd \\
    \vd &amp; \sp &amp; \vd &amp; \sp &amp; \vd &amp; \sp &amp; \vd &amp; \sp &amp; \vd &amp; \dd
  \end{array}

  Can we find a closed-form expression that tells us the number at the
  \( m \)th row and \( n \)th column?
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#patterns-on-the-edges">Patterns on the Edges</a>
    <ul>
      <li><a href="#computing-edge-numbers">Computing Edge Numbers</a></li>
      <li><a href="#computing-all-grid-numbers-1">Computing All Grid Numbers</a></li>
      <li><a href="#closed-form-expression-1">Closed Form Expression</a></li>
    </ul>
  </li>
  <li><a href="#patterns-on-the-diagonal">Patterns on the Diagonal</a>
    <ul>
      <li><a href="#computing-diagonal-numbers">Computing Diagonal Numbers</a></li>
      <li><a href="#computing-all-grid-numbers-2">Computing All Grid Numbers</a></li>
      <li><a href="#closed-form-expression-2">Closed Form Expression</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
</ul>
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  Before we explore this problem further, let us rewrite the zigzag
  number spiral grid in a cleaner form, omitting the arrows:

  \begin{array}{rrrrrl}
      1 &amp;   2 &amp;   9  &amp;  10 &amp;  25 &amp; \cd \\
      4 &amp;   3 &amp;   8  &amp;  11 &amp;  24 &amp; \cd \\
      5 &amp;   6 &amp;   7  &amp;  12 &amp;  23 &amp; \cd \\
     16 &amp;  15 &amp;  14  &amp;  13 &amp;  22 &amp; \cd \\
     17 &amp;  18 &amp;  19  &amp;  20 &amp;  21 &amp; \cd \\
    \vd &amp; \vd &amp; \vd  &amp; \vd &amp; \vd &amp; \dd
  \end{array}

  Let \( f(m, n) \) denote the number at the \( m \)th row and
  \( n \)th column.  For example, \( f(1, 1) = 1 \) and \( f(2, 5) =
  24.  \)  We want to find a closed-form expression for \( f(m, n).  \)
</p>
<p>
  Let us first clarify what we mean by a <em>closed-form
  expression</em>.  There is no universal definition of a closed-form
  expression, but the term typically refers to a mathematical
  expression involving variables and constants, built using a finite
  combination of basic operations: addition, subtraction,
  multiplication, division, integer exponents, roots with integer
  index and functions such as exponentials, logarithms and
  trigonometric functions.
</p>
<p>
  In this article, however, we need only addition, subtraction,
  division, squares and square roots.  This may be a bit of a spoiler,
  but I must mention that the \( \max \) function appears in the
  closed-form expressions we are about to see.  If you are concerned
  about whether functions like \( \max \) and \( \min \) are permitted
  in such expressions, note that

  \begin{align*}
    \max(m, n) &amp; = \frac{m + n + \sqrt{(m - n)^2}}{2}, \\
    \min(m, n) &amp; = \frac{m + n - \sqrt{(m - n)^2}}{2}.
  \end{align*}

  So \( \max \) and \( \min \) are simply shorthand for expressions
  involving addition, subtraction, division, squares and square roots.
  In the discussion that follows, we will use only the \( \max \)
  function.
</p>
<h2 id="patterns-on-the-edges">Patterns on the Edges<a href="#patterns-on-the-edges"></a></h2>
<p>
  Let us begin by analysing the edge numbers.  Number the rows as \(
  1, 2, 3 \dots \) and the columns likewise.  Observe where the spiral
  touches the left edge and changes direction.  This happens only on
  even-numbered rows.  Similarly, each time the spiral touches the top
  edge and changes direction, it does so on odd-numbered columns.  In
  the following subsections, we take a closer look at this behaviour
  of the spiral.
</p>
<p>
  I should mention that this section takes a rather long path to
  arrive at the closed-form solution.  Personally, I enjoy such long
  tours.  If you prefer a more direct approach, feel free to skip
  ahead to
  <a href="#patterns-on-the-diagonal">Patterns on the Diagonal</a> for
  a shorter discussion that reaches the same result.
</p>
<h3 id="computing-edge-numbers">Computing Edge Numbers<a href="#computing-edge-numbers"></a></h3>
<p>
  Each time the spiral reaches the left edge of the grid, it does so
  at some \( m \)th row where \( m \) is even.  The \( m \times m \)
  subgrid formed by the first \( m \) rows and the first \( m \)
  columns contains \( m^2 \) consecutive numbers.  Since the numbers
  strictly increase as the spiral grows, the largest of these
  \( m^2 \) numbers must appear at the position where the spiral
  touches the left edge.  This is illustrated in the figure below.
</p>
<figure>
  \begin{array}{rrrr:rl}
     1     &amp;   2 &amp;   9  &amp;  10 &amp;  25 &amp; \cd \\
     4     &amp;   3 &amp;   8  &amp;  11 &amp;  24 &amp; \cd \\
     5     &amp;   6 &amp;   7  &amp;  12 &amp;  23 &amp; \cd \\
    \hl 16 &amp;  15 &amp;  14  &amp;  13 &amp;  22 &amp; \cd \\
    \hdashline
    17     &amp;  18 &amp;  19  &amp;  20 &amp;  21 &amp; \cd \\
    \vd    &amp; \vd &amp; \vd  &amp; \vd &amp; \vd &amp; \dd
  \end{array}
  <figcaption>
    The spiral touches the left edge on the \( 4 \)th row where the
    number is \( 4^2 \)
  </figcaption>
</figure>
<p>
  Whenever the spiral touches the left edge at the \( m \)th row
  (where \( m \) is even), the number in the first column of that row
  is \( m^2.  \)  Hence, we conclude that \( f(m, 1) = m^2 \) when \( m
  \) is even.  Immediately after touching the left edge, the spiral
  turns downwards into the first column of the next row.  Thus, in the
  next row, i.e. in the \( (m + 1) \)th row, we have \( f(m + 1, 1) =
  m^2 + 1, \) where \( m + 1 \) is odd.  This can be restated as \(
  f(m, 1) = (m - 1)^2 + 1 \) when \( m \) is odd.  Since \( f(1, 1) =
  1, \) we can summarise the two formulas we have found here as:

  \[
    f(m, 1) =
      \begin{cases}
        m^2           &amp; \text{if } m \equiv 0 \pmod{2}, \\
        (m - 1)^2 + 1 &amp; \text{if } m \equiv 1 \pmod{2}.
      \end{cases}
  \]
</p>
<p>
  We can perform a similar analysis for the numbers at the top edge
  and note that whenever the spiral touches the top edge at the
  \( n \)th column (where \( n \) is odd), the number in the first row
  of that column is \( n^2.  \)  This is illustrated below.
</p>
<figure>
  \begin{array}{rrr:rrl}
     1 &amp;   2 &amp; \hl 9 &amp;  10 &amp;  25 &amp; \cd \\
     4 &amp;   3 &amp;     8 &amp;  11 &amp;  24 &amp; \cd \\
     5 &amp;   6 &amp;     7 &amp;  12 &amp;  23 &amp; \cd \\
    \hdashline
    16 &amp;  15 &amp;    14 &amp;  13 &amp;  22 &amp; \cd \\
    17 &amp;  18 &amp;    19 &amp;  20 &amp;  21 &amp; \cd \\
    \vd &amp; \vd &amp;  \vd &amp; \vd &amp; \vd &amp; \dd
  \end{array}
  <figcaption>
    The spiral touches the top edge on the \( 3 \)rd column where the
    number is \( 3^2 \)
  </figcaption>
</figure>
<p>
  Immediately after touching the top edge, the spiral turns right into
  the next column.  These observations give us the following formula
  for the numbers at the top edge:

  \[
    f(1, n) =
      \begin{cases}
        n^2           &amp; \text{if } n \equiv 1 \pmod{2}, \\
        (n - 1)^2 + 1 &amp; \text{if } n \equiv 0 \pmod{2}.
      \end{cases}
  \]

  Next we will find a formula for any arbitrary number anywhere in the
  grid.
</p>
<h3 id="computing-all-grid-numbers-1">Computing All Grid Numbers<a href="#computing-all-grid-numbers-1"></a></h3>
<p>
 Since the spiral touches the left edge on even-numbered rows, then
 turns downwards into the next (odd-numbered) row and then starts
 moving right until the diagonal (where it changes direction again),
 the following two rules hold:
</p>
<ul>
  <li>
    On every odd-numbered row, as we go from left to right, the
    numbers increase until we reach the diagonal.
  </li>
  <li>
    On every even-numbered row, as we go from left to right, the
    numbers decrease until we reach the diagonal.
  </li>
</ul>
<p>
  Note that all the numbers we considered in the above two points lie
  on or below the diagonal (or equivalently, on or to the left of the
  diagonal).  Therefore, on an odd-numbered row, we can find the
  numbers on or below the diagonal using the formula \( f(m, n) = f(m,
  1) + (n - 1), \) where \( m \) is odd.  Similarly, on even-numbered
  rows, we can find the numbers on or below the diagonal using the
  formula \( f(m, n) = f(m, 1) - (n - 1), \) where \( m \) is even.
</p>
<p>
  By a similar analysis, the following rules hold when we consider the
  numbers in a column:
</p>
<ul>
  <li>
    On every even-numbered column, as we go from top to bottom, the
    numbers increase until we reach the diagonal.
  </li>
  <li>
    On every odd-numbered column, as we go from top to bottom, the
    numbers decrease until we reach the diagonal.
  </li>
</ul>
<p>
  Now the numbers on or above the diagonal can be found using the
  formula \( f(m, n) = f(1, n) - (m - 1) \) when \( n \) is odd and \(
  f(m, n) = f(1, n) + (m - 1), \) when \( n \) is even.
</p>
<p>
  Can we determine from the values of \( m \) and \( n \) if the
  number \( f(m, n) \) is above the diagonal or below it?  Yes, if \(
  m \le n, \) then \( f(m, n) \) lies on or above the diagonal.
  However, if \( m \ge n, \) then \( f(m, n) \) lies on or below the
  diagonal.
</p>
<p>
  We now have everything we need to write a general formula for
  finding the numbers anywhere in the grid.  Using the four formulas
  and the two inequalities obtained in this section, we get

  \[
    f(m, n) =
      \begin{cases}
        f(1, n) + (m - 1)
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        f(1, n) - (m - 1)
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        f(m, 1) - (n - 1)
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        f(m, 1) + (n - 1)
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  Using the equations for \( f(1, n) \) and \( f(m, 1) \) from the
  previous section, the above formulas can be rewritten as

  \[
    f(m, n) =
      \begin{cases}
        (n - 1)^2 + 1 + (m - 1)
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        n^2 - (m - 1)
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        m^2 - (n - 1)
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        (m - 1)^2 + 1 + (n - 1)
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  Simplifying the expressions on the right-hand side, we get

  \[
    f(m, n) =
      \begin{cases}
        (n - 1)^2 + m
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        n^2 - m + 1
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        m^2 - n + 1
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        (m - 1)^2 + n
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  This is pretty good.  We now have a piecewise formula that works for
  any position in the grid.  Let us now explore whether we can express
  it as a single closed-form expression.
</p>
<h3 id="closed-form-expression-1">Closed Form Expression<a href="#closed-form-expression-1"></a></h3>
<p>
  First, we will rewrite the piecewise formula from the previous
  section in the following form:

  \[
    f(m, n) =
      \begin{cases}
        (n^2 - n + 1) + (m - n)
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        (n^2 - n + 1) - (m - n)
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        (m^2 - m + 1) + (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        (m^2 - m + 1) - (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  This is the same formula, rewritten to reveal common patterns
  between the four expressions on the right-hand side.  In each
  expression, one variable plays the dominant role, occurring several
  times, while the other appears only once.  For example, in the first
  two expressions, \( n \) plays the dominant role whereas \( m \)
  occurs only once.  If we look closely, we realise that it is the
  variable that is greater than or equal to the other that plays the
  dominant role.  Therefore the first and third expressions may be
  written as

  \[
    \left( (\max(m, n))^2 - \max(m, n) + 1 \right) + (m - n).
  \]

  Similarly, the second and fourth expressions may be written as

  \[
    \left( (\max(m, n))^2 - \max(m, n) + 1 \right) - (m - n).
  \]

  We have made some progress towards a closed-form expression.  We
  have collapsed the four expressions in the piecewise formula to just
  two.  The only difference between them lies in the sign of the
  second term: it is positive when the dominant variable is even and
  negative when it is odd.  This observation allows us to unify both
  cases into a single expression:

  \[
    f(m, n) = (\max(m, n))^2 - \max(m, n) + 1 + (-1)^{\max(m, n)} (m - n).
  \]

  Now we have a closed-form expression for \( f(m, n) \) that gives
  the number at any position in the grid.
</p>
<h2 id="patterns-on-the-diagonal">Patterns on the Diagonal<a href="#patterns-on-the-diagonal"></a></h2>
<p>
  As mentioned earlier, there is a shorter route to the same
  closed-form expression.  This alternative approach is based on
  analysing the numbers along the diagonal of the grid.  We still need
  to examine the edge numbers, but not all of them as we did in the
  previous section.  Some of the reasoning about edge values will be
  repeated here to ensure this section is self-contained.
</p>
<h3 id="computing-diagonal-numbers">Computing Diagonal Numbers<a href="#computing-diagonal-numbers"></a></h3>
<p>
  A number on the diagonal has the same row number and column number.
  In other words, a diagonal number has the value \( f(n, n) \) for
  some positive integer \( n.  \)  Consider the case when \( n \) is
  even.  In this case, the diagonal number is on a segment of the
  spiral that is moving to the left.  The \( n \times n \) subgrid
  formed by the first \( n \) rows and the first \( n \) columns
  contains exactly \( n^2 \) consecutive numbers.  Since the diagonal
  number is on the last row of this subgrid and the numbers in this
  row increase as we move from right to left, the largest number in
  the subgrid must be on the left edge of this row.  Therefore the
  number at the left edge is \( f(n, 1) = n^2, \) where \( n \) is
  even.  This is illustrated below.
</p>
<figure>
  \begin{array}{rrrr:rl}
     1     &amp;   2 &amp;   9  &amp;     10 &amp;  25 &amp; \cd \\
     4     &amp;   3 &amp;   8  &amp;     11 &amp;  24 &amp; \cd \\
     5     &amp;   6 &amp;   7  &amp;     12 &amp;  23 &amp; \cd \\
    \hl 16 &amp;  15 &amp;  14  &amp; \hl 13 &amp;  22 &amp; \cd \\
    \hdashline
    17     &amp;  18 &amp;  19  &amp;     20 &amp;  21 &amp; \cd \\
    \vd    &amp; \vd &amp; \vd  &amp;    \vd &amp; \vd &amp; \dd
  \end{array}
  <figcaption>
    The spiral touches the left edge on the \( 4 \)th row where the
    number is \( 4^2 \)
  </figcaption>
</figure>
<p>
  From the diagonal to the edge of the subgrid, there are \( n \)
  consecutive numbers.  In a sequence of \( n \) consecutive numbers,
  the difference between the maximum number and the minimum number is
  \( n - 1.  \)  Therefore, \( n^2 - f(n, n) = n - 1.  \)  This gives us

  \[
    f(n, n) = n^2 - n + 1 \quad \text{if } n \equiv 0 \pmod{2}.
  \]
</p>
<p>
  Now consider the case when \( n \) is odd.
</p>
<figure>
  \begin{array}{rrr:rrl}
     1 &amp;   2 &amp; \hl 9 &amp;  10 &amp;  25 &amp; \cd \\
     4 &amp;   3 &amp;     8 &amp;  11 &amp;  24 &amp; \cd \\
     5 &amp;   6 &amp; \hl 7 &amp;  12 &amp;  23 &amp; \cd \\
    \hdashline
    16 &amp;  15 &amp;    14 &amp;  13 &amp;  22 &amp; \cd \\
    17 &amp;  18 &amp;    19 &amp;  20 &amp;  21 &amp; \cd \\
    \vd &amp; \vd &amp;  \vd &amp; \vd &amp; \vd &amp; \dd
  \end{array}
  <figcaption>
    The spiral touches the top edge on the \( 3 \)rd column where the
    number is \( 3^2 \)
  </figcaption>
</figure>
<p>
  By a similar reasoning, for odd \( n, \) the \( n \)th column has
  numbers that increase as we move up from the diagonal number towards
  the top edge.  Therefore \( f(1, n) = n^2 \) and since \( n^2 - f(n,
  n) = n - 1, \) we again obtain

  \[
    f(n, n) = n^2 - n + 1 \quad \text{if } n \equiv 1 \pmod{2}.
  \]

  Since \( f(n, n) \) takes the same form for both odd and even
  \( n, \) we can write

  \[
    f(n, n) = n^2 - n + 1
  \]

  for all positive integers \( n.  \)
</p>
<h3 id="computing-all-grid-numbers-2">Computing All Grid Numbers<a href="#computing-all-grid-numbers-2"></a></h3>
<p>
  If \( m \le n, \) then the number \( f(m, n) \) lies on or above the
  diagonal number \( f(n, n).  \)  If \( n \) is even, then the numbers
  decrease as we go from the diagonal up to the top edge.  Therefore
  \( f(m, n) \le f(n, n) \) and \( f(m, n) = f(n, n) - (n - m).  \)  If
  \( n \) is odd, then the numbers increase as we go from the diagonal
  up to the top edge and therefore \( f(m, n) \ge f(n, n) \) and \(
  f(m, n) = f(n, n) + (n - m).  \)
</p>
<p>
  If \( m \ge n, \) then the number \( f(m, n) \) lies on or below the
  diagonal number \( f(m, m).  \)  By a similar analysis, we find that
  \( f(m, n) = f(m, m) + (m - n) \) if \( n \) is even and \( f(m, n)
  = f(m, m) - (m - n) \) if \( n \) is odd.  We summarise these
  results as follows:

  \[
    f(m, n) =
      \begin{cases}
        f(n, n) - (n - m)
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        f(n, n) + (n - m)
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        f(m, m) + (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        f(m, m) - (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  Note that the above formula can be rewritten as

  \[
    f(m, n) =
      \begin{cases}
        f(n, n) + (m - n)
        &amp; \text{if } m \le n \text{ and } n \equiv 0 \pmod{2}, \\
        f(n, n) - (m - n)
        &amp; \text{if } m \le n \text{ and } n \equiv 1 \pmod{2}, \\
        f(m, m) + (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 0 \pmod{2}, \\
        f(m, m) - (m - n)
        &amp; \text{if } m \ge n \text{ and } m \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]
</p>
<h3 id="closed-form-expression-2">Closed Form Expression<a href="#closed-form-expression-2"></a></h3>
<p>
  If we take a close look at the last formula in the previous section,
  we find that in each expression, one variable plays a dominant role,
  i.e. it occurs more frequently in the expression than the other.  In
  the first two expressions \( n \) plays the dominant role whereas in
  the last two expressions \( m \) plays the dominant role.  In fact,
  in each expression, the dominant variable is the one that is greater
  than or equal to the other.  With this in mind, we can rewrite the
  above formula as

  \[
    f(m, n) =
      \begin{cases}
        f(\max(m, n), \max(m, n)) + (m - n)
        &amp; \text{if } \max(m, n) \equiv 0 \pmod{2}, \\
        f(\max(m, n), \max(m, n)) - (m - n)
        &amp; \text{if } \max(m, n) \equiv 1 \pmod{2}.  \\
      \end{cases}
  \]

  The only difference between the expressions is the sign of the
  second term: it is positive when \( \max(m, n) \) is even and
  negative when \( \max(m, n) \) is odd.  As a result, we can rewrite
  the above formula as a single expression like this:

  \[
    f(m, n) = f(\max(m, n), \max(m, n)) + (-1)^{\max(m, n)} (m - n).
  \]

  Using the formula \( f(n, n) = n^2 - n + 1 \) from the previous
  section, we get

  \[
    f(m, n) = (\max(m, n))^2 - \max(m, n) + 1 + (-1)^{\max(m, n)} (m - n).
  \]

  We arrive again at the same closed-form expression, this time by
  focusing on the diagonal of the grid.
</p>
<h2 id="references">References<a href="#references"></a></h2>
<ul>
  <li>
    <a href="https://cses.fi/problemset/task/1071">Number Spiral</a>
    from the CSES Problem Set
  </li>
  <li>
    <a href="https://mathworld.wolfram.com/Closed-FormSolution.html">Closed-Form Solution</a>
    by Christopher Stover and Eric W. Weisstein
  </li>
  <li>
    <a href="https://mathworld.wolfram.com/PiecewiseFunction.html">Piecewise Function</a>
    by Eric W. Weisstein
  </li>
</ul>
<!-- ### -->
<p>
  <a href="https://susam.net/zigzag-number-spiral.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Product of Additive Inverses</title>
<link>https://susam.net/product-of-additive-inverses.html</link>
<guid isPermaLink="false">rxpnz</guid>
<pubDate>Thu, 29 May 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  A negative number multiplied by another negative number results in a
  positive number.  Most of us learnt this rule during our primary or
  secondary school years.  'Negative times negative equals positive'
  was a phrase drummed into us during mathematics lessons.  In this
  article, we will prove this rule, not just for numbers but for any
  algebraic structure that, in a general sense, behaves somewhat like
  numbers.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#illustration">Illustration</a></li>
  <li><a href="#ring-axioms">Ring Axioms</a></li>
  <li><a href="#closure-properties">Closure Properties</a></li>
  <li><a href="#inverse-of-inverse">Inverse of Inverse</a></li>
  <li><a href="#multiplication-by-zero">Multiplication by Zero</a></li>
  <li><a href="#multiplication-by-additive-inverse">Multiplication by Additive Inverse</a></li>
  <li><a href="#product-of-additive-inverses">Product of Additive Inverses</a></li>
  <li><a href="#alternate-proof">Alternate Proof</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="illustration">Illustration<a href="#illustration"></a></h2>
<p>
  Let us begin with a quick illustration that shows why the product of
  two negative numbers must be positive for arithmetic to make sense.
  Consider

  \[
    7 \times 8 = 56.
  \]

  The above equation can also be written as

  \[
    (10 - 3) \times (10 - 2) = 56.
  \]

  Using the distributive property of multiplication over subtraction,
  we get

  \[
    (10 - 3) \times 10 + (10 - 3) \times (-2) = 56.
  \]

  Using the distributive property again, we have

  \[
    10 \times 10 + (-3) \times 10 + 10 \times (-2) + (-3) \times (-2) = 56.
  \]

  Now, we will take it for granted that a positive times a negative is
  negative.  We will prove all of this rigorously later, but for now,
  we are just working through an illustration, so we will accept that
  rule and see where it leads.  The equation becomes:

  \[
    100 + (-30) + (-20) + (-3) \times (-2) = 56.
  \]

  Adding the first three terms gives

  \[
    50 + (-3) \times (-2) = 56.
  \]

  Subtracting \( 50 \) from both sides, we get

  \[
    (-3) \times (-2) = 6.
  \]

  What we have seen here is that if we accept \( 7 \times 8 = 56 \)
  and that positive times negative gives a negative result, then we
  must also accept that \( (-3) \times (-2) = 6.  \)
</p>
<h2 id="ring-axioms">Ring Axioms<a href="#ring-axioms"></a></h2>
<p>
  From this section onwards, we take a rigorous approach.  We want to
  show that the rule 'negative times negative equals positive' holds,
  in a general sense, for any set of elements that share certain
  properties with numbers.  As it turns out, these elements do not
  need to possess all the properties of complex numbers, real numbers
  or even rational numbers.  In fact, if they satisfy a small and
  specific set of properties held by the integers, then the rule still
  holds.  These properties are known as the <em>ring axioms</em>.
</p>
<p>
  A ring is an algebraic structure consisting of a set \( R \) with
  two binary operations \( + \) and \( \cdot, \) called addition and
  multiplication respectively, satisfying the following axioms:
</p>
<ol>
  <li>
    <p>
      <strong>Associativity of addition:</strong> For all \( a, b, c
      \in R, \) we have \( a + (b + c) = (a + b) + c.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Commutativity of addition:</strong> For all \( a, b \in
      R, \) we have \( a + b = b + a.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Additive identity:</strong> There exists an element \( 0
      \in R \) such that for all \( a \in R, \) we have \( a + 0 = a =
      0 + a.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Additive inverse:</strong> For each \( a \in R, \) there
      exists an element \( -a \in R \) such that \( a + (-a) = 0 =
      (-a) + a.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Associativity of multiplication:</strong> For all \( a,
      b, c \in R, \) we have \( a \cdot (b \cdot c) = (a \cdot b)
      \cdot c.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Left distributivity of multiplication over
      addition:</strong> For all \( a, b, c \in R, \) we have \( a
      \cdot (b + c) = (a \cdot b) + (a \cdot c).  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Right distributivity of multiplication over
      addition:</strong> For all \( a, b, c \in R, \) we have \( (b +
      c) \cdot a = (b \cdot a) + (c \cdot a).  \)
    </p>
  </li>
</ol>
<p>
  Note that we do not assume that the ring contains multiplicative
  identity, nor do we assume that multiplication is commutative.  Many
  familiar types of numbers form rings.  For example, the set of
  integers forms a ring with the usual addition and multiplication
  operations.  The sets of rational numbers, real numbers and complex
  numbers satisfy the ring axioms too.
</p>
<p>
  Rings need not consist of numbers; they may contain elements
  of <em>any</em> type.  As long as a set of elements, together with
  suitable addition and multiplication operations, satisfies the seven
  axioms above, it forms a ring.  For example, the set of all
  polynomials in the indeterminate \( t \) with coefficients in some
  ring \( R \) forms a ring under the usual addition and
  multiplication of polynomials.  Such a ring is called
  a <em>polynomial ring</em> and it is denoted \( R[t].  \)
</p>
<h2 id="closure-properties">Closure Properties<a href="#closure-properties"></a></h2>
<p>
  Some texts include the following additional axioms for the closure
  properties of a ring:
</p>
<ol>
  <li>
    <p>
      <strong>Closure under addition:</strong> For all
      \( a, b \in R, \) we have \( a + b \in R.  \)
    </p>
  </li>
  <li>
    <p>
      <strong>Closure under multiplication:</strong> For all \( a, b
      \in R, \) we have \( a \cdot b \in R.  \)
    </p>
  </li>
</ol>
<p>
  However, stating these axioms explicitly is usually considered
  redundant because a binary operation is closed by definition.  A
  binary operation \( \circ \) on a set \( M \) is defined to be a
  function

  \[
    \circ : M \times M \to M; \quad (a, b) \mapsto a \circ b.
  \]

  This definition automatically implies the closure property, since
  the domain and codomain are the same.  The addition and
  multiplication operations on a ring \( R \) may be defined as

  \begin{align*}
        + &amp;: R \times R \to R; \quad (a, b) \mapsto a + b, \\
    \cdot &amp;: R \times R \to R; \quad (a, b) \mapsto a \cdot b.
  \end{align*}

  These definitions imply that a ring is closed under addition and
  multiplication.  In practice, while deciding if some set \( R \)
  forms a ring, we should always verify that the addition and
  multiplication operations indeed have \( R \) as the codomain to
  confirm that the closure property holds.
</p>
<h2 id="inverse-of-inverse">Inverse of Inverse<a href="#inverse-of-inverse"></a></h2>
<p id="theorem-1">
  <strong>Theorem 1.</strong>
  <em>
    Let \( R \) be a ring with \( + \) and \( \cdot \) operations.
    Then for all \( a \in R, \) we have

    \[
      -(-a) = a.
    \]
  </em>
</p>
<p>
  <em>Proof.</em> This result follows directly from the additive
  inverse axiom.  First, observe that

  \[
    a + (-a) = 0.
  \]

  Therefore \( a \) is an additive inverse of \( -a, \) i.e.

  \[
    -(-a) = a.
  \]

  This completes the proof.
</p>
<p>
  Notice that this proof does not involve the multiplication operation
  of a ring at all.  In fact, it holds true in a more general
  algebraic structure known as a <em>group</em>, which requires only a
  binary operation with associativity, an identity element and
  inverses.  A ring, under addition, is also a group.  Since the proof
  relies solely on these additive group properties, this theorem holds
  for all groups.  However, for brevity and to avoid introducing group
  axioms separately, I have stated and proved this theorem in the
  context of rings.
</p>
<p>
  It is also worth noting that the additive identity is unique in a
  ring (as well as in any group), but since this fact is not needed
  for later results, its proof has been omitted.  Even if,
  hypothetically, there were two distinct additive identities, \( 0 \)
  and \( 0', \) in a ring (there are not, of course), the arguments
  below would still hold if we simply focus on \( 0.  \)
</p>
<h2 id="multiplication-by-zero">Multiplication by Zero<a href="#multiplication-by-zero"></a></h2>
<p id="theorem-2">
  <strong>Theorem 2.</strong>
  <em>
    Let \( R \) be a ring with \( + \) and \( \cdot \) operations.  Then
    for all \( a \in R, \) we have

    \[
      a \cdot 0 = 0 \cdot a = 0.
    \]
  </em>
</p>
<p>
  <em>Proof.</em> Using the additive identity axiom, we get

  \[
    0 + 0 = 0.
  \]

  Multiplying both sides on the left by \( a, \) we get

  \[
    a \cdot (0 + 0) = a \cdot 0.
  \]

  Using the left distributivity axiom, we get

  \[
    a \cdot 0 + a \cdot 0 = a \cdot 0.
  \]

  Let \( b = a \cdot 0.  \)  Then

  \[
    b + b = b.
  \]

  Since a ring is closed under multiplication, \( b \in R.  \)  By the
  additive inverse axiom, there exists \( -b \in R \) such that \( b +
  (-b) = 0.  \)  Adding \( -b \) to both sides of the above equation,
  we get

  \[
    (b + b) + (-b) = b + (-b).
  \]

  By associativity of addition in a ring, we get

  \[
    b + (b + (-b)) = b + (-b).
  \]

  Since \( b + (-b) = 0, \) the above equation becomes

  \[
    b + 0 = 0.
  \]

  By the additive identity axiom, we get

  \[
    b = 0.
  \]

  Since \( b = a \cdot 0, \) the above equation may be written as

  \[
    a \cdot 0 = 0.
  \]

  A similar argument shows that

  \[
    0 \cdot a = 0.
  \]

  This completes the proof.
</p>
<h2 id="multiplication-by-additive-inverse">Multiplication by Additive Inverse<a href="#multiplication-by-additive-inverse"></a></h2>
<p id="theorem-3">
  <strong>Theorem 3.</strong>
  <em>
    Let \( R \) be a ring with \( + \) and \( \cdot \) operations.
    Then for all \( a, b \in R, \) we have

    \begin{align*}
      a \cdot (-b) &amp;= -(a \cdot b), \\
      (-a) \cdot b &amp;= -(a \cdot b).
    \end{align*}
    </em>
</p>
<p>
  <em>Proof.</em> Using the left distributivity and additive inverse
  properties of a ring along with <a href="#theorem-2">Theorem 2</a>,
  we get

  \[
    a \cdot b + a \cdot (-b)
    = a \cdot (b + (-b))
    = a \cdot 0
    = 0.
  \]

  Therefore \( a \cdot (-b) \) is an additive inverse of \( a \cdot b
 , \) i.e.

  \[
    -(a \cdot b) = a \cdot (-b).
  \]

  Similarly

  \[
    a \cdot b + (-a) \cdot b
    = (a + (-a)) \cdot b
    = 0 \cdot b
    = 0
  \]

  and thus

  \[
    -(a \cdot b) = (-a) \cdot b.
  \]

  This completes the proof.
</p>
<h2 id="product-of-additive-inverses">Product of Additive Inverses<a href="#product-of-additive-inverses"></a></h2>
<p>
  <strong>Theorem 4.</strong>
  <em>
    Let \( R \) be a ring with \( + \) and \( \cdot \) operations.  Then
    for all \( a, b \in R, \) we have

    \[
      (-a) \cdot (-b) = a \cdot b.
    \]
  </em>
</p>
<p>
  <em>Proof.</em>
  From <a href="#theorem-3">Theorem 3</a>, we know that

  \[
    a \cdot (-b) = -(a \cdot b).
  \]

  Substituting \( a \) with \( -a, \) we get

  \[
    (-a) \cdot (-b) = -((-a) \cdot b).
  \]

  Again by <a href="#theorem-3">Theorem 3</a>, we have \( (-a) \cdot b
  = -(a \cdot b).  \)  Substituting this in the above equation, we
  obtain

  \[
    (-a) \cdot (-b) = -(-(a \cdot b)).
  \]

  Now using <a href="#theorem-1">Theorem 1</a>, the right-hand side
  becomes \( a \cdot b, \) so we get

  \[
    (-a) \cdot (-b) = a \cdot b.
  \]

  This completes the proof.
</p>
<h2 id="alternate-proof">Alternate Proof<a href="#alternate-proof"></a></h2>
<p>
  The above sequence of theorems is not the only way to arrive
  at <a href="#theorem-4">Theorem 4</a>.  There are other ways to
  reach this result as well.  Let us briefly discuss another such
  proof.  From <a href="#theorem-3">Theorem 3</a> we know that \( a
  \cdot b \) is the additive inverse of \( a \cdot (-b).  \)  In a very
  similar way, we can show that \( (-a) \cdot (-b) \) is also the
  additive inverse of \( a \cdot (-b).  \)  The proof goes as follows:

  \[
    (-a) \cdot (-b) + a \cdot (-b)
    = (-a + a) \cdot (-b)
    = 0 \cdot (-b)
    = 0.
  \]

  Note that we used <a href="#theorem-2">Theorem 2</a> again for the
  last equality.  So now we know that both \( (-a) \cdot (-b) \) and
  \( a \cdot b \) are additive inverses of \( a \cdot (-b).  \)  Does
  this mean that \( (-a) \cdot (-b) = a \cdot b?  \)  Yes, since the
  additive inverse of an element is unique in a ring.  Let us prove
  this now.

  Let \( b \) and \( c \) be additive inverses of \( a.  \)  Then \( a
  + b = b + a = 0 \) and \( a + c = c + a = 0.  \)  Using this, we get

  \[
    b = b + 0 = b + (a + c) = (b + a) + c = 0 + c = c.
  \]

  Since \( (-a) \cdot (-b) \) and \( a \cdot b \) are additive
  inverses of the same element \(a \cdot (-b) \) and since the
  additive inverse of an element is unique in a ring, it follows that

  \[
    (-a) \cdot (-b) = a \cdot b.
  \]

  Note that we do not need <a href="#theorem-1">Theorem 1</a> in this
  alternate proof but we introduced a new theorem about the uniqueness
  of the additive inverse to complete this proof.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  Theorems 1 to 4 establish certain algebraic properties that hold in
  any ring.  Although these results were proven abstractly for rings,
  they reflect properties we are already familiar with from our
  experience with numbers.  For example, in the ring of integers, we
  observe \( -(-2) = 2 \) which is a specific case
  of <a href="#theorem-1">Theorem 1</a>.
</p>
<p>
  Similarly, <a href="#theorem-2">Theorem 2</a> confirms the
  well-known fact that multiplying any integer by \( 0 \) yields
  \( 0.  \)  For example, \( 2 \cdot 0 = 0.  \)
</p>
<p>
  Then <a href="#theorem-3">Theorem 3</a> implies the rule that
  multiplying a positive number by a negative number yields a negative
  result.  For example, \( 2 \cdot (-3) = -(2 \cdot 3) = -6.  \)
</p>
<p>
  Finally, <a href="#theorem-4">Theorem 4</a> implies that the product
  of two negative numbers is positive.  For example, \( (-2) \cdot
  (-3) = 2 \cdot 3 = 6.  \)
</p>
<p>
  These familiar results are not limited to the ring of integers.  The
  results hold in any ring, including polynomial rings, rings of
  integers modulo a fixed positive integer and many other algebraic
  systems.  These results demonstrate how the ring axioms formalise
  familiar arithmetic rules within a more general algebraic framework.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/product-of-additive-inverses.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Two Ideals of Fields</title>
<link>https://susam.net/two-ideals-of-fields.html</link>
<guid isPermaLink="false">xsuzd</guid>
<pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  A field has exactly two ideals: the zero ideal, which contains only
  the additive identity and the whole field itself.  These are known
  as trivial ideals.  Further if a commutative ring, with distinct
  additive and multiplicative identities, has no ideals other than the
  trivial ones, then it must be a field.  These two facts are elegant
  in their symmetry and simplicity.  In this article, we will explore
  why these facts are true.  Familiarity with algebraic structures
  such as groups, rings and fields is assumed.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#definition-of-ideals">Definition of Ideals</a></li>
  <li><a href="#examples-of-ideals">Examples of Ideals</a></li>
  <li><a href="#known-results">Known Results</a></li>
  <li><a href="#ideals-of-fields">Ideals of Fields</a></li>
  <li><a href="#rings-with-trivial-ideals">Rings With Trivial Ideals</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="definition-of-ideals">Definition of Ideals<a href="#definition-of-ideals"></a></h2>
<p>
  A left ideal of a ring \( R \) is a subset \( I \subseteq R \) such
  that \( I \) is an additive subgroup of \( R \) and for all \( a \in
  I \) and \( r \in R, \) we have \( r \cdot a \in I.  \)  We say that
  a left ideal absorbs multiplication from the left by any ring
  element; or equivalently, that it is closed under left
  multiplication by any ring element.
</p>
<p>
  Similarly, a right ideal of a ring \( R \) is a subset \( I
  \subseteq R \) such that \( I \) is an additive subgroup of \( R \)
  and for all \( a \in I \) and \( r \in R, \) we have \( a \cdot r
  \in I.  \)  We say that a right ideal absorbs multiplication from the
  right by any ring element; or equivalently, that it is closed under
  right multiplication by any ring element.
</p>
<p>
  In a commutative ring \( R, \) every left ideal is also a right
  ideal and vice versa.  This is because for all \( a \in I \) and \(
  r \in R, \) we have \( r \cdot a = a \cdot r.  \)  Therefore, when
  working with commutative rings, we do not need to distinguish
  between left and right ideals and we simply refer to them as ideals.
  In this case, the ideal is said to absorb multiplication by any ring
  element; or equivalently, it is said to be closed under
  multiplication by any ring element.
</p>
<h2 id="examples-of-ideals">Examples of Ideals<a href="#examples-of-ideals"></a></h2>
<p>
  Consider the set of even integers

  \[
    \langle 2 \rangle = \{ 2n : n \in \mathbb{Z} \}.
  \]

  This is an ideal of \( \mathbb{Z}.  \)  Indeed, if we multiply any
  even integer by any integer, the result is an even integer.  In
  other words, the set of even integers absorbs multiplication by any
  integer.  Equivalently, the set of even integers is closed under
  multiplication by any integer.
</p>
<p>
  Let us see another example.  Consider the ring of polynomials in the
  indeterminate \( t \) with integer coefficients, denoted \(
  \mathbb{Z}[t].  \)  The set

  \[
    \langle 2, t \rangle = \{ 2f + tg : f, g \in \mathbb{Z}[t] \}
  \]

  is an ideal of \( \mathbb{Z}[t].  \)  Every element of this ideal is
  a linear combination of \( 2 \) and \( t \) with polynomial
  coefficients.  If we take any element \( 2f + tg \in \langle 2, t
  \rangle \) where \( f, g \in \mathbb{Z}[t] \) and multiply it by any
  polynomial \( h \in \mathbb{Z}[t], \) we obtain \( 2fh + tgh, \)
  which is again an element of \( \langle 2, t \rangle.  \)  Hence \(
  \langle 2, t \rangle \) absorbs multiplication by any element of \(
  \mathbb{Z}[t], \) i.e. it is closed under multiplication by elements
  of \( \mathbb{Z}[t].  \)
</p>
<h2 id="known-results">Known Results<a href="#known-results"></a></h2>
<p>
  For the sake of brevity, we assume the following standard results.
</p>
<p id="zero-multiplication">
  <strong>Proposition 1.</strong>
  <em>
    Let \( R \) be a ring.  Then, for all \( a \in R, \) we have

    \[
      a \cdot 0 = 0 \cdot a = 0.
    \]
  </em>
</p>
<p id="principal-ideal">
  <strong>Proposition 2.</strong>
  <em>
    Let \( R \) be a ring and let \( a \in R.  \)  Then

    \begin{align*}
      I_L &amp;= \{ r \cdot a : r \in R \}, \\
      I_R &amp;= \{ a \cdot r : r \in R \}
    \end{align*}

    are respectively a left ideal and a right ideal of \( R.  \)  If \(
    R \) is commutative, then \( I_L = I_R \) and we write

    \[
      \langle a \rangle = \{ a \cdot r : r \in R \}
    \]

    and say that \( \langle a \rangle \) is an ideal of \( R \)
    generated by \( a.  \)
  </em>
</p>
<h2 id="ideals-of-fields">Ideals of Fields<a href="#ideals-of-fields"></a></h2>
<p>
  In this section, we show that a field \( K \) has only two ideals:
  \( \{ 0 \} \) and \( K \) itself.
</p>
<p>
  Clearly \( \{ 0 \} \) is an ideal of \( K, \) as it satisfies the
  definition of an ideal.  It is the trivial additive subgroup of \(
  K \) and by <a href="#zero-multiplication">Proposition 1</a>, for
  all \( r \in K, \) we have \( r \cdot 0 = 0 \in \{ 0 \}.  \)
</p>
<p>
  Now \( K \) is also an ideal of itself.  Since \( K \) is an additive
  group by the definition of a field, it is an additive subgroup of
  itself.  Moreover, as a field, \( K \) is closed under
  multiplication, so for all \( a, r \in K \) we have \( a \cdot r \in
  K.  \)
</p>
<p>
  We will now show that \( \{ 0 \} \) and \( K \) are
  the <em>only</em> ideals of \( K.  \)  Let \( I \) be an ideal of \(
  K.  \)  There are two cases to consider: \( I = \{ 0 \} \) and \( I
  \ne \{ 0 \}.  \)  Suppose \( I \ne \{ 0 \}.  \)  Then there exists a
  non-zero element \( b \in I.  \)  Since \( b \ne 0 \) and \( K \) is
  a field, \( b \) has a multiplicative inverse \( b^{-1} \in K.  \)
  Since \( b \in I, \) \( b^{-1} \in K \) and \( I \) is closed under
  multiplication by any element of \( K, \) we have

  \[
    1 = b \cdot b^{-1} \in I.
  \]

  Now, let \( c \in K.  \)  Since \( 1 \in I, \) \( c \in K \) and \(
  I \) is an ideal of \( K, \) we get

  \[
    c = 1 \cdot c \in I.
  \]

  Thus \( K \subseteq I \) and since \( I \subseteq K \) by
  definition, we conclude \( I = K.  \)  Therefore the only ideals of
  \( K \) are \( \{ 0 \} \) and \( K \) itself.
</p>
<h2 id="rings-with-trivial-ideals">Rings With Trivial Ideals<a href="#rings-with-trivial-ideals"></a></h2>
<p>
  We now show that if \( R \) is a commutative ring with \( 1 \ne 0 \)
  and the only ideals of \( R \) are \( \{ 0 \} \) and \( R \) itself,
  then \( R \) must be a field.  To do this, we first show that every
  non-zero element of \( R \) has a multiplicative inverse in \( R.  \)
  Let \( a \in R \) with \( a \ne 0.  \)  We now show that there exists
  a multiplicative inverse \( a^{-1} \in R.  \)
  By <a href="#principal-ideal">Proposition 2</a>, the set

  \[
    \langle a \rangle = \{ a \cdot r : r \in R \}.
  \]

  is an ideal of \( R.  \)  Since \( a = a \cdot 1 \in \langle a
  \rangle, \) we have \( \langle a \rangle \ne \{ 0 \}.  \)  By
  assumption, the only ideals of \( R \) are \( \{ 0 \} \) and
  \( R, \) so it must be that \( \langle a \rangle = R.  \)  Therefore
  \( 1 \in \langle a \rangle \) and

  \[
    1 = a \cdot s
  \]

  for some \( s \in R.  \)  Thus \( a \) has a multiplicative inverse
  \( s \in R \) and this holds for every non-zero \( a \in R.  \)
</p>
<p>
  The remaining properties of fields, namely, associativity and
  commutativity of addition and multiplication, the existence of
  distinct additive and multiplicative identities, the existence of
  additive inverses and the distributivity of multiplication over
  addition, are inherited from the ring \( R.  \)  Therefore \( R \) is
  a field.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  To summarise, any commutative ring with distinct additive and
  multiplicative identities that has only trivial ideals is a field
  and every field has only trivial ideals.
</p>
<p>
  Note that every field is also a commutative ring with distinct
  additive and multiplicative identities.  Therefore, we can say that
  every field is a commutative ring with distinct additive and
  multiplicative identities and only trivial ideals and vice versa.
  It is neat how the two facts align so nicely.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/two-ideals-of-fields.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>From Finite Integral Domains to Finite Fields</title>
<link>https://susam.net/from-finite-integral-domains-to-finite-fields.html</link>
<guid isPermaLink="false">ojxkk</guid>
<pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  In this article, we explore a few well-known results from abstract
  algebra pertaining to fields and integral domains.  We ask ourselves
  whether every field is an integral domain and whether every integral
  domain is a field.  We begin with the definition of an integral
  domain, discuss a few established results and then proceed to answer
  these questions.  Familiarity with algebraic structures such as
  rings and fields is assumed.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#definition-of-integral-domain">Definition of Integral Domain</a></li>
  <li><a href="#examples-of-integral-domain">Examples of Integral Domains</a></li>
  <li><a href="#known-results">Known Results</a></li>
  <li><a href="#on-distinct-identities">On Distinct Identities</a></li>
  <li><a href="#every-field-is-an-integral-domain">Every Field Is an Integral Domain</a></li>
  <li><a href="#infinite-integral-domains">Infinite Integral Domains</a></li>
  <li><a href="#every-finite-integral-domain-is-a-field">Every Finite Integral Domain Is a Field</a>
    <ul>
      <li><a href="#alternate-proof">Alternate Proof</a></li>
    </ul>
  </li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="definition-of-integral-domain">Definition of Integral Domain<a href="#definition-of-integral-domain"></a></h2>
<p>
  An <em>integral domain</em> is a commutative ring, with distinct
  additive and multiplicative identities, in which the product of any
  two non-zero elements is also non-zero.
</p>
<p>
  Equivalently, an integral domain is a commutative ring, with
  distinct additive and multiplicative identities, such that if the
  product of two elements is zero, then one of the elements must be
  zero.
</p>
<p>
  Using standard notation, we can write that a commutative ring
  \( R \) is an integral domain if \( 0 \ne 1 \) and for
  \( a, b \in R, \)

  \[
    a \ne 0 \text{ and } b \ne 0 \implies a \cdot b \ne 0
  \]

  or equivalently,

  \[
    a \cdot b = 0 \implies a = 0 \text{ or } b = 0.
  \]

  There are many other alternative ways to define an integral domain
  which are all equivalent.  In a ring \( R, \) a <em>zero
  divisor</em> is a non-zero element \( a \in R \) such that there
  exists a non-zero element \( b \in R \) with \( ab = 0.  \)  With
  this definition of a zero divisor, we can define an integral domain
  to be a unital commutative ring, with \( 0 \ne 1, \) that has no
  zero divisors.
</p>
<h2 id="examples-of-integral-domain">Examples of Integral Domains<a href="#examples-of-integral-domain"></a></h2>
<p>
  The ring of integers \( \mathbb{Z} \) is an integral domain since
  the product of two non-zero integers is non-zero.  The field of
  rational numbers \( \mathbb{Q} \) is also an integral domain.  The
  ring of polynomials in the indeterminate \( t \) with coefficients
  in an integral domain \( R, \) denoted \( R[t], \) is an integral
  domain as well.
</p>
<p>
  The ring of integers modulo 5, denoted \( \mathbb{Z}_5 \) is an
  integral domain.  However, the ring of integers modulo 6, denoted \(
  \mathbb{Z}_6, \) is not an integral domain since \( 2 \cdot 3 = 0 \)
  in \( \mathbb{Z}_6.  \)  In other words, \( \mathbb{Z}_6 \) has zero
  divisors, namely \( 2 \) and \( 3, \) so it is not an integral
  domain.  In fact, the ring of integers modulo \( n, \) denoted \(
  \mathbb{Z}_n \) is an integral domain if and only if \( n \) is
  prime.
</p>
<h2 id="known-results">Known Results<a href="#known-results"></a></h2>
<p>
  For the sake of brevity, we assume the following known results.
</p>
<p id="zero-multiplication">
  <strong>Proposition 1.</strong>
  <em>
    Let \( R \) be a ring.  Then, for all \( a \in R, \) we have

    \[
      a \cdot 0 = 0 \cdot a = 0.
    \]
  </em>
</p>
<p id="cancellation-property">
  <strong>Proposition 2.</strong>
  <em>
    Let \( D \) be an integral domain.  Then, for all \( a, b, c \in D \) such
    that \( a \ne 0, \) we have

    \[
      a \cdot b = a \cdot c \implies b = c.
    \]
  </em>
</p>
<p>
  The second result is also known as the <em>cancellation property of
  integral domains</em>.
</p>
<h2 id="on-distinct-identities">On Distinct Identities<a href="#on-distinct-identities"></a></h2>
<p>
  We have been mentioning the distinctiveness of the additive and
  multiplicative identities as a property of an integral domain.  Some
  texts express this more concisely by saying that an integral domain
  is a <em>non-zero</em> unital commutative ring without zero
  divisors, i.e. the zero ring \( \{ 0 \} \) is excluded from the
  definition.
</p>
<p>
  This follows directly from
  <a href="#zero-multiplication">Proposition 1</a>.  If
  \( 0 = 1 \in R, \) then for all \( r \in R, \) we get

  \[
    r = r \cdot 1 = r \cdot 0 = 0
  \]

  which means that every element of \( R \) is zero, i.e. \( R = \{ 0
  \}.  \)  To summarise

  \[
    0 = 1 \in R \implies R = \{ 0 \}
  \]

  or equivalently, for a ring \( R \) with unity,

  \[
    R \ne \{ 0 \} \implies 0 \ne 1.
  \]

  Further, if \( 0 \) and \( 1 \) are two distinct elements of
  \( R, \) then \( R \) has at least two elements, so for a ring
  \( R \) with unity,

  \[
    0 \ne 1 \implies R \ne \{ 0 \}
  \]

  Therefore, a ring with unity has distinct additive and
  multiplicative identities if and only if it is a non-zero ring.  This
  is why an integral domain can also be defined as a non-zero unital
  commutative ring without zero divisors.
</p>
<h2 id="every-field-is-an-integral-domain">Every Field Is an Integral Domain<a href="#every-field-is-an-integral-domain"></a></h2>
<p>
  We now show that every field is indeed an integral domain.  Let
  \(F \) be a field and let \( a, b \in F \) such that \( ab = 0.  \)
  There are two cases to consider: \( a = 0 \) and \( a \ne 0.  \)  If
  \( a = 0, \) we are done.
</p>
<p>
  Now suppose \( a \ne 0.  \)  Then by the properties of fields, there exists a
  multiplicative inverse \( a^{-1} \in F \) such that \( a \cdot
  a^{-1} = 1.  \)  Then using the properties of fields, we get

  \[
    b = b \cdot 1 = b \cdot (a \cdot a^{-1}) = (a \cdot b) \cdot
    a^{-1} = 0 \cdot a^{-1} = 0.
  \]

  The last equality follows from
  <a href="#zero-multiplication">Proposition 1</a>.  We have shown
  that if \( ab = 0, \) then either \( a = 0 \) or \( b = 0.  \)
  Therefore, if both \( a \ne 0 \) and \( b \ne 0, \) then it must be
  that \( ab \ne 0.  \)  Therefore \( F \) is an integral domain.
</p>
<h2 id="infinite-integral-domains">Infinite Integral Domains<a href="#infinite-integral-domains"></a></h2>
<p>
  Now we arrive at the next natural question.  Is every integral
  domain a field?
</p>
<p>
  The ring of integers \( \mathbb{Z} \) is an integral domain but it
  is not a field since \( 2 \in \mathbb{Z}, \) but \( 2^{-1} \notin
  \mathbb{Z}.  \)  Therefore, \( \mathbb{Z} \) is an example of an
  infinite integral domain that is not a field.
</p>
<p>
  Next we ask ourselves: Is every infinite integral domain not a
  field?  Not quite!  Some infinite integral domains are, in fact,
  fields.  This follows directly from the result in the previous
  section.  Every field is an integral domain and there are plenty of
  infinite fields, so they must all be integral domains too.  Consider
  the field of rational numbers \( \mathbb{Q} \) or the field of
  complex numbers \( \mathbb{C}.  \)  Since these are fields, they are
  also integral domains.  So, clearly, there are infinite integral
  domains that are also fields.
</p>
<h2 id="every-finite-integral-domain-is-a-field">Every Finite Integral Domain Is a Field<a href="#every-finite-integral-domain-is-a-field"></a></h2>
<p>
  We will now turn our attention to finite integral domains.  Is every
  finite integral domain a field?  Yes!  This can be shown as follows.
</p>
<p>
  Let \( D \) be a finite integral domain.  Let \( a \in D \) with \(
  a \ne 0.  \)  Consider the set

  \[
    A = \{ a, a^2, a^3, \dots \}.
  \]

  Since a ring is closed under multiplication, every element of
  \( A \) belongs to \( D, \) so \( A \subseteq D.  \)  Since \( D \)
  is finite, \( A \) is finite too.  Therefore, by the pigeonhole
  principle, there exist integers \( m \gt n \ge 0 \) such that

  \[
    a^m = a^n
  \]

  This equation can be rewritten as

  \[
    a \cdot a^{m - n - 1} \cdot a^n = 1 \cdot a^n.
  \]

  Since \( a \) is a non-zero element of an integral domain, it
  follows that \( a^n \ne 0.  \)  Therefore we can use
  <a href="#cancellation-property">Proposition 2</a> (the cancellation
  property of integral domains) to get

  \[
    a \cdot a^{m - n - 1} = 1.
  \]

  Since a ring is closed under multiplication and since \( m - n - 1
  \ge 0, \) it follows that \( a^{m - n - 1} \in D.  \)  Thus every
  non-zero element \( a \in D \) has a multiplicative inverse in
  \( D.  \)  This establishes the multiplicative inverse property of a
  field.
</p>
<p>
  Since an integral domain has distinct additive and multiplicative
  identities, it satisfies two additional field properties: the
  existence of an additive identity and a distinct multiplicative
  identity.
</p>
<p>
  Finally, the remaining field properties are inherited from the ring
  structure, i.e. associativity and commutativity of addition and
  multiplication, the existence of additive inverses and the
  distributivity of multiplication over addition all hold in \( D, \)
  since they hold in any ring.  Thus, \( D \) satisfies all the field
  properties.  Therefore \( D \) is a field.
</p>
<h3 id="alternate-proof">Alternate Proof<a href="#alternate-proof"></a></h3>
<p>
  The proof in the previous section presents what I initially came up
  with while working through these concepts and proving these results
  for myself.  However, I later found that there is another proof that
  is quite popular in the literature.  This alternate proof differs in
  one key aspect: it does not invoke the cancellation property of
  integral domains stated in
  <a href="#cancellation-property">Proposition 2</a>.  Let us examine
  this alternate proof.
</p>
<p>
  As before, we consider the set \( A = \{ a, a^2, a^3, \dots \}
  \subseteq D, \) where \( a \in D \) and \( a \ne 0 \) and we obtain
  the equation

  \[
    a^m = a^n
  \]

  for some integers \( m \gt n \ge 0.  \)  As before, we use the fact
  that \( a \) is an element of an integral domain to conclude that \(
  a^n \ne 0.  \)  Now adding the additive inverse of \( a^n \) to both
  sides we get

  \[
    a^m - a^n = 0.
  \]

  Using the distributivity property of rings, we get

  \[
    a^n (a^{m - n} - 1) = 0
  \]

  Since a ring is closed under addition and multiplication, both \(
  a^n \) and \( a^{m - n} - 1 \) belong to \( D.  \)  As \( D \) is an
  integral domain and \( a^n \ne 0, \) we conclude that \( a^{m - n} -
  1 = 0.  \)  Therefore \( a^{m - n} = 1.  \)  Since \( m - n \ge 1, \)
  we can write:

  \[
    a \cdot a^{m - n - 1} = 1.
  \]

  Therefore every non-zero element \( a \in D \) has a multiplicative
  inverse in \( D.  \)  The remaining properties of a field are
  established in the same manner as in the previous section.  Hence,
  if \( D \) is a finite integral domain, then it is also a field.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  We now summarise all the results here before concluding the article:
</p>
<ul>
  <li>
    Every field is an integral domain.
  </li>
  <li>
    Every <em>finite</em> integral domain is a field.
  </li>
  <li>
    Some infinite integral domains are not fields.  A convenient
    example is the set of integers \( \mathbb{Z}.  \)
  </li>
  <li>
    Some infinite integral domains are fields.  Every infinite field,
    such as \( \mathbb{Q}, \) \( \mathbb{R} \) or \( \mathbb{C} \) is
    an example.
  </li>
</ul>
<p>
  It is worth reiterating here that the fourth result in the summary
  above follows from the fact that every field is an integral domain.
  These results reveal how structure and size interact in algebraic
  systems.  It is interesting how simply being finite guarantees that
  an integral domain is a field.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/from-finite-integral-domains-to-finite-fields.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Lemma for FTGT</title>
<link>https://susam.net/lemma-for-ftgt.html</link>
<guid isPermaLink="false">udjib</guid>
<pubDate>Sun, 09 Mar 2025 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  This post illustrates a key lemma that is used in proving
  the <em>fundamental theorem of Galois theory</em> (FTGT).  Note that
  FTGT is not covered in this post.  The focus of this post is on
  understanding and proving this lemma only.  Here is the lemma from
  the book <em>Galois Theory</em>, 5th ed. by Stewart (2023):
</p>
<div class="highlight">
  <p>
    <strong>Lemma 12.1.</strong>
    <em>
      Suppose that \( L/K \) is a field extension, \( M \) is an
      intermediate field, and \( \tau \) is a \( K \)-automorphism of \(
      L.  \)  Then \( \tau M^* \tau^{-1} = \tau(M)^{*}.  \)
    </em>
  </p>
</div>
<p>
  The notation \( M^* \) denotes the group of all
  \( M \)-automorphisms of \( L \) with composition as the group
  operation.  Note that Stewart writes \( \tau(M)^{*} = \tau M^*
  \tau^{-1} \) while stating the lemma but I have reversed the LHS and
  RHS to maintain consistency with the equations that appear in the
  discussion below.
</p>
<p>
  To build intuition for this lemma, I'll first present an
  illustration, followed by a proof.  The discussion below assumes
  familiarity with field extensions and field automorphisms, as
  several notations and results from these areas will be used
  implicitly without detailed justification.  This post is meant to
  serve as a set of notes on the lemma, not a comprehensive tutorial.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#illustration">Illustration</a>
    <ul>
      <li><a href="#concrete-example">Concrete Example</a></li>
      <li><a href="#lhs-subset-of-rhs">LHS &sube; RHS</a></li>
      <li><a href="#lhs-superset-of-rhs">LHS &supe; RHS</a></li>
      <li><a href="#lhs-equals-rhs">LHS = RHS</a></li>
    </ul>
  </li>
  <li><a href="#proof">Proof</a></li>
</ul>
<h2 id="illustration">Illustration<a href="#illustration"></a></h2>
<h3 id="concrete-example">Concrete Example<a href="#concrete-example"></a></h3>
<p>
  Let \( L = \mathbb{Q}(\sqrt{2}, \sqrt{3}), \) \( K = \mathbb{Q} \)
  and \( M = \mathbb{Q}(\sqrt{2}).  \)  Note that

  \begin{align*}
    L &amp;= \{ a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6} : a, b, c, d \in \mathbb{Q} \}, \\
    M &amp;= \{ k + l \sqrt{2} : k, l \in \mathbb{Q} \}.
  \end{align*}

  Now the group of \( K \)-automorphisms of \( L \) is

  \[
    K^* = \{\phi_1, \phi_2, \phi_3, \phi_4 \}
  \]

  where each \( \phi_i \) is given by

  \begin{align*}
    \phi_1 &amp;:
    a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6} \mapsto
    a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6}, \\

    \phi_2 &amp;:
    a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6} \mapsto
    a - b \sqrt{2} + c \sqrt{3} - d \sqrt{6}, \\

    \phi_3 &amp;:
    a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6} \mapsto
    a + b \sqrt{2} - c \sqrt{3} - d \sqrt{6}, \\

    \phi_4 &amp;:
    a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6} \mapsto
    a - b \sqrt{2} - c \sqrt{3} + d \sqrt{6}.
  \end{align*}

  Then \( M^* = \{ \phi_1, \phi_3 \}.  \)  Let \( \tau = \phi_2.  \)
  Then

  \begin{align*}
  \tau(M)
    &amp;= \{ \tau(x) : x \in \mathbb{M} \} \\
    &amp;= \{ \tau(k + l \sqrt{2}) : k, l \in \mathbb{Q} \} \\
    &amp;= \{ k - l \sqrt{2} : k, l \in \mathbb{Q} \}.
  \end{align*}

  Note that in this case we ended up with \( \tau(M) = M \) but we
  will be careful not to utilise this fact.  We will ensure that the
  steps below work without assuming \( \tau(M) = M.  \)  Next we find

  \begin{equation}
    \tau(M)^* = \{ \phi_1, \phi_3 \}.
    \label{eq-tau-m-ast}
  \end{equation}

  Now

  \begin{align*}
    \tau M^* \tau^{-1}
    &amp;= \{ \tau \gamma \tau^{-1} : \gamma \in {M^*} \} \\
    &amp;= \{ \tau \phi_1 \tau^{-1}, \tau \phi_3 \tau^{-1} \}.
  \end{align*}

  Let us now find out how each element of \( \tau M^* \tau^{-1} \)
  transforms the elements of \( L.  \)  For all \( a + b \sqrt{2} + c
  \sqrt{3} + d \sqrt{6} \in L, \) we get

  \begin{align*}
    (\tau \phi_1 \tau^{-1})(a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6})
    &amp;= (\tau \phi_1)(a - b\sqrt{2} + c\sqrt{3} - d\sqrt{6}) \\
    &amp;= \tau (a - b\sqrt{2} + c\sqrt{3} - d\sqrt{6}) \\
    &amp;= a + b\sqrt{2} + c\sqrt{3} + d\sqrt{6}).
  \end{align*}

  Therefore

  \[
    \tau \phi_1 \tau^{-1} = \phi_1.
  \]

  Similarly,

  \begin{align*}
    (\tau \phi_3 \tau^{-1})(a + b \sqrt{2} + c \sqrt{3} + d \sqrt{6})
    &amp;= (\tau \phi_3)(a - b\sqrt{2} + c\sqrt{3} - d\sqrt{6}) \\
    &amp;= \tau (a - b\sqrt{2} - c\sqrt{3} + d\sqrt{6}) \\
    &amp;= a + b\sqrt{2} - c\sqrt{3} - d\sqrt{6}.
  \end{align*}

  Therefore

  \[
    \tau \phi_3 \tau^{-1} = \phi_3.
  \]

  We have shown that

  \begin{equation}
    \tau M^* \tau^{-1} = \{ \phi_1, \phi_3 \}.
    \label{eq-tau-coset}
  \end{equation}

  From \( \eqnref{eq-tau-m-ast}{1} \) and \( \eqnref{eq-tau-coset}{2} \)
  we see that

  \[
     \tau M^* \tau^{-1} = \tau(M)^*.
  \]

  Since we are working with a concrete example of \( \tau \) here, we
  know exactly how it behaves, so we succeeded in demonstrating the
  above equality.  However, in a general proof, \( \tau \) is going to
  be an arbitrary \( K \)-automorphism of \( L, \) so we cannot know
  exactly how it behaves and as a result, we cannot obtain the above
  equation directly.  Therefore, in a general proof, we we will first
  show that \( \tau M^* \tau^{-1} \subseteq \tau(M)^* \) and then we
  will show that \( \tau M^* \tau^{-1} \supseteq \tau(M)^* \) in order
  to prove the above equation.
</p>
<h3 id="lhs-subset-of-rhs">LHS &sube; RHS<a href="#lhs-subset-of-rhs"></a></h3>
<p>
  Once again, let us see how each element of \( \tau M^* \tau^{-1} \)
  transforms the elements of \( \tau(M).  \)  Note that this time we
  are not going to examine how they transform arbitrary elements of \(
  L.  \)  We are only going to see how they transform the elements of
  \( \tau(M).  \)  For all \( k - l \sqrt{2} \in \tau(M), \) we get

  \begin{align*}
    (\tau \phi_1 \tau^{-1})(k - l \sqrt{2})
    &amp;= (\tau \phi_1)(k + l \sqrt{2}) \\
    &amp;= \tau(k + l \sqrt{2}) \\
    &amp;= k - l \sqrt{2}.
  \end{align*}

  Similarly, for all \( k - l \sqrt{2} \in \tau(M), \) we get

  \begin{align*}
    (\tau \phi_3 \tau^{-1})(k - l \sqrt{2})
    &amp;= (\tau \phi_3)(k + l \sqrt{2}) \\
    &amp;= \tau(k + l \sqrt{2}) \\
    &amp;= k - l \sqrt{2}.
  \end{align*}

  Note above that both \( \phi_1 \) and \( \phi_3 \) fix \( k + l
  \sqrt{2} \in M \) because \( \phi_1, \phi_2 \in M^*, \) the set of
  \( M \)-automorphisms of \( L.  \)  This detail will be used in the
  general proof.
</p>
<p>
  Since both \( \tau \phi_1 \tau^{-1} \) and
  \( \tau \phi_3 \tau^{-1} \) fix the elements of \( \tau(M), \) they
  are both \( \tau(M) \)-automorphisms of \( L.  \)  Therefore \( \tau
  M^* \tau^{-1} \subseteq \tau(M)^{*}.  \)
</p>
<h3 id="lhs-superset-of-rhs">LHS &supe; RHS<a href="#lhs-superset-of-rhs"></a></h3>
<p>
  Consider the set \( \tau^{-1} \tau(M)^* \tau \) and examine how its
  elements transform the elements of \( M.  \)  For all \( k + l
  \sqrt{2} \in M, \) we get

  \begin{align*}
    (\tau^{-1} \phi_1 \tau)(k + l \sqrt{2})
    &amp;= (\tau^{-1} \phi_1)(k - \sqrt{2}) \\
    &amp;= \tau^{-1}(k - \sqrt{2}) \\
    &amp;= k + l \sqrt{2}.
  \end{align*}

  Similarly, for all \( k + l \sqrt{2} \in M, \) we get

  \begin{align*}
    (\tau^{-1} \phi_3 \tau)(k + l \sqrt{2})
    &amp;= (\tau^{-1} \phi_3)(k - \sqrt{2}) \\
    &amp;= \tau^{-1}(k - \sqrt{2}) \\
    &amp;= k + l \sqrt{2}.
  \end{align*}

  Here both \( \phi_1 \) and \( \phi_3 \) fix \( k - l \sqrt{2} \in
  \tau(M) \) because \( \phi_1, \phi_2 \in \tau(M)^*, \) the set of \(
  \tau(M) \)-automorphisms of \( L.  \)
</p>
<p>
  Since both \( \tau^{-1} \phi_1 \tau \) and
  \( \tau^{-1} \phi_3 \tau \) fix the elements of \( M, \) they are
  both \( M \)-automorphisms of \( L.  \)  Therefore \( \tau^{-1}
  \tau(M)^* \tau \subseteq M^* \) which implies \( \tau M^* \tau^{-1}
  \supseteq \tau(M)^*.  \)
</p>
<h3 id="lhs-equals-rhs">LHS = RHS<a href="#lhs-equals-rhs"></a></h3>
<p>
  The previous two sections complete the illustration of the lemma
  with the chosen example.  We have shown that \( \tau M^* \tau^{-1}
  \subseteq \tau(M)^{*} \) and \( \tau M^* \tau^{-1} \supseteq
  \tau(M)^*.  \)  Therefore \( \tau M^* \tau^{-1} = \tau(M)^*.  \)
</p>
<h2 id="proof">Proof<a href="#proof"></a></h2>
<p>
  The ideas presented in the previous sections will now be extended to
  formulate a general proof.  For clarity, the lemma is stated once
  again below before proceeding with the proof.
</p>
<p>
  <strong>Lemma 12.1.</strong>
  <em>
    Suppose that \( L/K \) is a field extension, \( M \) is an
    intermediate field, and \( \tau \) is a \( K \)-automorphism of \(
    L.  \)  Then \( \tau M^* \tau^{-1} = \tau(M)^{*}.  \)
  </em>
</p>
<p>
  <em>Proof.</em>

  For all \( \gamma \in M^*, \) \( x' \in \tau(M), \) we use the
  notation \( x = \tau^{-1}(x') \in M \) and get

  \[
    (\tau \gamma \tau^{-1})(x') = (\tau \gamma)(x) = \tau(x) = x'.
  \]

  In the second equality above, we have used the fact that \( \gamma
  \in M^* \) which implies that \( \gamma \) is an \( M \)-automorphism
  of \( L \) which allows us to conclude that \( \gamma(x) = x \) for
  \( x \in M.  \)  Since every \( \tau \gamma \tau^{-1} \in \tau M^*
  \tau^{-1} \) fixes all elements \( x' \in \tau(M), \) each \( \tau
  \gamma \tau^{-1} \) must be a \( \tau(M) \)-automorphism of \( L.  \)
  Thus \( \tau M^* \tau^{-1} \subseteq \tau(M)^*.  \)
</p>
<p>
  Similarly, for all \( \gamma' \in \tau(M)^*, \) \( x \in M, \) we
  use the notation \( x' = \tau(x) \in \tau(M) \) and get

  \[
  (\tau^{-1} \gamma' \tau)(x) = (\tau^{-1} \gamma')(x') = \tau^{-1}(x') = x.
  \]

  In the second equality above, we have used the fact that \( \gamma'
  \in \tau(M)^* \) which implies that \( \gamma' \) is an
  \( \tau(M) \)-automorphism of \( L \) which allows us to conclude
  that \( \gamma'(x') = x' \) for \( x' \in \tau(M).  \)  Since every
  \( \tau^{-1} \gamma' \tau \in \tau^{-1} \tau(M)^* \tau \) fixes all
  elements \( x \in M, \) each \( \tau^{-1} \gamma' \tau \) must be an
  \( M \)-automorphism of \( L.  \)  Thus \( \tau^{-1} \tau(M)^* \tau
  \subseteq M^*.  \)  This implies \( \tau M^* \tau^{-1} \supseteq
  \tau(M)^*.  \)
</p>
<p>
  We have shown that \( \tau M^* \tau^{-1} \subseteq \tau(M)^* \) and
  \( \tau M^* \tau^{-1} \supseteq \tau(M)^*.  \)  Therefore \( \tau M^*
  \tau^{-1} = \tau(M)^*.  \)
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/lemma-for-ftgt.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Function</title>
<link>https://susam.net/function.html</link>
<guid isPermaLink="false">talpc</guid>
<pubDate>Sun, 20 Oct 2024 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  In mathematics, a function \( f \) from a set \( X \) to a set \(
  Y \) is a relation that associates each element of \( X \) with
  exactly one element of \( Y.  \)  This page describes the commonly
  used notation, terminology and concepts pertaining to functions.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#definition">Definition</a></li>
  <li><a href="#notation">Notation</a></li>
  <li><a href="#domain-codomain-and-image">Domain, Codomain and Image</a></li>
  <li><a href="#injection-surjection-and-bijection">Injection, Surjection and Bijection</a></li>
</ul>
<h2 id="definition">Definition<a href="#definition"></a></h2>
<p>
  A function \( f \) from a set \( X \) to a set \( Y \) is a binary
  relation \( R \) that satisfies the following conditions:
</p>
<ul>
  <li>
    \( R \subseteq \{ (x, y) \mid x \in X, y \in Y \} = X \times Y.  \)
  </li>
  <li>
    For every \( x \in X, \) there exists \( y \in Y \) such that \(
    (x, y) \in R.  \)
  </li>
  <li>
    If \( (x, y) \in R \) and \( (x, z) \in R, \) then \( y = z.  \)
  </li>
</ul>
<p>
  The set \( X \) is called the domain of \( f \) and the set \( Y \)
  is called the codomain of \( f.  \)  The relation \( R \) is also
  known as the graph of \( f.  \)
</p>
<h2 id="notation">Notation<a href="#notation"></a></h2>
<p>
  Let \( f \) be a function from a set \( X \) to a set \( Y.  \)  Then
  the name \( f \) represents the function and the notation \( f(x) \)
  represents the application of the function to the argument \( x, \)
  i.e. \( f(x) \) represents the value of \( f \) for the element \( x
  \in X.  \)  In other words, for all \( x \in X, \) we have \( (x,
  f(x)) \in R.  \)
</p>
<p>
  A function \( f \) with domain \( X \) and codomain \( Y \) is also
  written as \( f : X \to Y.  \)  The function \( f \) may also be
  written as \( x \mapsto f(x).  \)  This notation specifies a function
  that maps \( x \) to \( f(x).  \)
</p>
<p>
  Formally, \( f(x) \) denotes the application of the function \( f \)
  to the argument \( x.  \)  However, in practice, it is common to use
  the expression \( f(x) \) to refer to both the function itself and
  its output for a given \( x, \) which is a slight deviation from
  strict notation.  Similarly, the function \( x \mapsto g(x) \) is
  often written as \( f(x) = g(x).  \)  For example, the function \( x
  \mapsto x^2 - 1 \) may also be written as \( f(x) = x^2 - 1.  \)
</p>
<p>
  Consider a function \( f \) that returns the square of a real
  number.  The following are common notations used to define this
  function, roughly ordered from the most formal form to the least
  formal one:
</p>
<ul>
  <li>
    \( f : \mathbb{R} \to \mathbb{R} ; \; x \mapsto x^2, \)
  </li>
  <li>
    \( f : \mathbb{R} \to \mathbb{R} : x \mapsto x^2, \)
  </li>
  <li>
    \( f : x \mapsto x^2, \)
  </li>
  <li>
    \( f: \mathbb{R} \to \mathbb{R} \) where \( f(x) = x^2, \)
  </li>
  <li>
    \( f(x) = x^2.  \)
  </li>
</ul>
<h2 id="domain-codomain-and-image">Domain, Codomain and Image<a href="#domain-codomain-and-image"></a></h2>
<p>
  The <em>domain</em> of a function is the set of all values for which
  the function is defined.
</p>
<p>
  A <em>codomain</em> of a function \( f \) is a set within which the
  values \( f(x) \) for all \( x \in X \) must lie, where \( X \) is
  the domain of \( f.  \)
</p>
<p>
  The <em>image</em> of a function \( f \) is the set \( \{ f(x) \mid
  x \in X \} \) where \( X \) is the domain of \( f.  \)
</p>
<p>
  The term <em>range</em> is often used as a synonym of image.
  However the use of this term is inconsistent across literature.
  Some old books use the term range to mean codomain while other books
  use this term to mean the image.  Therefore it is best to use the
  term image because it is free from such ambiguity.
</p>
<h2 id="injection-surjection-and-bijection">Injection, Surjection and Bijection<a href="#injection-surjection-and-bijection"></a></h2>
<p>
  A function \( f : X \to Y \) is <em>injective</em> if \( \forall a,
  b \in X, a \neq b \implies f(a) \neq f(b).  \)  A function is
  injective, if each element of the codomain is mapped to by <em>at
  most</em> one element of the domain.  An injective function is also
  known as a <em>one-to-one</em> function or an injection.
</p>
<p>
  A function \( f : X \to Y \) is <em>surjective</em> if \( \forall y
  \in Y, \exists x \in X \) such that \( y = f(x).  \)  A function is
  surjective, if each element of the codomain is mapped to by <em>at
  least</em> one element of the domain.  A surjective function is also
  known as an <em>onto</em> function or a surjection.
</p>
<p>
  A function \( f : X \to Y \) is <em>bijective</em> if \( \forall y
  \in Y, \) there exists exactly one \( x \in X, \) such that \( y =
  f(x).  \)  A function is bijective, if each element of the codomain
  is mapped to by <em>exactly</em> one element of the domain.  A
  bijective function is also known as a <em>one-to-one
  correspondence</em> or bijection.  A bijection is both injective and
  surjective.  In other words, a bijection is both <em>one-to-one and
  onto</em>.
</p>
<p>
  The function \( f : \mathbb{R} \to \mathbb{R}; \; x \mapsto e^x \)
  is injective but not surjective.  It is injective because distinct
  values of \( x \) produce distinct values of \( e^x.  \)  However, it
  is not surjective as no value in the domain maps to negative numbers
  in the codomain, leaving some elements in the codomain unmapped.
</p>
<p>
  The function \( f : \mathbb{R} \to \mathbb{R}; \; x \mapsto x^3 - x \)
  is surjective but not injective.  It is surjective because every
  value in the codomain is mapped to by at least one value in the
  domain.  However, it is not injective, as distinct values in the
  domain can map to the same value in the codomain.  For example, \(
  f(-1) = f(0) = f(1) = 0.  \)
</p>
<p>
  The function \( f : \mathbb{R} \to \mathbb{R}; \; x \mapsto x + 1 \)
  is bijective.  It is both injective and surjective.  This function
  is invertible with the inverse given by the function \( x \mapsto x
  - 1.  \)
</p>
<p>
  The function \( f : \mathbb{R} \to \mathbb{R}; \; x \mapsto x^2 \)
  is neither injective nor surjective.  First, the function is not
  injective because distinct values in the domain can map to the same
  value in the codomain.  For example, \( f(-2) = f(2) = 4.  \)
  Additionally, the function is not surjective because no value in the
  domain maps to the negative numbers in the codomain.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/function.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/definition.html">#definition</a>
</p>
]]>
</description>
</item>
<item>
<title>Real Analysis</title>
<link>https://susam.net/cc/real-analysis/</link>
<guid isPermaLink="false">ohpij</guid>
<pubDate>Sat, 27 Jul 2024 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h1>Real Analysis</h1>
<div class="highlight">
  <p>
    Meeting time: 19:00 UTC on Saturdays and Sundays
    usually.<sup>&dagger;</sup>
  </p>
  <p>
    Meeting duration: 40 minutes.
  </p>
  <p>
    Meeting link: <a href="../../meet/">susam.net/meet</a>
  </p>
  <p>
    Meeting log: <a href="log.html#upcoming">click here</a>
  </p>
  <p>
    Reference book:
    <a href="https://link.springer.com/book/10.1007/978-1-4471-0341-7"><em>Real
    Analysis</em></a> by John M. Howie (2001)
  </p>
  <p>
    Chapter notes: <a href="ch01.html">ch01.html</a>
  </p>
  <p>
    Club channel:
    <a href="https://app.element.io/#/room/#susam:matrix.org">#susam:matrix.org</a> /
    <a href="https://web.libera.chat/#susam">#susam:libera.chat</a><sup>&ddagger;</sup>
  </p>
  <p>
    Mastodon:
    <a href="https://mastodon.social/@susam">@susam@mastodon.social</a>
  </p>
  <p>
    Started: 27 Jul 2024
  </p>
</div>
<p>
  <small>&dagger; There are some exceptions to this schedule
  occasionally.  Join
  <a href="https://app.element.io/#/room/#susam:matrix.org">#susam:matrix.org</a>
  or <a href="https://web.libera.chat/#susam">#susam:libera.chat</a>
  or follow <a href="https://mastodon.social/@susam">@susam@mastodon.social</a>
  to receive schedule updates.</small>
</p>
<p>
  <small>&ddagger; You only need to join either the Matrix channel or
  the Libera channel, not both.  Both channels are bridged together.
  If you are not an active IRC user, prefer joining the Matrix channel
  because it is more convenient for someone unfamiliar with IRC.  For
  example, you can close your browser or client and your chat session
  will still stay alive on Matrix.  You can connect back the next day
  and catch up with the messages.  Doing that with IRC requires
  slightly more work such as setting up IRC bouncers etc.</small>
</p>
<p>
  The primary reference book for these meetings is
  <em>Real Analysis</em> written by the Scottish mathematician John
  Mackintosh Howie.
</p>
<p>
  These meetings are hosted by Susam and attended by some members of
  <code>##math</code> channel of Libera IRC network as well as by some
  members from <a href="https://news.ycombinator.com/">Hacker
  News</a>.
</p>
<p>
  You are welcome to join these meetings anytime.  If you are
  concerned that the meetings may not make sense if you join when we
  are in the middle of a chapter, please free to talk to us about it
  in the <a href="../#join">group channel</a>.  I can
  recommend the next best time to begin joining the meetings.
  Usually, it would be when we begin reading a new section or chapter
  that is fairly self-contained and does not depend a lot on material
  we have read previously.
</p>
<h2 id="faq">FAQ<a href="#faq"></a></h2>
<ol>
  <li>
    <h3 id="what-is-this-club-about">What is this club about?<a href="#what-is-this-club-about"></a></h3>
    <p>
      This is a hobby club with a focus on mathematics and
      computation.  This club picks reading material about concepts
      and technologies that have been around for a long time and have
      an air of timelessness around them.  See the blog
      post <a href="../../reading-classic-computation-books.html">Reading
      Classic Computation Books</a> for more details.
    </p>
  </li>
  <li>
    <h3 id="who-runs-this-club">Who runs this club?<a href="#who-runs-this-club"></a></h3>
    <p>
      My name is Susam.  This is my website.  I run this club.  I host
      the book club meetings.  The last two series of book club
      meetings I hosted were about <a href="../iant/">analytic
      number theory</a> and <a href="../mastering-emacs/">Emacs</a>.
      Some members of Libera IRC network and Hacker News participated
      in those meetings.  This new series is going to be about real
      analysis.
    </p>
  </li>
  <li>
    <h3 id="what-topics-are-you-going-to-discuss-in-the-meetings">What topics are you going to discuss in the meetings?<a href="#what-topics-are-you-going-to-discuss-in-the-meetings"></a></h3>
    <p>
      We will discuss the content of the reference book.  This
      includes topics like sequences and series, functions and
      continuity, differentiation and integration, Taylor's theorem,
      the Riemann integral, etc.
    </p>
  </li>
  <li>
    <h3 id="why-did-you-not-pick-rudins-text-for-the-meetings">Why did you not pick Rudin's text for the meetings?<a href="#why-did-you-not-pick-rudins-text-for-the-meetings"></a></h3>
    <p>
      The first series of book club meeting I organised focussed on
      the book <em>Introduction to Analytic Number Theory</em> written
      by Tom M. Apostol.  While the book excelled in rigour, some
      members, especially those without a strong mathematics
      background, found the constant alternation between theorems and
      proofs a bit too dry.  Personally, I thoroughly enjoyed it.
      See <a href="../../journey-to-prime-number-theorem.html">Journey
      to Prime Number Theorem</a> for a related post.
    </p>
    <p>
      For the current series of meetings, I have chosen a more
      lightweight book that is easy to read.  Howie's relaxed writing
      style in this book seems enjoyable.  I do not know yet if this
      ease of reading comes at the cost of rigour.  I might be able to
      assess that better as we make more progress through this book.
    </p>
  <li>
    <h3 id="have-you-read-the-book">Have you read the entire book?<a href="#have-you-read-the-book"></a></h3>
    <p>
      No, I have read the first few chapters of the book.  This means
      that I am a few chapters ahead of someone who has just begun
      reading this book, so I can maintain a steady pace in the club
      discussions and also be able to tell if some topics or areas of
      confusion in an earlier chapter will be clarified in a later
      chapter.
    </p>
  </li>
  <li>
    <h3 id="what-is-planned">What is planned for the next few meetings?<a href="#what-is-planned"></a></h3>
    <p>
      See the <a href="log.html">meeting log</a> which contains a
      rough plan for the next few meetings along with an archive of
      all previous meetings.
    </p>
  </li>
  <li>
    <h3 id="do-i-need-to-read-in-advance">Do I need to read the planned chapters/pages in advance before coming to the meetings?<a href="#do-i-need-to-read-in-advance"></a></h3>
    <p>
      Not at all.  It is up to you, really.  If you would like to read
      the chapters in advance and come, that's great.  But it is not
      necessary.  We are going to discuss every page of the book in
      detail anyway.
    </p>
  </li>
  <li>
    <h3 id="can-i-lurk">Can I just lurk in the meetings?<a href="#can-i-lurk"></a></h3>
    <p>
      Yes!  Lurking is absolutely fine in our club meetings.  In fact,
      most members of the club join in and stay silent throughout the
      meetings.  Only a few members talk via audio/video or chat.
      This is considered absolutely normal in this club, so please do
      not hesitate to join our meetings!
    </p>
  </li>
  <li>
    <h3 id="where-can-i-ask-questions">I have more questions.  Where can I ask?<a href="#where-can-i-ask-questions"></a></h3>
    <p>
      Join the club channel
      at <a href="https://app.element.io/#/room/#susam:matrix.org">#susam:matrix.org</a>
      or <a href="https://web.libera.chat/#susam">#susam:libera.chat</a>
      to ask more questions.
    </p>
  </li>
</ol>
<!-- ### -->
<p>
  <a href="https://susam.net/cc/real-analysis/">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/real-analysis.html">#real-analysis</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Perron's Paradox</title>
<link>https://susam.net/perrons-paradox.html</link>
<guid isPermaLink="false">skrsn</guid>
<pubDate>Wed, 10 Apr 2024 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Oskar Perron, a German mathematician, introduced Perron's paradox to
  illustrate the danger of assuming the existence of a solution to an
  optimisation problem.  The paradox works like this:
</p>
<div class="highlight">
  Let \( n \) be the largest positive integer.  Then either \( n =
  1 \) or \( n \gt 1.  \)  If \( n \gt 1, \) then \( n^2 \gt n, \)
  contradicting the definition of \( n.  \)  Hence \( n = 1.  \)
</div>
<p>
  We get this absurd result because of the incorrect assumption that
  there exists an integer that is the largest of all the integers.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/perrons-paradox.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Logarithm Notation</title>
<link>https://susam.net/logarithm-notation.html</link>
<guid isPermaLink="false">ipgnh</guid>
<pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  We know that the natural logarithm of a number \( x, \) i.e. the
  logarithm of \( x \) to the base \( e, \) is sometimes denoted as \(
  \ln x.  \)  It has other notations too.  For example, many
  mathematics textbooks just use the notation \( \log x \) after
  establishing once that this notation denotes the natural logarithm.
  The most descriptive notation is perhaps \( \log_e x \) but this is
  most definitely an overkill.  I have never seen any serious textbook
  use this notation.
</p>
<p>
  Let us focus on \( \ln x \) again.  Is it not peculiar?  What does
  \( \ln \) stand for really?  Logarithm natural?  Sounds very unnatural.
</p>
<p>
  Well, as a kid I learnt that \( \ln \) here stands for the Latin
  phrase "logarithmus naturalis".  It is only recently that I bothered
  to verify if this expansion of \( \ln x \) that I learnt as a kid is
  really true.  The most credible discussion of this that I could find
  online is this thread on Mathematics Stack Exchange:
  <a href="https://math.stackexchange.com/q/1694">math.stackexchange.com/q/1694</a>.
  The answer by Dan Velleman points us to page 277 of an 1875
  book <em>Lehrbuch der Mathematik</em> by Anton Steinhauser.  Quoting
  the relevant portion from the page:
</p>
<blockquote>
  Man pflegt nun, um Verwechslungen dieser beiden Systeme vorzubeugen,
  mit log.nat. a (gesprochen: logarithmus naturalis a) oder ln. a,
  oder am einfachsten mit la den natrlichen, mit log.brigg. a
  (gesprochen: Logarithmus briggus a) oder log.a, oder am einfachsten
  mit lg. a den gemeinen Logarithmus (von a) zu bezeichnen.
</blockquote>
<p>
  Translated to English, it says:
</p>
<blockquote>
  One is accustomed now, in order to prevent confusion between these
  two systems, to use log.nat. a (pronounced: logarithmus naturalis a)
  or ln. a, or most simply la for the natural, and log.brigg. a
  (pronounced: logarithmus briggus a) or log. a, or most simply lg. a
  to denote the common logarithm (of a).
</blockquote>
<p>
  So it does look like what I learnt as a kid is correct and the
  earliest possible reference of this the Internet is able to find for
  us is the 1875 book quoted above.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/logarithm-notation.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Thurston's Paean</title>
<link>https://susam.net/thurstons-paean.html</link>
<guid isPermaLink="false">iipnj</guid>
<pubDate>Tue, 18 Jul 2023 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  I recently came across a beautiful and thoughtful answer on
  MathOverflow by the late mathematician William Thurston.  A brief
  background about him from
  the <a href="https://en.wikipedia.org/wiki/William_Thurston">Wikipedia
  article</a> about him:
</p>
<blockquote>
  <p>
    William Paul Thurston (October 30, 1946 &ndash; August 21, 2012)
    was an American mathematician.  He was a pioneer in the field of
    low-dimensional topology and was awarded the Fields Medal in 1982
    for his contributions to the study of 3-manifolds.
  </p>
  <p>
    Thurston was a professor of mathematics at Princeton University,
    University of California at Davis and Cornell University.  He was
    also a director of the Mathematical Sciences Research Institute.
  </p>
</blockquote>
<p>
  MathOverflow makes all answers posted to the website available under
  a Creative Commons licence.  In particular, all answers posted
  before 08 Apr 2011 (UTC) are available under the terms of the
  Creative Commons Attribution-ShareAlike 2.5 Generic (CC BY-SA 2.5)
  licence.  Thurston wrote the answer I am about to share on 30 Oct
  2010.  Due to the licence terms, this post too is available under
  the terms of the same licence.
</p>
<p>
  Thurston posted his answer while replying to a MathOverflow
  question:
  <a href="https://mathoverflow.net/q/43690"><em>What's a
  mathematician to do?</em></a>.  The question enquires about how an
  ordinary mathematician can contribute to mathematics.  Thurston's
  answer
  from <a href="https://mathoverflow.net/a/44213">mathoverflow.net/a/44213</a>
  is reproduced below:
</p>
<blockquote>
  <p>
    It's not <em>mathematics</em> that you need to contribute to.
    It's deeper than that: how might you contribute to humanity, and
    even deeper, to the well-being of the world, by pursuing
    mathematics?  Such a question is not possible to answer in a
    purely intellectual way, because the effects of our actions go far
    beyond our understanding.  We are deeply social and deeply
    instinctual animals, so much that our well-being depends on many
    things we do that are hard to explain in an intellectual way.
    That is why you do well to follow your heart and your passion.
    Bare reason is likely to lead you astray.  None of us are smart
    and wise enough to figure it out intellectually.
  </p>
  <p>
    The product of mathematics is clarity and understanding.  Not
    theorems, by themselves.  Is there, for example any real reason
    that even such famous results as Fermat's Last Theorem, or the
    Poincar&eacute; conjecture, really matter?  Their real importance
    is not in their specific statements, but their role in challenging
    our understanding, presenting challenges that led to mathematical
    developments that increased our understanding.
  </p>
  <p>
    The world does not suffer from an oversupply of clarity and
    understanding (to put it mildly).  How and whether specific
    mathematics might lead to improving the world (whatever that
    means) is usually impossible to tease out, but mathematics
    collectively is extremely important.
  </p>
  <p>
    I think of mathematics as having a large component of psychology,
    because of its strong dependence on human minds.  Dehumanized
    mathematics would be more like computer code, which is very
    different.  Mathematical ideas, even simple ideas, are often hard
    to transplant from mind to mind.  There are many ideas in
    mathematics that may be hard to get, but are easy once you get
    them.  Because of this, mathematical understanding does not expand
    in a monotone direction.  Our understanding frequently
    deteriorates as well.  There are several obvious mechanisms of
    decay.  The experts in a subject retire and die, or simply move on
    to other subjects and forget.  Mathematics is commonly explained
    and recorded in symbolic and concrete forms that are easy to
    communicate, rather than in conceptual forms that are easy to
    understand once communicated.  Translation in the direction
    conceptual -&gt; concrete and symbolic is much easier than
    translation in the reverse direction, and symbolic forms often
    replaces the conceptual forms of understanding.  And mathematical
    conventions and taken-for-granted knowledge change, so older texts
    may become hard to understand.
  </p>
  <p>
    In short, mathematics only exists in a living community of
    mathematicians that spreads understanding and breaths life into
    ideas both old and new.  The real satisfaction from mathematics is
    in learning from others and sharing with others.  All of us have
    clear understanding of a few things and murky concepts of many
    more.  There is no way to run out of ideas in need of
    clarification.  The question of who is the first person to ever
    set foot on some square meter of land is really secondary.
    Revolutionary change does matter, but revolutions are few, and
    they are not self-sustaining --- they depend very heavily on the
    community of mathematicians.
  </p>
</blockquote>
<p>
  In the comments to the answer, one of the commenters
  was <a href="https://users.cs.utah.edu/~suresh/">Suresh
  Venkatasubramanian</a> who was a professor in the School of
  Computing at the University of Utah back then.  He
  is <a href="https://vivo.brown.edu/display/suresh">now</a> a
  professor of Computer Science and Data Science at Brown University.
  In his <a href="https://mathoverflow.net/questions/43690/whats-a-mathematician-to-do/44213#comment271029_44213">comment</a>,
  Suresh proposed that this answer be called <em>Thurston's
  Paean</em>.  Here is his complete comment:
</p>
<blockquote>
  <p>
    This seems like an ideal counterpoint to Hardy's Lament.  I'm
    calling it Thurston's Paean :).  Seems poignant now that he has
    passed.
  </p>
</blockquote>
<p>
  Thurston's answer does appear to be a perfect complement to Hardy's
  lament in the 1940 essay <em>A Mathematician's Apology</em>.  While
  Hardy's lament is remarkably beautiful and introspective, it may
  also feel a little depressing at places.  Thurston's post on the
  other hand is full of hope and purpose that goes beyond the actual
  work of doing mathematics.  Indeed <em>Thurston's Paean</em> is a
  befitting title for his answer.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/thurstons-paean.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/miscellaneous.html">#miscellaneous</a> |
  <a href="https://susam.net/tag/quote.html">#quote</a>
</p>
]]>
</description>
</item>
<item>
<title>Integrating Factor</title>
<link>https://susam.net/integrating-factor.html</link>
<guid isPermaLink="false">cczvm</guid>
<pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  One of the many techniques for solving ordinary differential
  equations involves using an <em>integrating factor</em>.  An
  integrating factor is a function that a differential equation is
  multiplied by to simplify it and make it integrable.  It almost
  appears to work like magic!
</p>
<h2 id="method">The Method<a href="#method"></a></h2>
<p>
  Let us first see how the integrating factor method works.  In this
  post, we will work with linear first-order ordinary differential
  equations of type

  \[
    \frac{dy}{dx} + y P(x) = Q(x)
  \]

  to discuss, reason about and illustrate this method.  We will also
  often use the Leibniz's notation \( dy/dx \) and the Lagrange's
  notation \( y'(x) \) or simply \( y' \) interchangeably as is
  typical in calculus.  They all mean the same thing: the derivative
  of the function \( y \) with respect to \( x.  \)  Thus the above
  differential equation may also be written as

  \[
    y' + y P(x) = Q(x).
  \]

  Given a differential equation of this form, we first find an
  integrating factor \( M(x) \) using the formula

  \[
    M(x) = e^{\int P(x) \, dx}.
  \]

  Then we multiply both sides of the differential equation by this
  integrating factor.  Now remarkably, the left-hand side (LHS)
  reduces to a single term consisting only of a derivative.  As a
  result, we can get rid of that derivative by integrating both sides
  of the equation and we then proceed to obtain a solution.
</p>
<h2 id="example">An Example<a href="#example"></a></h2>
<p>
  Here is an example that demonstrates the method of using an
  integrating factor.  Let us say we want to solve the differential
  equation

  \[
    y' + y \left( \frac{x + 1}{x} \right) = \frac{1}{x}.
  \]

  Indeed this is in the form \( y' + y P(x) = Q(x) \) with \( P(x) =
  (x + 1)/x \) and \( Q(x) = 1/x.  \)  We first obtain the integrating
  factor

  \[
    M(x)
    = e^{\int P(x) \, dx}
    = e^{\int (x + 1)/x \, dx}
    = e^{\int (1 + 1/x) \, dx}
    = e^{x + \ln x}
    = x e^x.
  \]

  Now we multiply both sides of the differential equation by this
  integrating factor and get

  \[
    y' x e^x + y (x + 1) e^x = e^x.
  \]

  The LHS can now be simplified to \( \frac{d}{dx} (y x e^x).  \)  This
  can be verified using the product rule for derivatives.  This
  simplification of the LHS is the remarkable feature of this method.
  Therefore the above equation can be written as

  \[
    \frac{d}{dx} (y x e^x) = e^x.
  \]

  Note that the expression on the LHS is a product of the function \(
  y \) and the integrating factor \( x e^x.  \)  We will discuss this
  observation in more detail a little later.  Let us first complete
  solving this differential equation.  Since the LHS is now a single
  term that consists of a derivative, obtaining a solution now simply
  involves integrating both sides with respect to \( x.  \)
  Integrating both sides we get

  \[
    y x e^x = e^x + C
  \]

  where \( C \) is the constant of integration.  Finally, we divide
  both sides by the integrating factor \( x e^x \) to get

  \[
    y = \frac{1}{x} + \frac{C}{x e^x}.
  \]

  We have now obtained a solution for the differential equation.  If
  we review the steps above, we will find that after multiplying both
  sides of the given differential equation by the integrating factor,
  the differential equation becomes significantly simpler and
  integrable.  In fact, after multiplying both sides of the given
  differential equation by the integrating factor, the LHS always
  becomes the derivative of the product of the function \( y \) and
  the integrating factor.  We will now see why this is so.
</p>
<h2 id="interesting-relationship">An Interesting Relationship<a href="#interesting-relationship"></a></h2>
<p>
  Consider once again the linear first-order differential equation

  \begin{equation}
    \label{eq-if-diff}
    y' + yP(x) = Q(x).
  \end{equation}

  We first find the integrating factor

  \begin{equation}
    \label{eq-if-integrating-factor}
    M(x) = e^{\int P(x)\, dx}.
  \end{equation}

  The integrating factor obtained like this satisfies an interesting
  relationship:

  \begin{equation}
    \label{eq-if-property}
    M'(x) = M(x) P(x).
  \end{equation}

  We can prove this relationship easily by differentiating both sides
  of \( \eqnref{eq-if-integrating-factor}{2} \) as follows:

  \[
    M'(x)
    = \frac{d}{dx} \left( e^{\int P(x)\, dx} \right)
    = e^{\int P(x)\, dx} \frac{d}{dx} \left( \int P(x)\, dx \right)
    = M(x) P(x).
  \]

  Note that we use the chain rule to work out the derivative above.
  This beautiful result is due to how the derivative of the
  exponential function works.  When we apply the chain rule to obtain
  the derivative of \( e^{f(x)} \) we get

  \[
    \frac{d}{dx} e^{f(x)} = e^{f(x)} f'(x).
  \]

  This nice property of the exponential function leads to the
  interesting relationship in \( \eqnref{eq-if-property}{3}.  \)
</p>
<h2 id="simplification-of-lhs">Simplification of LHS<a href="#simplification-of-lhs"></a></h2>
<p>
  Now let us multiply both sides of the differential equation \(
  \eqnref{eq-if-diff}{1} \) by the integrating factor \( M(x) \) By
  doing so, we get

  \[
    y' M(x) + y P(x) M(x) = Q(x) M(x).
  \]

  But from \( \eqnref{eq-if-property}{3} \) we know that \( P(x) M(x)
  = M'(x), \) so the above equation can be written as

  \[
    y' M(x) + y M'(x) = Q(x) M(x).
  \]

  Look what we have got on the LHS!  We have the expansion of \(
  \frac{d}{dx}(yM(x)) \) on the LHS.  By product rule of
  differentiation, we have
  \( \frac{d}{dx}(yM(x)) = y' M(x) + y M'(x).  \)  Therefore the above
  equation can be written as

  \[
    \frac{d}{dx}(yM(x)) = Q(x) M(x).
  \]

  The "magic" has occurred here!  Multiplying both sides of the
  differential equation by the integrating factor has led us to an
  equation that has got a single derivative only on the LHS.  As a
  result, finding the solution is now a simple matter of integrating
  both sides, i.e.

  \[
    y M(x) = \int Q(x) M(x) \, dx.
  \]

  Thus

  \[
    y = \frac{1}{M(x)} \int Q(x) M(x) \, dx.
  \]

  Note that the result of indefinite integral on the RHS will contain
  the constant of integration, which we will denote as \( C, \) so the
  final solution looks like

  \begin{equation}
    \label{eq-if-general-solution}
    y = \frac{1}{M(x)} \int Q(x) M(x) \, dx + \frac{C}{M(x)}.
  \end{equation}
</p>
<h2 id="illustration">Illustration<a href="#illustration"></a></h2>
<p>
  Let us illustrate the method and its magic with a very simple
  differential equation:

  \[
    y' + \frac{y}{x} = x.
  \]

  First we note that this equation is in the form
  \( y' + yP(x) = Q(x) \) with \( P(x) = 1/x \) and \( Q(x) = x.  \)
  We then find the integrating factor

  \[
  M(x)
  = e^{\int P(x) \, dx}
  = e^{\int \frac{1}{x} \, dx}
  = e^{\ln x}
  = x.
  \]

  Then we multiply both sides of the differential equation by the
  integrating factor to get

  \[
    y'x + y = x^2.
  \]

  Now indeed the LHS can be written down as a single derivative as
  shown below:

  \[
    \frac{d}{dx} yx = x^2.
  \]

  Note that the LHS is the derivative of the product of \( y \) and
  the integrating factor \( x.  \)  This is exactly what we discussed
  in the previous section.  We integrate both sides of the above
  equation to get

  \[
    yx = \frac{x^3}{3} + C.
  \]

  Finally we divide both sides by the integrating factor \( x \) to
  get

  \[
    y = \frac{x^2}{3} + \frac{C}{x}.
  \]

  We have arrived at the solution \( y(x) \) for the differential
  equation.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  In this post, we used very simple and convenient differential
  equations that led to nice closed-form solutions.  In practice,
  differential equations can be quite complicated and may not always
  lead to closed-form solutions.  In such cases, we leave the result
  in the form of an expression that contains an unsolved integral.
  Such solutions may resemble the form shown in
  \( \eqnref{eq-if-general-solution}{4}.  \)
</p>
<p>
  The method of using integrating factors to solve differential
  equations can also be extended to linear higher-order differential
  equations.  That is something we did not discuss in this post.
  However, I hope that the intuition gained from understanding how and
  why this method works for linear first-order differential equations
  will be useful while studying such extensions of this method.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/integrating-factor.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>GCD Grid</title>
<link>https://susam.net/gcd-grid.html</link>
<guid isPermaLink="false">rxbih</guid>
<pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  I recently completed reading the book <em>Introduction to Analytic
  Number Theory</em> written by Tom M. Apostol and published in 1976.
  It is a fantastic book that takes us through a breathtaking journey
  of analytic number theory.  The journey begins with simple
  properties of divisibility and ends with integer partitions.  During
  this journey, we learn about several fascinating concepts such as
  the M&ouml;bius function, Dirichlet multiplication, Chebyshev's
  functions, Dirichlet characters, quadratic residues, the Riemann
  zeta function, etc.  An analytic proof of the prime number theorem
  is also presented in the book.
</p>
<p>
  One of the things about the book that caught my interest from the
  very beginning was its front cover.  It has a peculiarly drawn grid
  of white boxes and red empty regions that looks quite interesting.
  Here is the grid from the front cover of the book:
</p>
<figure>
  <a href="files/blog/iant-cover.png"><img
      src="files/blog/iant-cover.png"
      alt="A diagram of a grid with cells and empty region"></a>
  <figcaption>
    Diagram of a grid on the front cover of the book <em>Introduction
    to Analytic Number Theory</em>
  </figcaption>
</figure>
<p>
  Can we come up with a simple and elegant rule that defines this
  grid?  Here is one I could come up with:
</p>
<div class="highlight">
  <em>
    Number the columns in the grid 0, 1, 2 and so on from left to
    right.  Number the rows in the grid 0, 1, 2 and so on from bottom
    to top.  Let \( (x, y) \) represent the cell at column \( x \) and
    row \( y.  \)  Then a box exists at \( (x, y) \) if and only if \(
    \gcd(x, y) \ne 1.  \)
  </em>
</div>
<p>
  We define \( \gcd(x, y) \) to be a nonnegative common divisor of \(
  x \) and \( y \) such that every common divisor of \( x \) and
  \( y \) also divides \( \gcd(x, y).  \)  Let us now see if we can
  explain some of the interesting properties of this grid using the
  above rule:
</p>
<ol>
  <li>
    <p>
      When \( x = 0 \) and \( y \ne 1, \) we get \( \gcd(x, y) =
      \lvert y \rvert \ne 1, \) so the entire column at \( x = 0 \)
      has boxes except at \( (0, 1).  \)  Similarly, the entire row at
      \( y = 0 \) has boxes except at \( (1, 0).  \)
    </p>
  </li>
  <li>
    <p>
      The cell \( (0, 0) \) has a box because \( \gcd(0, 0) \ne 1.  \)
      In fact, \( \gcd(0, 0) = 0.  \)  This follows from the definition
      of the \( \gcd \) function.  We will discuss this in more detail
      later in this post.
    </p>
  </li>
  <li>
    <p>
      Every diagonal cell \( (x, x) \) has a box except at
      \( (1, 1) \) because \( \gcd(x, x) = \lvert x \rvert \) for all
      integers \( x.  \)
    </p>
  </li>
  <li>
    <p>
      The grid is symmetric about the diagonal cells \( (x, x) \)
      because \( \gcd(x, y) = \gcd(y, x).  \)
    </p>
  </li>
  <li>
    <p>
      A column at \( x \) has exactly one cell below the diagonal if
      and only if \( x \) is prime.  For example, check the column for
      \( x = 5.  \)  It has exactly one cell below the diagonal.  We
      know that \( 5 \) is prime.  Now check the column for
      \( x = 6.  \)  It has four cells below the diagonal.  We know
      that \( 6 \) is not prime.
    </p>
  </li>
</ol>
<p>
  Let us now elaborate the second point in the list above.  If \(
  \gcd(0, 0) \) is \( 0, \) then \( 0 \) must divide \( 0.  \)  Does \(
  0 \) really divide \( 0?  \)  Isn't \( 0/0 \) undefined?  Yes, even
  though \( 0/0 \) is undefined, \( 0 \) divides \( 0.  \)  We say an
  integer \( d \) divides an integer \( n \) when \( n = cd \) for
  some integer \( c.  \)  We have \( 0 = 0 \cdot 0, \) so indeed
  \( 0 \) divides \( 0.  \)
</p>
<p>
  We have shown that \( 0 \) divides \( 0 \) but we have not shown yet
  that \( \gcd(0, 0) = 0.  \)  Is \( \gcd(0, 0) \) really \( 0?  \)
  Every integer divides \( 0, \) e.g. \( 1 \) divdes \( 0, \) \( 2 \)
  divides \( 0, \) \( 3 \) divides \( 0, \) etc.  There does not seem
  to be a greatest common divisor of \( 0 \) and \( 0.  \)  Shouldn't
  \( \gcd(0, 0) \) be called either infinity or undefined?  No, we
  need to look at the definition of \( \gcd \) introduced earlier.  As
  per the definition, every common divisor of integers \( x \) and \(
  y \) must also divide \( \gcd(x, y).  \)  With this requirement in
  mind, we see that \( \gcd(0, 0) \) must be \( 0.  \)  This definition
  also makes \( \gcd(n, 0) = \gcd(0, n) = \lvert n \rvert \) for all
  integers \( n.  \)  Further, this definition makes B&eacute;zout's
  identity hold for all integers.  B&eacute;zout's identity states
  that there exists integers \( m \) and \( n \) such that \( mx + ny
  = \gcd(x, y).  \)  Indeed if we have \( \gcd(0, 0) = 0, \) we get \(
  0 \cdot 0 + 0 \cdot 0 = 0 = \gcd(0, 0).  \)
</p>
<p>
  That's all I wanted to share about the front cover of the book.
  While the front cover is quite interesting, the content of the book
  is even more fascinating.  I found chapters 12 and 13 of the book to
  be the most interesting.  In chapter 12, the book teaches how to
  prove that the Riemann zeta function \( \zeta(s) \) vanishes at
  every negative even integer \( s.  \)  Through several contour
  integrals and clever use of Cauchy's residue theorem, it shows in
  the end that \( \zeta(-2n) = 0 \) for \( n = 1, 2, 3, \dots.  \)  In
  chapter 13, the book shows us how to obtain zero-free regions where
  \( \zeta(s) \) does not vanish.  The book exposes various subtle
  nuances of the zeta function with great rigour and thoroughness.
  Results like \( \zeta(-1) = -1/12 \) that once felt mysterious look
  crystal clear and obvious after working through this book.  I
  strongly recommend this book to anyone who wants to learn analytic
  number theory.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/gcd-grid.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a>
</p>
]]>
</description>
</item>
<item>
<title>Final IANT Meeting Today</title>
<link>https://susam.net/final-iant-meeting.html</link>
<guid isPermaLink="false">qegsg</guid>
<pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  We have been reading the book <em>Introduction to Analytic Number
  Theory</em> by Apostol (1976) since March 2021.  It has been going
  consistently since then and the previous few posts on this blog
  provide an account of how this journey has been so far.  After about
  seven months of reading this book together, we are having our final
  meeting for this book today.  This is going to be
  the <a href="cc/iant/log.html#120">120th meeting</a> of our book
  discussion group.  The meeting notes from all previous reading
  sessions are archived at
  <a href="cc/iant/">IANT Notes</a>.  We will discuss the
  final two pages of this book today and complete reading this book.
</p>
<p>
  In the meeting today, we will look at some applications of the
  recursion formula related to partition functions that we learnt
  earlier.  Here is an excerpt from the book that shows a specific
  example that demonstrates the richness and beauty of concepts one
  can discover while studying analytic number theory:
</p>
<blockquote>
  Equation (24) becomes

  \[
  np(n) = \sum_{k=1}^n \sigma(k) p(n - k).
  \]

  a remarkable relation connecting a function of multiplicative number
  theory with one of additive number theory.
</blockquote>
<p>
  Now what equation (24) contains is not important for this post.  Of
  course, you can refer to the book if you really want to know what
  equation (24) is.  We learnt to prove that equation in the
  penultimate meeting for this subject yesterday.  In this post, I
  will emphasise how indeed this equation is remarkable.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#divisor-sum-function">The Divisor Sum Function</a></li>
  <li><a href="#unrestricted-partition-function">The Unrestricted Partition Function</a></li>
  <li><a href="#linkage-of-two-theorems">The Linkage of Two Theories</a></li>
  <li><a href="#final-meeting">The Final Meeting</a></li>
  <li><a href="#thanks">Thanks!</a></li>
</ul>
<h2 id="divisor-sum-function">The Divisor Sum Function<a href="#divisor-sum-function"></a></h2>
<p>
  The divisor sum function \( \sigma(n) \) represents the sum of all
  positive divisors of \( n.  \)  Here are some examples:

  \begin{align*}
  \sigma(1) &amp;= 1, \\
  \sigma(2) &amp;= 1 + 2 = 3, \\
  \sigma(3) &amp;= 1 + 3 = 4, \\
  \sigma(4) &amp;= 1 + 2 + 4 = 7, \\
  \sigma(5) &amp;= 1 + 5 = 6.
  \end{align*}

  We have spent a good amount of time with this function in the
  initial chapters of the book.  However, for the purpose of this blog
  post, the definition and the examples above are good enough.
</p>
<h2 id="unrestricted-partition-function">The Unrestricted Partition Function<a href="#unrestricted-partition-function"></a></h2>
<p>
  The \( p(n) \) function is the unrestricted partition function.  It
  represents the number of ways \( n \) can be written as a sum of
  positive integers \( \le n.  \)  Further, we let \( p(0) = 1.  \)
  Here are some examples:

  \begin{align*}
  p(1) &amp;= 1, \\
  p(2) &amp;= 2, \\
  p(3) &amp;= 3, \\
  p(4) &amp;= 4, \\
  p(5) &amp;= 7.
  \end{align*}

  Let me illustration the last value.  The integer \( 5 \) can be
  represented as a sum of positive integers \( \le 5 \) in 7 different
  ways.  They are: \( 5, \) \( 4 + 1, \) \( 3 + 2, \) \( 3 + 1 + 1, \)
  \( 2 + 2 + 1, \) \( 2 + 1 + 1 + 1 \) and \( 1 + 1 + 1 + 1 + 1.  \)
  Thus \( p(n) = 5.  \)
</p>
<h2 id="linkage-of-two-theorems">The Linkage of Two Theories<a href="#linkage-of-two-theorems"></a></h2>
<p>
  The divisor sum function comes from multiplicative number theory.
  The partition function comes from additive number theory.  Yet these
  two very different things get linked together in the formula
  mentioned in the excerpt included above.  Here is the formula once
  again:

  \[
    np(n) = \sum_{k=1}^n \sigma(k) p(n - k).
  \]

  How beautiful!  How nicely the divisor sum function and the
  unrestricted partition function appear together elegantly in a
  single equation!  Further, this equation provides a recursion
  formula for the partition function.

  Here is an illustration of this equation with \( n = 5 \):

  \[
    5 \cdot p(5) = 5 \cdot 7 = 35.
  \]

  \begin{align*}
    \sum_{k=1}^5 \sigma(k) p(5 - k)
    &amp;= \sigma(1) p(4) + \sigma(2) p(3) + \sigma(3) p(2) + \sigma(4) p(1) + \sigma(5) p(0) \\
    &amp;= (1)(5) + (3)(3) + (4)(2) + (7)(1) + (6)(1) \\
    &amp;= 5 + 9 + 8 + 7 + 6 \\
    &amp;= 35.
  \end{align*}

  We will go through this topic once more in the meeting today, so if
  you are interested to see this formula worked out in a step-by-step
  manner, do join our final meeting for this book.
</p>
<h2 id="final-meeting">The Final Meeting<a href="#final-meeting"></a></h2>
<p>
  The final meeting is coming up at 17:00 UTC today.  Visit
  the <a href="cc/iant/">analytic number theory page</a> to
  get the meeting link.  This is not going to be the final meeting for
  our overall book discussion group though.  This is going to be the
  finally meeting for only the analytic number theory book.  We will
  have more meetings for another book after a short break.
</p>
<p>
  The meeting today is going to be a lightweight session.  The last
  two pages that we will discuss today contain some examples of
  recursion formulas and some commentary about Ramanujan's partition
  identities.  Most of it should make sense even to those who have not
  been part of our meetings earlier, so everyone is welcome to join
  this meeting today, even if only to lurk.  You can also join our
  group by joining our IRC channel where we will publish updates about
  future meetings.  Our channel details are available in the
  <a href="cc/#join">main page here</a>.
</p>
<h2 id="thanks">Thanks!<a href="#thanks"></a></h2>
<p>
  A big thank you to the Hacker News community and the Libera IRC
  mathematics and algorithms communities who showed interest in these
  meetings, joined the meetings and made this series of meetings
  successful.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/final-iant-meeting.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Journey to Integer Partitions</title>
<link>https://susam.net/journey-to-integer-partitions.html</link>
<guid isPermaLink="false">zktck</guid>
<pubDate>Sat, 18 Sep 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="introduction">Introduction<a href="#introduction"></a></h2>
<p>
  After <a href="cc/iant/log.html#114">114 meetings</a> and 75 hours
  of studying together, our analytic number theory book discussion
  group has finally reached the final chapter of the
  book <em>Introduction to Analytic Number Theory</em> by Apostol
  (1976).  We have less than 18 pages to read in order to complete
  reading this book.  Considering that we meet 3-4 times in a week and
  we discuss about 2-3 pages in every meeting, it appears that we
  would be able to complete reading this book in another 2 weeks.
</p>
<p>
  Reading this book has been quite a journey!  The previous three
  posts on this blog provide an account of how this journey has been.
  It has been fun, of course.  The best part of hosting a book
  discussion group like this has been the number of extremely smart
  people I got an opportunity to meet and interact with.  The insights
  and comments on the study material that others shared during the
  meetings were very helpful.
</p>
<p>
  The <a href="cc/iant/log.html">meeting log</a> shows that our
  meetings started really small with only 4 participants in the first
  meeting in March 2021 and then it gradually grew to about 10-12
  regular members within a month.  Then a few months later, the number
  of participants began dwindling a little.  This happened because
  some members of the group had to drop out as they got busy with
  other personal or professional engagements.  However, six months
  later, we still have about 4-5 regular participants meeting
  consistently.  I think it is pretty good that we have made it this
  far.
</p>
<h2 id="unrestricted-partitions">Unrestricted Partitions<a href="#unrestricted-partitions"></a></h2>
<p>
  The final chapter on integer partitions is very unlike all the
  previous 12 chapters.  While the previous chapters dealt
  with <em>multiplicative number theory</em>, this final chapter deals
  with <em>additive number theory</em>.  For example, the first
  theorem talks about an interesting property of <em>unrestricted
  partitions</em>.  We study the number of ways a positive integer can
  be expressed as a sum of positive integers.  The number of summands
  is unrestricted, repetition of summands is allowed and the order of
  the summands is not taken into account.  For example, the number 3
  has 3 partitions: 3, 2 + 1 and 1 + 1 + 1.  Similarly, the number 4
  has 5 partitions: 4, 3 + 1, 2 + 2, 2 + 1 + 1 and 1 + 1 + 1 + 1.
</p>
<p>
  I have always wanted to learn about partitions more deeply, so I am
  quite happy that this book ends with a chapter on partitions.  The
  subject of partitions is rich with very interesting results obtained
  by various accomplished mathematicians.  In the book, the first
  theorem about partitions is a very simple one that follows from the
  geometric representation of partitions.  Let us see an illustration
  first.
</p>
<p>
  How many partitions of 6 are there?  There are 11 partitions of 6.
  They are 6, 5 + 1, 4 + 2, 4 + 1 + 1, 3 + 3, 3 + 2 + 1, 3 + 1 + 1 +
  1, 2 + 2 + 2, 2 + 2 + 1 + 1, 2 + 1 + 1 + 1 + 1 and 1 + 1 + 1 + 1 + 1
  + 1.  Now how many of these partitions are made up of 5 parts?  Each
  summand is called a part.  The answer is 2.  There are 2 partitions
  of 6 that are made up of 5 parts.  They are 3 + 1 + 1 + 1 and 2 + 2
  + 1 + 1.  Let us represent both these partitions as arrangements of
  lattice points.  Here is the representation of the partition 3 + 1 +
  1 + 1:
</p>
<pre>
<code>&bull; &bull; &bull;
&bull;
&bull;
&bull;</code>
</pre>
<p>
  Now if we read this arrangement from left to right, column by
  column, we get another partition of 6, i.e. 4 + 1 + 1.  Note that
  the number of parts in 3 + 1 + 1 + 1 (i.e. 4) appears as the largest
  part in 4 + 1 + 1.  Similarly, the number of parts in 4 + 1 + 1
  (i.e. 3) appears as the largest part in 3 + 1 + 1 + 1.  Let us see
  one more example of this relationship.  Here is the geometric
  representation of 2 + 2 + 1 + 1:
</p>
<pre>
<code>&bull; &bull;
&bull; &bull;
&bull;
&bull;</code>
</pre>
<p>
  Once again, reading this representation from left to right, we get 4
  + 2, another partition of 6.  Once again, we can see that the number
  of partitions in 2 + 2 + 1 + 1 (i.e. 4) appears as the largest part
  in 4 + 2 and vice versa.  These observations lead to the first
  theorem in the chapter on partitions:
</p>
<blockquote>
  <strong>Theorem 14.1</strong>
  <em>
    The number ofpartitions of \( n \)
    into \( m \) parts is equal to the number of partitions of \( n \)
    into parts, the largest of which is \( m.  \)
  </em>
</blockquote>
<p>
  That was a brief introduction to the chapter on partitions.  In the
  next two or so weeks, we will dive deeper into the theory of
  partitions.
</p>
<h2 id="next-meeting">Next Meeting<a href="#next-meeting"></a></h2>
<p>
  If this blog post was fun for you, consider joining our next
  meeting.  Our next meeting is on Tue, 21 Sep 2021 at 17:00 UTC.
  Since we are at the beginning of a new chapter, it is a good time
  for new participants to join us.  It is also a good time for members
  who have been away for a while to join us back.  Since this chapter
  does not depend much on the previous chapters, new participants
  should be able to join our reading sessions for this chapter and
  follow along easily without too much effort.
</p>
<p>
  To join our discussions, see our channel details in the
  <a href="cc/#join">main page here</a>.  To get the
  meeting link for the next meeting, visit the
  <a href="cc/iant/">analytic number theory book
  page</a>.
</p>
<p>
  It is worth mentioning here that lurking is absolutely fine in our
  meetings.  In fact, most participants of our meetings join in and
  stay silent throughout the meeting.  Only a few members talk via
  audio/video or chat.  This is considered absolutely normal in our
  meetings, so please do not hesitate to join our meetings!
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/journey-to-integer-partitions.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Journey to the Prime Number Theorem</title>
<link>https://susam.net/journey-to-prime-number-theorem.html</link>
<guid isPermaLink="false">fooiz</guid>
<pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  How long does it take to start with zero knowledge of analytic
  number theory and successfully learn the analytic proof of the prime
  number theorem?  Take a guess!  I will share my answer in the next
  two paragraphs.  This is something I had wondered when we began our
  analytic number theory book discussion group in March 2021.  Back
  then, I thought it would take at least 100 hours of effort.
</p>
<p>
  The book I had chosen for our discussions was <em>Introduction to
  Analytic Number Theory</em> by Apostol (1976).  I have been hosting
  40-minute meetings for about 3-4 days every week since March 2021.
  We discuss a couple of pages of the book in every meeting.  Most
  participants in this meeting are from Hacker News and Libera IRC
  network.  For a long time, I was eager to learn the proof of the
  prime number theorem.  For those unfamiliar with the theorem, I will
  describe it briefly in further sections.  Let me first answer the
  question I asked in the previous paragraph.
</p>
<p>
  So how long does it take to start with no knolwedge of analytic
  number theory and teach ourselves the analytic proof of the prime
  number theorem?  Turns out, it takes 72 hours!  It took our group 72
  hours spread across <a href="cc/iant/log.html#110">110 meetings</a>
  over 6 months to be able to understand the proof.  It is worth
  noting here that most of us in this group have full-time jobs and
  other personal obligations!  We were all doing this for fun, for the
  joy of learning!
</p>
<p>
  Now I must mention that the 72 hours noted above is only the time
  spent together in reading the book and working through the theorems
  and proofs.  It does not include the personal time spent in solving
  problems, reading some sections again, taking notes, etc.  All of
  that was done in our personal time.  We did discuss the solutions to
  some of the very interesting problems in our meetings just to take a
  break from the theorem-and-proof style of reading but most of these
  72 hours of meetings focussed on working through the theorems and
  proofs in the book.
</p>
<p>
  It may be possible to achieve this milestone in lesser number of
  hours, perhaps by reading the book alone which for some folks might
  be faster than studying in a group or perhaps by skipping some
  chapters for topics that look very familiar.  In our discussions,
  however, we did not skip any chapter.  There were in fact a few
  chapters we could have skipped.  All members of these meetings were
  very familiar with divisibility, greatest common divisor, the
  fundamental theorem of arithmetic, etc. discussed in Chapter 1.
  Most of us were also very familiar with the concepts discussed in
  Chapter 5 such as congruences, residue classes, the Euler-Fermat
  theorem, the Chinese remainder theorem, etc.  Despite being familiar
  with these concepts, we decided not to skip any chapter for the sake
  of completeness of our coverage of the material.  In fact, we read
  every single line of the book and deliberated over every single
  concept discussed in the book.  With this detailed and tedious
  approach to reading the book, it took us 72 hours to read about 290
  pages and learn the analytic proof of the prime number theorem in
  Chapter 13.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#prime-number-theorem">Prime Number Theorem</a></li>
  <li><a href="#the-basics">Equivalent Forms</a></li>
  <li><a href="#dirichlet-dirichlet-dirichlet">Dirichlet, Dirichlet, Dirichlet!</a></li>
  <li><a href="#chain-of-proofs">Chain of Proofs</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="prime-number-theorem">Prime Number Theorem<a href="#prime-number-theorem"></a></h2>
<p>
  The prime number theorem is a very curious fact about the
  distribution of prime numbers that Gauss noticed in the year 1792
  when he was about 15 years old.  He noticed that the occurrence of
  primes become rarer and rarer as we expand our search for them to
  larger and larger integers.  For example, there are 4 primes between
  1 and 10, i.e. 40% of the numbers between 1 and 10 are primes!  But
  there are only 25 primes between 1 and 100, i.e. only 25% of the
  numbers between 1 and 100 are primes.  If we go up to 1000, we
  notice that there are only 168 primes between 1 and 1000, i.e. only
  16.8% of the numbers between 1 and 1000 are primes.  Formally, we
  denote these facts with the mathematical notation \( \pi(x) \) that
  denotes the prime counting function.  We say \( \pi(10) = 4, \) \(
  \pi(100) = 25, \) \( \pi(1000) = 168 \) and so on.  Note that we
  allow \( x \) to be a real number, so while \( \pi(10) = 4, \) we
  have \( \pi(10.3) = 4 \) as well.  One of the reasons we let \( x \)
  be a real number in the definition of \( \pi(x) \) is because it
  makes various problems we come across during the study of this
  function more convenient to work on using real analysis.
</p>
<p>
  We observe that the 'density' of primes continue to fall as we make
  \( x \) larger and larger.  In formal notation, we see that the
  ratio \( \pi(x) / x \) is \( 0.4 \) when \( x = 10.  \)  This ratio
  falls to \( 0.25 \) when \( x = 100.  \)  It falls further to \(
  0.168 \) when \( x = 1000 \) and so on.  Can we predict by how much
  this "density" falls?  The answer is yes.  That leads us to the
  prime number theorem.  The prime number theorem states that \(
  \pi(x) / x \) is asymptotic to \( 1 / \log x \) as \( x \)
  approaches infinity, i.e.

  \[
    \frac{\pi(x)}{x} \sim \frac{1}{\log x} \text{ as } x \to \infty.
  \]

  For those unfamiliar with the notation of asymptotic equality, here
  is another equivalent way to state the above relationship,

  \[
    \lim_{x \to \infty} \frac{\pi(x) / x}{1 / \log x} = 1.
  \]

  We could also write this as

  \[
    \lim_{x \to \infty} \frac{\pi(x)}{x / \log x} = 1
  \]

  or

  \[
    \pi(x) \sim \frac{x}{\log x} \text{ as } x \to \infty.
  \]

  Let us see how well this formula works as an estimate for the
  density of primes for small values of \( x.  \)
</p>
<table style="text-align: right" class="grid center">
  <tr>
    <th style="text-align: right">\( x \)</th>
    <th style="text-align: right">\( \pi(x) \)</th>
    <th style="text-align: right">\( x / \log x \)</th>
  </tr>
  <tr>
    <td>10</td>
    <td>4</td>
    <td>4.3</td>
  </tr>
  <tr>
    <td>100</td>
    <td>25</td>
    <td>21.7</td>
  </tr>
  <tr>
    <td>1000</td>
    <td>168</td>
    <td>144.8</td>
  </tr>
  <tr>
    <td>10000</td>
    <td>1229</td>
    <td>1085.7</td>
  </tr>
  <tr>
    <td>100000</td>
    <td>9592</td>
    <td>8685.9</td>
  </tr>
</table>
<p>
  Not bad!  In fact, the last two columns begin to agree more and more
  as \( x \) becomes larger and larger.
</p>
<p>
  The analytic proof of the prime number theorem was achieved with an
  intricate chain of equivalences and implications between various
  theorems.  The book consumes 13 chapters and 290 pages before
  completing the proof of the prime number theorem.  Each page is also
  quite dense with information.  The amount of commentary or
  illustrations is very little in the book.  Most of the book keeps
  alternating between theorem statements and proofs.  Occasionally,
  for especially long chapters with an intricate sequence of proofs,
  Apostol provides a plan of the proof in the introductions to such
  chapters.  It is quite hard to summarise a large and dense volume of
  work like this in a blog post but I will make an attempt to paint a
  very high-level picture of some of the key concepts that are
  involved in the proof.
</p>
<h2 id="the-basics">Equivalent Forms<a href="#the-basics"></a></h2>
<p>
  Everything from Chapters 1 to 3 is about building basic concepts and
  tools we will use later to work on the problem of the prime number
  theorem.  These concepts and tools were very interesting on their
  own.  They involved divisibility, various number-theoretic
  functions, Dirichlet products, the big oh notation, etc.  Chapter 4
  was the first chapter where we engaged ourselves with the prime
  number theorem.  This chapter taught us several other formulas that
  were logically equivalent to the prime number theorem.  One
  equivalence that would play a big role later was the equivalence
  between the prime number theorem

  \[
    \lim_{x \to \infty} \frac{\pi(x) \log x}{x} = 1
  \]

  and the following form:

  \[
    \lim_{x \to \infty} \frac{\psi(x)}{x} = 1.
  \]

  If we could prove one, the validity of the other would be
  established automatically.  The notation \( \psi(x) \) denotes the
  Chebyshev function which in turn is defined in terms of the Mangoldt
  function \( \Lambda(n) \) as \( \psi(x) = \sum_{n \le x} \Lambda(n).  \)
  Note that the formula above can also be stated using the asymptotic
  equality notation as follows:

  \[
    \psi(x) \sim x \text{ as } x \to \infty.
  \]

  There were several other equivalent forms too shown in Chapter 4.
  The fact that all these various forms were equivalent to each other
  was rigorously proved in the chapter.  Thus proving any one of the
  equivalent forms would be sufficient to prove the prime number
  theorem.  But in Chapter 4, we did not know how to prove any of the
  equivalent forms.  We could only prove the equivalence of the
  various formulas, not the formulas themselves.  We only learnt that
  if any of the equivalent forms is true, so is the prime number
  theorem.  Similarly, if any of the equivalent forms is false, so is
  the prime number theorem.  We would visit the prime number theorem
  again in Chapter 13 which would complete the proof of the prime
  number theorem by showing that the equivalent form mentioned above
  is indeed true.
</p>
<h2 id="dirichlet-dirichlet-dirichlet">Dirichlet, Dirichlet, Dirichlet!<a href="#dirichlet-dirichlet-dirichlet"></a></h2>
<p>
  Chapters 5 to 10 introduced more concepts involving congruences,
  finite abelian groups, their characters, Dirichlet characters,
  Dirichlet's theorem on primes in arithmetic progressions, Gauss
  sums, quadratic residues, primitive roots, etc.  Some of these
  concepts would turn out to be very important in proving the prime
  number theorem but most of them probably are not too important if
  understanding the proof of the prime number theorem is the only
  goal.  Regardless, all of these chapters were very interesting.
</p>
<p>
  It was in Chapters 11 and 12 that we felt that we were getting
  closer and closer to the proof of the prime number theorem.  Chapter
  11 began a detailed and rigorous study of convergence and divergence
  of Dirichlet series.  The Riemann zeta function is a specific type
  of Dirichlet series.  Chapter 12 introduced analytic continuation of
  the Riemann zeta function.  We could then show interesting results
  like \( \zeta(0) = -1/2 \) and \( \zeta(-1) = -1/12 \) using the
  analytic continuation of the zeta function.  This chapter also
  showed us why all trivial zeroes of \( \zeta(s) \) must lie at
  negative even integers.
</p>
<p>
  One thing I realised during the study of this book is how frequently
  we use concepts, operations, functions and theorems named after
  Dirichlet.  It was impossible to get through a meeting without
  having uttered "Dirichlet" at least a dozen times!
</p>
<h2 id="chain-of-proofs">Chain of Proofs<a href="#chain-of-proofs"></a></h2>
<p>
  Finally, Chapter 13 showed us how to prove the prime number theorem.
  The plan of the proof was laid out in the first section.  Our goal
  in this chapter is to prove that \( \psi(x) \sim x \) as \( x \to
  \infty.  \)  This is equivalent to the prime number theorem, so
  proving this amounts to proving the prime number theorem too.
</p>
<p>
  Next we learn that the asymptotic relation \( \psi_1(x) \sim x^2 / 2 \)
  as \( x \to \infty \) implies the previous asymptotic relationship.
  Here \( \psi_1(x) \) is defined as \( \psi_1(x) = \int_1^x \psi(t)
  \, dt.  \)  This implication is proved quite easily in one and a half
  pages.  But we still need to show that the asymptotic relation \(
  \psi_1(x) \sim x^2 / 2 \) as \( x \to \infty \) indeed holds good.
  Proving this takes a lot of work.  To prove this asymptotic relation
  we first learn to arrive at the following equation involving a
  contour integral:

  \[
    \frac{\psi_1(x)}{x^2} - \frac{1}{2} \left( 1 - \frac{1}{x} \right)^2
    = \frac{1}{2\pi i} \int_{c - \infty i}^{c + \infty i} \frac{x^{s - 1}}{s(s + 1)}
    \left( -\frac{\zeta'(s)}{\zeta(s)} - \frac{1}{s - 1} \right) \, ds
  \]

  for \( c \gt 1.  \)  The equation above looks quite complex initially
  but each part of it becomes friendly as we learn to derive it and
  then work on each part of it while working out further proofs.  Now
  if we could somehow show that the integral on the right hand side of
  the above equation approaches 0 as \( x \to \infty, \) that would
  end up proving the asymptotic relation involving \( \psi_1(x) \) and
  thus end up proving the prime number theorem by equivalence.
  However, proving that this integral indeed becomes 0 as \( x \to
  \infty \) requires a careful study of \( \zeta(s)/\zeta'(s) \) in
  the vicinity of the line \( \operatorname{Re}(s) = 1.  \)  This is
  the topic that most of the chapter deals with.
</p>
<p>
  This plan of the proof looked quite convoluted initially but Apostol
  has done a great job in this chapter to first walk us through this
  plan and then prove each fact that we need to make the proof work in
  a detailed and rigorous manner.  When we reached the end of the
  proof, one of our regular members remarked, "Now the proof does not
  look so complex!"
</p>
<p>
  Would the elementary proof of the prime number theory have been
  easier?  I don't know.  I have not studied the elementary proof.
  But Apostol does say this at the beginning of Chapter 13,
</p>
<blockquote>
  The analytic proof is shorter than the elementary proof sketched in
  Chapter 4 and its principal ideas are easier to comprehend.
</blockquote>
<p>
  Learning the analytic proof itself was quite a long journey that
  required dedication and consistency in our studies over a period of
  6 months.  If we trust the above excerpt from the book, then I think
  it is fair to assume that the elementary proof is even more
  formidable.
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  That was an account of our journey through an analytic number theory
  book from its first chapter up to the analytic proof of the prime
  number theorem.  We have not completed reading the entire book
  though.  We still have about another 30 pages to go through.  In the
  remaining study of this book, we will learn more about zero-free
  regions for \( \zeta(s), \) the application of the prime number
  theorem to the divisor function and the Euler totient function.  The
  next and the final chapter too has a lot to offer such as integer
  partition, Euler's pentagonal-number theorem and the partition
  identities of Ramanujan.  I am pretty hopeful that we will be
  complete reading this book in another few weeks of meetings.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/journey-to-prime-number-theorem.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>One Hundred Meetings</title>
<link>https://susam.net/one-hundred-meetings.html</link>
<guid isPermaLink="false">roecl</guid>
<pubDate>Fri, 20 Aug 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Today, our computation book discussion group is going to have the
  100th meeting!  Yes, <a href="cc/iant/log.html#100">the 100th
  meeting</a>!  We began these book discussion meetings about five
  months ago.  The first book we picked up for our discussions
  was <em>Introduction to Analytic Number Theory</em> by Apostol
  (1976).  We have been reading this book together for the last five
  months.  We have a tiny but consistent community of 6 to 8
  participants who meet regularly to study this book and share our
  understanding and insights with each other.
</p>
<p>
  In this blog post, I will talk about my personal experience hosting
  these meetings and my personal journey about reading this book.  It
  is worth keeping in mind then that what I am about to write below
  may not have any resemblance with the experience of other
  participants of these meetings.
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#the-reading-experience">The Reading Experience</a></li>
  <li><a href="#the-learning-experience">The Learning Experience</a></li>
  <li><a href="#three-concepts">Three Concepts</a>
    <ul>
      <li><a href="#the-mobius-function">The M&ouml;bius Function</a></li>
      <li><a href="#dirichlet-product">Dirichlet Product</a></li>
      <li><a href="#hurwitz-zeta-function">Hurwitz Zeta Function</a></li>
    </ul>
  </li>
  <li><a href="#the-next-meeting">The Next Meeting</a></li>
  <li><a href="#join-us">Join Us</a></li>
</ul>
<h2 id="the-reading-experience">The Reading Experience<a href="#the-reading-experience"></a></h2>
<p>
  As far as I know, everyone who joins our meetings are involved in
  computer programming in one form or another.  A few of them have
  very strong background in mathematics.  I host these meetings
  everyday and discuss a few sections of the book in detail.  I show
  how to work through the proofs, explain some of the steps, etc.
  Sometimes I get stuck in some step that I find too unobvious.
  Sometimes the steps are obvious but my brain is too slow to
  understand why the steps work.  But these tiny glitches have not
  been a problem so far, thanks to all the members who join these
  meetings on a daily basis and contribute their explanations of the
  proofs.
</p>
<p>
  I believe the group members are the best part of these discussions.
  Thanks to the insights and explanation of the reading material
  shared by all these members, I am fairly confident that we are able
  to take a close look at every proof and convince ourselves that
  every step of the proofs work.
</p>
<h2 id="the-learning-experience">The Learning Experience<a href="#the-learning-experience"></a></h2>
<p>
  The first web meeting to discuss the chosen analytic number theory
  book occurred on 5 Mar 2021.  See the blog
  post <a href="reading-classic-computation-books.html">Reading
  Classic Computation Books</a> to read about the early days of our
  group and how it was formed.  Back then, I knew little to nothing
  about analytic number theory.  Although I was familiar with some of
  the elementary concepts like divisibility, Euler's totient function,
  modular arithmetic, calculus and related theorems, chapter 2 of the
  book itself proved to be a significant challenge for me.  In the
  second chapter, it became clear to me that we will be building new
  levels of mathematical abstractions, use these abstractions to build
  yet another layer of abstractions and so on.  The chapter began with
  a description of the M&ouml;bius function, a very neat and
  interesting function that I was previously unaware of.  That was
  fun!  But soon, this chapter began adding new layers of abstractions
  such as Dirichlet product, Dirichlet inverse, generalised
  convolution, etc.  I could almost feel my brain stretching and
  growing as we went through each page of this chapter.
</p>
<p>
  I often saw that after I have learnt a new concept in a chapter, it
  would not become intuitive immediately.  I would understand the
  concepts, understand the related theorems, understand each step of
  the proofs, solve exercise problems, know how to apply the theorems
  when needed and yet I could not "feel" them.  I wanted to not just
  understand the concepts but I also wanted to "feel" the concepts
  like the way I could feel algebra, calculus, computer programming,
  etc.  In the initial days, I wondered if I was too old to develop
  good intuition for all these new and highly sophisticated concepts.
</p>
<p>
  Despite always feeling that all these concepts were too technical
  and quite unintuitive, I kept going.  I kept hosting these
  discussions with a frequency of about 3-5 days every week.  We
  continued discussing the various chapters and the proofs in them.
  And then suddenly one day while reading chapter 4, something
  interesting happened.  As we were employing Dirichlet products to
  obtain some useful results, I realised that the concept
  of <em>Dirichlet products</em> which once felt so foreign two
  chapters earlier, now felt completely intuitive.  I
  could <em>see</em> different functions being equivalent to Dirichlet
  products intuitively and effortlessly.  Dirichlet products felt no
  more alien than, say, arithmetic multiplication.  I could "feel" it
  now.  It was a great feeling.  I realised that sometimes it might
  take a few additional chapters of reading and using those concepts
  over and over again before they really begin to feel intuitive.
</p>
<h2 id="three-concepts">Three Concepts<a href="#three-concepts"></a></h2>
<p>
  In this section, I will pick three interesting concepts from
  different parts of the book to provide a glimpse of what the journey
  has been like.  These three things occur in the book again and again
  and play a very important role in several chapters of the book.  Of
  course, it goes without saying that there are many interesting
  concepts in the book and many of them may be more important than the
  ones I am about to show below.
</p>
<h3 id="the-mobius-function">The M&ouml;bius Function<a href="#the-mobius-function"></a></h3>
<p>
 For any positive integer \( n, \) the M&ouml;bius function \( \mu(n)
 \) is defined as follows:

 \[
   \mu(1) = 1;
 \]

 If \( n \gt 1, \) write \( n = p_1^{a_1} \dots p_k^{a_k} \) (prime
 factorisation).  Then

 \begin{align*}
   \mu(n) &amp; = (-1)^k \text{ if } a_1 = a_2 = \dots = a_k = 1, \\
   \mu(n) &amp; = 0 \text{ otherwise}.
 \end{align*}

 If \( n \ge 1, \) we have

 \[
   \sum_{d \mid n} \mu(d) =
   \begin{cases}
   1 &amp; \text{ if } n = 1, \\
   0 &amp; \text{ if } n \gt 1.
   \end{cases}
 \]
</p>
<p>
  I was unfamiliar with this function prior to reading the book.  It
  felt like a nice little cute function initially but as we went
  through more chapters, it soon became clear that this function plays
  a major role in analytic number theory.
</p>
<p>
  As a simple example, we will soon see in this post that the Euler's
  totient function can be expressed as a Dirichlet product of the
  M&ouml;bius function and the arithmetical function \( N(n) = n.  \)
</p>
<p>
  As a more sophisticated example, the Dirichlet series with
  coefficients as the M&ouml;bius function is the multiplicative
  inverse of the Riemann zeta function, i.e. if \( s = \sigma + it \)
  is a complex number with its real part \( \sigma \gt 1, \) we have

  \[
    \sum_{n=1}^{\infty} \frac{\mu(n)}{n^s} = \frac{1}{\zeta(s)}.
  \]

  This immediately shows that \( \zeta(s) \ne 0 \) for \( \sigma \gt
  1.  \)
</p>
<h3 id="dirichlet-product">Dirichlet Product<a href="#dirichlet-product"></a></h3>
<p>
  If \( f \) and \( g \) are two arithmetical functions, their
  Dirichlet product \( f * g \) is defined as:

  \[
    (f * g)(n) = \sum_{d \mid n} f(d) g\left( \frac{n}{d} \right).
  \]

  Dirichlet products appear to pop up magically at various places in
  number theory.  Here is a simple example:

  \[
    \varphi(n) = \sum_{d \mid n} \mu(d) \frac{n}{d}.
  \]

  Therefore in the notation of Dirichlet products, the above equation
  can also be written as

  \[
    \varphi = \mu * N
  \]

  where \( N \) represents the arithmetical function \( N(n) = n \)
  for all \( n.  \)
</p>
<h3 id="hurwitz-zeta-function">Hurwitz Zeta Function<a href="#hurwitz-zeta-function"></a></h3>
<p>
  For complex numbers \( s = \sigma + it, \) the Hurwitz zeta function
  \( \zeta(s, a) \) is initially defined for \( \sigma \gt 1 \) as

  \[
    \zeta(s, a) = \sum_{n=0}^{\infty} \frac{1}{(n + a)^s}
  \]

  where \( a \) is a fixed real number, \( 0 \lt a \lt 1.  \)  Then by
  analytic continuation, it is defined for \( \sigma \le 1 \) as

  \[
    \zeta(s, a) = \Gamma(1 - s)I(s, a)
  \]

  where \( \Gamma \) represents the gamma function

  \[
    \Gamma(s) =  \int_0^{\infty} x^{s - 1} e^{-x} \, dx
  \]

  defined for \( \sigma \gt 0 \) and also defined, by analytic
  continuation, for \( \sigma \le 0 \) except for \( \sigma = 0, -1,
  -2, \dots \) (the nonpositive integers) and \( I(s, a) \) is defined
  by the contour integral

  \[
    I(s, a) = \frac{1}{2\pi i} \int_C \frac{z^{s-1} e^{az}}{1 - e^z} \, dz
  \]

  where \( 0 \lt a \le 1 \) and the contour \( C \) is a loop around
  the negative real axis composed of three parts \( C_1, \) \( C_2 \)
  and \( C_3 \) such that for \( c \lt 2\pi, \) we have \( z =
  re^{-\pi i} \) on \( C_1 \) and \( z = re^{\pi i} \) on \( C_3 \) as
  \( r \) varies from \( c \) to \( +\infty \) and \( z = ce^{i
  \theta} \) on \( C_2, \) \( -\pi \le \theta \le \pi.  \)
</p>
<p>
  Now admittedly, the definition or the analytic continuation of
  Hurwitz zeta function may seem very heavy and obscure to the
  uninitiated and it <em>is</em> indeed quite heavy.  It takes 6 pages
  in chapter 12 to build the prerequisite concepts before we arrive at
  this definition.  It is evident that this definition uses other
  concepts like the gamma function, a specific contour integral, etc.
  and it is only natural to expect that one has to gain sufficient
  expertise with the gamma function and contour integrals before the
  Hurwitz zeta function begins to feel intuitive.
</p>
<p>
  But once we have established the analytic continuation of the
  Hurwitz zeta function, many insightful facts about the Riemann zeta
  function follow readily.  It is easy to see that the Riemann zeta
  function can be defined in terms of the Hurwitz zeta function as

  \[
    \zeta(s) = \zeta(s, 1) = \sum_{n=1}^{\infty} \frac{1}{n^s}.
  \]

  Yes, the \( \zeta \) symbol is overloaded: \( \zeta(s, a) \) is the
  Hurwitz zeta function whereas \( \zeta(s) \) is the Riemann zeta
  function.  This relationship between the Riemann zeta function and
  the Hurwitz zeta function along with the analytic continuation of
  the Hurwitz zeta function opens new doors into the wonderful world
  of complex numbers and let us obtain beautiful and profound facts
  about the Riemann zeta function such as the fact that it has zeros
  at negative even integers, i.e. \( \zeta(n) = 0 \) for \( n = -2,
  -4, -6, \dots \) and the fact that \( \zeta(0) = -\frac{1}{2} \) and
  \( \zeta(-1) = -\frac{1}{12} \) and so on.
</p>
<p>
  I believe beautiful results like these obtained by digging deep into
  complex analysis are what makes the study of analytic number theory
  so rewarding.
</p>
<h2 id="the-next-meeting">The Next Meeting<a href="#the-next-meeting"></a></h2>
<p>
  The next meeting is coming up today in a few hours.  Are we planning
  anything special for the 100th meeting?
</p>
<p>
  I think the 100th meeting is a significant milestone in our journey
  of understanding the beautiful and interesting gems hidden away in
  the subject of analytic number theory.  This milestone has been
  possible only due to the sustained curiousity and eagerness among
  the members of the group to learn a significant area of mathematics
  and learn it well.  We have reached this milestone successfully due
  to the passion and love for mathematics that drive the regular
  members to join these meetings and go through a few pages of the
  book everyday.  In these meetings, we have read 12 chapters
  consisting of over 250 pages so far.  Many of us knew nothing about
  analytic number theory merely five months ago and now we can
  appreciate the Riemann zeta function at a deeper level.  We now
  understand what the Riemann hypothesis really means.  This has been
  a great journey so far.
</p>
<p>
  Despite being a significant milestone and cause for celebration, we
  are going to keep our 100th meeting fairly simple.  We will continue
  where we left off yesterday.  Today we have some more relationships
  between the gamma function and the Riemann zeta function to go
  through, so that is what we will do.  We will also show that \(
  \zeta(0) = -\frac{1}{2} \) and \( \zeta(-1) = -\frac{1}{12} \) using
  the analytic continuation of the Hurwitz zeta function today.
</p>
<h2 id="join-us">Join Us<a href="#join-us"></a></h2>
<p>
  If this blog post was fun for you and you would like to join our
  meetups, please go through <a href="cc/iant/">this
  page</a> to get the meeting link and join us.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/one-hundred-meetings.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Euler's Formula</title>
<link>https://susam.net/euler-formula.html</link>
<guid isPermaLink="false">fiusr</guid>
<pubDate>Fri, 04 Jun 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  I know that Euler's identity is widely regarded as the most
  beautiful theorem in mathematics.  In my opinion, the truly
  beautiful concept involved here is Euler's formula:

  \[
  e^{ix} = \cos x + i \sin x.
  \]

  It unifies algebra, trigonometry, complex numbers and calculus.
  Euler's identity is only a special case of Euler's formula, i.e.
  Euler's formula with \( x = \pi \) gives us Euler's identity:

  \[
  e^{i \pi} = -1.
  \]

  This is cute but Euler's formula is truly beautiful.  In fact with
  \( x = \tau = 2\pi, \) we get another cute result:

  \[
  e^{i \tau} = 1.
  \]

  Quoting an excerpt from Chapter 22 of <em>The Feynman Lectures on
  Physics, Volume I</em>:
</p>
<blockquote>
  <p>
    We summarise with this, the most remarkable formula in mathematics:

    \[
    e^{i \theta} = \cos \theta + i \sin \theta.
    \]

    This is our jewel.
  </p>
  <p>
    We may relate the geometry to the algebra by representing complex
    numbers in a plane; the horizontal position of a point is \( x, \)
    the vertical position of a point is \( y.  \)  We represent every
    complex number, \( x + iy.  \)  Then if the radial distance to this
    point is called \( r \) and the angle is called \( \theta, \) the
    algebraic law is that \( x + iy \) is written in the form \( r,
    e^{i \theta} \) where the geometrical relationships between
    \( x, \) \( y, \) \( r, \) and \( \theta \) are as shown.  This,
    then, is the unification of algebra and geometry.
  </p>
</blockquote>
<p>
  See the bottom of the page at
  <a href="https://www.feynmanlectures.caltech.edu/I_22.html">https://www.feynmanlectures.caltech.edu/I_22.html</a>
  for the above excerpt.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/euler-formula.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Notes on Chapter 3: Averages of Arithmetical Functions</title>
<link>https://susam.net/cc/iant/ch03.html</link>
<guid isPermaLink="false">sjxwm</guid>
<pubDate>Fri, 09 Apr 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[

<!-- Section 3.3 -->
<h2 id="3.3">&sect; 3.3: Euler's summation formula<a href="#3.3"></a></h2>
<h3 id="main-idea-behind-the-proof-of-eulers-summation-formula">Main idea behind the proof of Euler's summation formula<a href="#main-idea-behind-the-proof-of-eulers-summation-formula"></a></h3>
<p>
  Important numbers in the proof:

  \[
    0, \quad
    \underbrace{[y]}_{=\,m}, \quad
    y, \quad
    \underbrace{[y] + 1}_{=\,m + 1}, \quad
    \underbrace{[x]}_{=\,k}, \quad
    x.
  \]

  Splitting the definite integral:

  \[
    \int_y^x f(t)\,dt
    = \int_{y}^{[y] + 1} f(t)\,dt
    + \underbrace{\int_{[y] + 1}^{[y] + 2} f(t)\,dt + \dots
    + \int_{[x] - 1}^{[x]} f(t)\,dt}_{=\,\int_{[y] + 1}^{[x]} f(t)\, dt}
    + \int_{[x]}^{x} f(t)\,dt.
  \]

  Using the more convenient variables \( m \) and \( k, \) we get:

  \[
    \int_y^x f(t)\,dt
    = \int_m^{m + 1} f(t)\,dt
    + \underbrace{\int_{m + 1}^{m + 2} f(t)\,dt + \dots
    + \int_{k - 1}^{k} f(t)\,dt}_{=\,\int_{m + 1}^{k} f(t)\, dt}
    + \int_{k}^{x} f(t)\,dt.
  \]
</p>
<h3 id="sum-of-integrals-in-the-proof-of-eulers-summation-formula">Sum of integrals in the proof Euler's summation formula<a href="#sum-of-integrals-in-the-proof-of-eulers-summation-formula"></a></h3>
<p>
  \begin{align*}
    \int_{m + 1}^{k} [t] f'(t) dt
    &amp; = \int_{m + 1}^{m + 2} [t] f'(t) dt
    + \int_{m + 2}^{m + 3} [t] f'(t) dt + \dots
    + \int_{k - 1}^{k} [t] f'(t) dt \\
    &amp; =
    \begin{aligned}[t]
      &amp; (m + 2) f(m + 2) - (m + 1) f(m + 1) - f(m + 2) \\
    + &amp; (m + 3) f(m + 3) - (m + 2) f(m + 2) - f(m + 3) \\
      &amp; \dots \\
    + &amp; (k) f(k) - (k - 1) f(k - 1) - f(k)
    \end{aligned} \\
    &amp; = kf(k) - (m + 1)f(m + 1) - \sum_{n=m + 2}^{k} f(n) \\
    &amp; = kf(k) - mf(m + 1) - f(m + 1) - \sum_{n=m + 2}^{k} f(n) \\
    &amp; = kf(k) - mf(m + 1) - \sum_{n=m + 1}^{k} f(n) \\
    &amp; = kf(k) - mf(m + 1) - \sum_{y \lt n \le x} f(n).
  \end{align*}
</p>
<h3 id="equation-6-in-the-proof-of-eulers-summation-formula">Equation (6) in the proof of Euler's summation formula<a href="#equation-6-in-the-proof-of-eulers-summation-formula"></a></h3>
<p>
  \begin{align*}
    \sum_{y \lt n \le x} f(n)
    &amp; = - \int_{m + 1}^k [t] f'(t) \, dt + k f(k) - m f(m + 1) \\
    &amp; = \begin{aligned}[t]
          &amp; \left( - \int_y^{m + 1} [t] f'(t) \, dt
                       - \int_{m + 1}^k [t] f'(t) \, dt
                       - \int_k^x [t] f'(t) \, dt \right) \\
          &amp; + f(k) - m f(m + 1)
                + \int_y^{m + 1} [t] f'(t) \, dt
                + \int_k^x [t] f'(t) \, dt
        \end{aligned} \\
    &amp; = - \int_y^x [t] f'(t) \, dt + k f(k) - m f(m + 1)
            + \int_y^{m + 1} m f'(t) \, dt + \int_k^x k f'(t) \, dt \\
    &amp; = - \int_y^x [t] f'(t) \, dt + k f(k) - m f(m + 1)
            + \biggl( m f(m + 1) - m f(y) \biggr) + \biggl( k f(x) - k f(k) \biggr) \\
    &amp; = - \int_y^x [t] f'(t) \, dt + k f(x) - m f(y).
  \end{align*}
</p>
<h3 id="using-integration-by-parts-in-the-proof-of-eulers-summation-formula">Using integration by parts in the proof of Euler's summation formula<a href="#using-integration-by-parts-in-the-proof-of-eulers-summation-formula"></a></h3>
<p>
  Integration by parts:

  \[
    \int uv \, dt = u \int v \, dt - \int u' \left( \int v \, dt \right) \, dt.
  \]

  \[
    \int_y^x t f'(t) \, dt
    = \left. \left( t f(t) - \int f(t) \, dt \right) \right|_y^x
    = x f(x) - y f(y) - \int_y^x f(t) \, dt.
  \]

  Final step of the proof:

  \begin{align*}
    \sum_{y \lt n \le x} f(n)
    &amp; = -\int_y^x [t] f'(t) \, dt + k f(x) - m f(y) \\
    &amp; = \begin{aligned}[t]
          &amp; -\int_y^x [t] f'(t) \, dt + [x] f(x) - [y] f(y) \\
          &amp; + \underbrace{
            \left( \int_y^x t f'(t) \, dt - x f(x) + y f(y)
                   + \int_y^x f(t) \, dt \right)}_{0 \text{ by above definite integral}}
        \end{aligned} \\
    &amp; = \int_y^x f(t) \, dt + \int_y^x (t - [t]) f'(t) \, dt
        + f(x)([x] - x) - f(y)([y] - y).
  \end{align*}
</p>

<!-- Section 3.4 -->
<h2 id="3.4">&sect; 3.3: Some elementary asymptotic formulas<a href="#3.4"></a></h2>
<h3 id="splitting-integral-in-the-proof-of-theorem-3.2">Splitting integral in the proof of Theorem 3.2<a href="#splitting-integral-in-the-proof-of-theorem-3.2"></a></h3>
<p>
  Splitting definite integral:

  \begin{align*}
    &amp; \int_1^{\infty} f(t) \, dt = \int_1^{x} f(t) \, dt + \int_x^{\infty} f(t) \, dt \\
    &amp; \iff
    \int_1^{\infty} f(t) \, dt - \int_x^{\infty} f(t) \, dt = \int_1^x f(t) \, dt.
  \end{align*}

  Solving improper integral:

  \[
    \int_x^{\infty} \frac{1}{t^2} \, dt
    = \lim_{b \to \infty} \int_x^b \frac{1}{t^2} dt
    = \lim_{b \to \infty} \frac{-1}{t} \Biggr|_x^b
    = \left( \lim_{b \to \infty} \frac{-1}{b} \right) + \frac{1}{x}
    = 0 + \frac{1}{x} = \frac{1}{x}.
  \]
</p>
<h3 id="eulers-constant-in-the-proof-of-theorem-3.2-a">Euler's constant in the proof of Theorem 3.2 (a)<a href="#eulers-constant-in-the-proof-of-theorem-3.2-a"></a></h3>
<p>
  Definition of Euler's constant:

  \[
    C = \lim_{n \to \infty}
        \left( 1 + \frac{1}{2} + \frac{1}{3} + \dots + \frac{1}{n} - \log n \right)
      = \lim_{x \to \infty} \left( \sum_{n \le x} \frac{1}{n} - \log x \right).
  \]

  We begin with

  \[
    \sum_{n \le x} \frac{1}{n}
    = \log x
    + \underbrace{1 - \int_1^{\infty} \frac{t - [t]}{t^2} \, dt}_{\text{We will show below that this is \( C \)}}
    + O\left( \frac{1}{x} \right).
  \]

  Rearranging the terms, we get

  \[
    \sum_{n \le x} \frac{1}{n} - \log x
    = 1 - \int_1^{\infty} \frac{t - [t]}{t^2} \, dt
    + O\left( \frac{1}{x} \right).
  \]

  Using the definition of \( C, \) we get

  \begin{align*}
    C
    &amp; = \lim_{x \to \infty}
            \left( \sum_{n \le x} \frac{1}{n} - \log x \right) \\
    &amp; = \lim_{x \to \infty}
            \left( 1 - \int_1^{\infty} \frac{t - [t]}{t^2} \, dt
                     + O\left( \frac{1}{x} \right) \right) \\
    &amp; = 1 - \int_1^{\infty} \frac{t - [t]}{t^2} \, dt.
  \end{align*}
</p>
<h3 id="some-integrals-in-the-proof-of-theorem-3.2-b">Some integrals in the proof of Theorem 3.2 (b)<a href="#some-integrals-in-the-proof-of-theorem-3.2-b"></a></h3>
<p>
  \[
    \int_1^x \frac{dt}{t^s}
    = \frac{t^{-s + 1}}{-s + 1} \Biggr|_1^x
    = \frac{t^{1 - s}}{1 - s} \Biggr|_1^x
    = \frac{x^{1 - s}}{1 - s} - \frac{1}{1 - s}.
  \]

  \[
    \int_1^x \frac{t - [t]}{t^{s + 1}} \, dt
    =   \int_1^{\infty} \frac{t - [t]}{t^{s + 1}} \, dt
      - \int_x^{\infty} \frac{t - [t]}{t^{s + 1}} \, dt
    = \int_1^{\infty} \frac{t - [t]}{t^{s + 1}} \, dt
    + \underbrace{\frac{1}{s} O\left( x^{-s}\right)}_{\text{explained below}}.
  \]

  \[
    0
    \le \int_x^{\infty} \frac{t - [t]}{t^{s + 1}} \, dt
    \le \int_x^{\infty} \frac{1}{t^{s + 1}} \, dt
    = \frac{-1}{st^s} \Biggr|_x^\infty = \frac{1}{sx^s} = \frac{1}{s} x^{-s}.
  \]

  \begin{align*}
    \sum_{n \le x} \frac{1}{n^s}
    &amp; = \int_1^x \frac{dt}{t^s}
            - s \int_1^x \frac{t - [t]}{t^{s + 1}} + 1
            - \frac{x - [x]}{x^s} \, dt \\
    &amp; = \frac{x^{1 - s}}{1 - s} - \frac{1}{1 - s}
            - s \int_1^{\infty} \frac{t - [t]}{t^{s + 1}} \, dt + 1 + O(x^{-s}).
  \end{align*}
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/cc/iant/ch03.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Notes on Chapter 2: Arithmetical Functions and Dirichlet Multiplication</title>
<link>https://susam.net/cc/iant/ch02.html</link>
<guid isPermaLink="false">zgowu</guid>
<pubDate>Fri, 26 Mar 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[

<!-- Section 2.11 -->
<h2 id="2.11">&sect; 2.11: The inverse of a completely multiplicative function<a href="#2.11"></a></h2>
<h3 id="completely-multiplicative-function">Completely Multiplicative Function<a href="#completely-multiplicative-function"></a></h3>
<p>
  \[
    f(mn) = f(m) f(n) \text{ for all } m, n.
  \]
</p>
<h3 id="identity-function-dirichlet-product">Identity function (Dirichlet product)<a href="#identity-function-dirichlet-product"></a></h3>
<p>
  \[
    I(n) =
    \begin{cases}
    1 &amp; \text{ if } n = 1, \\
    0 &amp; \text{ if } n \gt 1.
    \end{cases}
  \]
</p>
<p>
  \begin{align*}
    f(n)I(n)
    &amp; =
    \begin{cases}
      1 \cdot 1    &amp; \text{ if } n = 1, \\
      f(n) \cdot 0 &amp; \text{ if } n \gt 1.
    \end{cases} \\
    &amp; = \begin{cases}
      1 &amp; \text{ if } n = 1, \\
      0 &amp; \text{ if } n \gt 1.
    \end{cases} \\
    &amp; = I(n).
  \end{align*}
</p>
<h3 id="mobius-function-for-prime-powers">M&ouml;bius function for prime powers<a href="#mobius-function-for-prime-powers"></a></h3>
<p>
  \[
    \mu(1) = 1, \qquad
    \mu(p) = -1, \qquad
    \mu(p^2) = \mu(p^3) = \dots = 0.
  \]
</p>
<h3 id="third-equation-in-the-proof-of-theorem-2.17">Third equation in the proof of Theorem 2.17<a href="#third-equation-in-the-proof-of-theorem-2.17"></a></h3>
<p>
  \begin{align*}
    \sum_{d \mid p^a} \mu(d) f(d) f\left(\frac{p^a}{d}\right)
    &amp; = \sum_{d = 1, p, p^2, \dots, p^a} \mu(d) f(d) f\left(\frac{p^a}{d}\right) \\
    &amp; = \begin{aligned}[t]
           &amp; \mu(1) f(1) f\left( \frac{p^a}{1} \right) +
                 \mu(p) f(p) f\left( \frac{p^a}{p} \right) \\
           &amp; + \underbrace{\mu(p^2) f(p^2) f\left( \frac{p^a}{p^2} \right) + \dots +
                               \mu(p^a) f(p^a) f\left( \frac{p^a}{p^a} \right)}_{=\,0}
        \end{aligned} \\
    &amp; = \mu(1) f(1) f(p^a) + \mu(p) f(p) f(p^{a - 1}) \\
    &amp; = f(p^a) - f(p) f(p^{a - 1}).
  \end{align*}
</p>
<h3 id="final-step-in-the-proof-of-theorem-2.17">Final step in the proof of Theorem 2.17<a href="#final-step-in-the-proof-of-theorem-2.17"></a></h3>
<p>
  \begin{align*}
    f(p^a)
    &amp; = f(p)f(p^{a - 1}) \\
    &amp; = f(p)f(p)f(p^{a - 2}) \\
    &amp; = \dots \\
    &amp; = \underbrace{f(p)f(p)f(p) \dots f(p)}_{a \text{ times}} \\
    &amp; = \left( f(p) \right)^a.
  \end{align*}
</p>
<h3 id="how-the-completely-multiplicative-property-works-for-prime-powers">How the completely multiplicative property works for prime powers<a href="#how-the-completely-multiplicative-property-works-for-prime-powers"></a></h3>
<p>
  \[
    f(mn) = f(m)f(n) \text{ whenever } (m, n) = 1.
  \]
</p>
<p>
  \[
    f(p_1^{\alpha_1} p_2^{\alpha_2} \dots p_k^{\alpha_k})
    = f(p_1^{\alpha_1}) f(p_2^{\alpha_2}) \dots f(p_k^{\alpha_k}).
  \]
</p>
<h3 id="euler-totient-function-as-dirichlet-product">Euler totient function as Dirichlet product<a href="#euler-totient-function-as-dirichlet-product"></a></h3>
<p>
  \[
    \varphi(n)
    = \sum_{d \mid n} \mu(d) \frac{n}{d}
    = \sum_{d \mid n} \mu(d) N\left(\frac{n}{d}\right)
    = (\mu * N)(n).
  \]
</p>
<h3 id="proof-of-theorem-2.18">Proof of Theorem 2.18<a href="#proof-of-theorem-2.18"></a></h3>
<p>
  Let \( f \) be multiplicative.  We want to show that

  \[
    \sum_{d \mid n} \mu(d) f(d) = \prod_{p \mid n} (1 - f(p)).
  \]

  Note the following:

  \[
    g(n)
    = \sum_{d \mid n} \mu(d) f(d)
    = \sum_{d \mid n} (\mu f) (d)
    u\left( \frac{n}{d} \right)
    = (\mu f) * u.
  \]

  The functions \( \mu \) and \( f \) are multiplicative.  Thus \( \mu
  f \) is multiplicative.  Thus \( (\mu f) * u \) is multiplicative.
  Therefore

  \[
    g(n) = g(p_1^{a_1} p_2^{a_2} \dots p_k^{a_k}) =
    g(p_1^{a_1}) g(p_2^{a_2}) \dots g(p_k^{a_k}).
  \]

  But

  \begin{align*}
    g(p_i^{a_i})
    &amp; = \sum_{d \mid p_i^{a_i}} \mu(d) f(d) \\
    &amp; = \mu(1) f(1) + \mu(p_i) f(p_i) +
             \underbrace{\mu(p_i^2) f(p_i^2) + \dots + \mu(p_i^{a_i}) f(p_i^{a_i})}_{=\,0} \\
    &amp; = 1 - f(p).
  \end{align*}

  From the two equations above, we get

  \begin{align*}
    g(n)
    &amp; = g(p_1^{a_1}) g(p_2^{a_2}) \dots g(p_k^{a_k}) \\
    &amp; = (1 - f(p_1)) (1 - f(p_2)) \dots (1 - f(p_k)) \\
    &amp; = \prod_{p \mid n} (1 - f(p)).
  \end{align*}
</p>

<!-- Section 2.15 -->
<h2 id="2.15">&sect; 2.15: Formal power series<a href="#2.15"></a></h2>

<h3 id="product-of-formal-power-series">Product of formal power series<a href="#product-of-formal-power-series"></a></h3>
<p>
  \begin{align*}
    A(x)B(x)
    &amp; = \left( \sum_{n=0}^{\infty} a(n) x^n \right) \left( \sum_{n=0}^{\infty} b(n) x^n \right) \\
    &amp; = \left( a(0) + a(1)x + a(2)x^2 + \dots \right)
            \left( b(0) + b(1)x + b(2)x^2 + \dots \right) \\
    &amp; = a(0)b(0) +
            \Bigl( a(0)b(1) + a(1)b(0) \Bigr) x +
            \Bigl( a(0)b(2) + a(1)b(1) + a(2)b(0) \Bigr) x^2 + \dots \\
    &amp; = \sum_{k=0}^0 a(k)b(n - k) + \sum_{k=0}^1 a(k)b(1 - k)x + \sum_{k=0}^2 a(k)b(2 - k)x^2 + \dots \\
    &amp; = \sum_{n=0}^{\infty} \sum_{k=0}^n a(k)b(n - k).
  \end{align*}
</p>
<h3 id="commutativity-of-product-of-formal-power-series">Commutativity of product of formal power series<a href="#commutativity-of-product-of-formal-power-series"></a></h3>
<p>
  \[
    A(x)B(x)
    = \sum_{n=0}^{\infty} \underbrace{\left\{ \sum_{k=0}^{n} a(k) b(n - k) \right\}}_{c(n)} x^n.
  \]
  \[
    B(x)A(x)
    = \sum_{n=0}^{\infty} \underbrace{\left\{ \sum_{k=0}^{n} a(n - k) b(k) \right\}}_{c'(n)} x^n.
  \]
  \[
    c(3) = a(0)b(3) + a(1)b(2) + a(2)b(1) + a(3)b(0).
  \]
  \[
    c'(3) = a(3)b(0) + a(2)b(1) + a(1)b(2) + a(0)b(3).
  \]
</p>
<h3 id="distributivity-of-multiplication-over-addition-in-formal-power-series">Distributivity of multiplication over addition in formal power series<a href="#distributivity-of-multiplication-over-addition-in-formal-power-series"></a></h3>
<p>
  \[
    A(x)\Bigl(B(x) + C(x)\Bigr) = A(x)B(x) + A(x)C(x).
  \]
  \[
    \Bigl(B(x) + C(x)\Bigr)A(x) = B(x)A(x) + C(x)A(x).
  \]

  \begin{align*}
    A(x)\Bigl(B(x) + C(x)\Bigr)
    &amp; = \left( \sum_{n=0}^{\infty} a(n) x^n \right)
        \left( \sum_{n=0}^{\infty} \Bigl( b(n) + c(n) \Bigr) x^n \right) \\
    &amp; = \sum_{n=0}^{\infty} \Bigl\{ \sum_{k=0}^{n} a(k) \Bigl( b(n - k) + c(n - k) \Bigr) \Bigr\} x^n.
  \end{align*}

  \[
    A(x)B(x) + A(x)C(x)
    = \sum_{n=0}^{\infty} \sum_{k=0}^n a(k) b(n - k) x^n +
    \sum_{n=0}^{\infty} \sum_{k=0}^n a(k) c(n - k) x^n.
  \]
</p>
<h3 id="determining-coefficients-of-inverse-of-power-series">Determining coefficients of inverse of power series<a href="#determining-coefficients-of-inverse-of-power-series"></a></h3>
<p>
  \begin{align*}
    A(x)B(x)
    &amp; = \sum_{n=0}^{\infty} \Bigl( \sum_{k=0}^{n} a(k) b(n - k) \Bigr) x^n \\
    &amp; = \Bigl( a(0) b(0) \Bigr) x^0 +
            \Bigl( a(0) b(1) + a(1) b(0) \Bigr) x^1 +
            \Bigl( a(0) b(2) + a(1) b(1) + a(2) b(0) \Bigr) x^2 +
            \dots \\
    &amp; = 1.
  \end{align*}
</p>
<h3 id="inverse-of-geometric-series">Inverse of geometric series<a href="#inverse-of-geometric-series"></a></h3>
<p>
  \begin{align*}
    A(x) &amp; = 1 + ax + (ax)^2 + (ax)^3 + \dots, \\
    B(x) &amp; = 1 - ax.
  \end{align*}

  \begin{align*}
    A(x) B(x)
    &amp; = \Bigl( 1 + ax + (ax)^2 + (ax)^3 + \dots \Bigr) (1 - ax) \\
    &amp; = \Bigl( 1 + ax + (ax)^2 + (ax)^3 + \dots \Bigr) -
            \Bigl( (ax) - (ax)^2 - (ax)^3 - \dots \Bigr)
    = 1.
  \end{align*}
</p>

<!-- Section 2.16 -->
<h2 id="2.16">&sect; 2.16: The Bell series of an arithmetical function<a href="#2.16"></a></h2>
<h3 id="bell-series-of-f-modulo-p">Bell series of \( f \) modulo \( p \)<a href="#bell-series-of-f-modulo-p"></a></h3>
<p>
  \[
    f_p(x)
    = \sum_{n=0}^{\infty} f(p^n) x^n
    = f(1) + f(p) x + f(p^2) x^2 + f(p^3) x^3 + \dots
  \]
</p>
<h3 id="theorem-2.24-uniqueness-theorem">Theorem 2.24: Uniqueness theorem<a href="#theorem-2.24-uniqueness-theorem"></a></h3>
<p>
  \begin{align*}
    f(n)
    &amp; = f(p_1^{a_1} p_2^{a_2} \dots p_k^{a_k})
    = f(p_1^{a_1}) f(p_2^{a_2}) \dots f(p_k^{a_k}), \\ \\

    g(n)
    &amp; = g(p_1^{a_1} p_2^{a_2} \dots p_k^{a_k})
    = g(p_1^{a_1}) g(p_2^{a_2}) \dots g(p_k^{a_k}).  \\
  \end{align*}
</p>
<h3 id="example-1-mobius-funcion">Example 1: Mbius function<a href="#example-1-mobius-funcion"></a></h3>
<p>
  \begin{align*}
    \mu_p(x) = \sum_{n=0}^{\infty} \mu(p^n) x^n
    &amp; = \mu(1) + \mu(p) x + \mu(p^2) x^2 + \mu(p^3) x^3 + \dots \\
    &amp; = 1 - x + 0 + 0 + \dots \\
    &amp; = 1 - x.
  \end{align*}
</p>

<!-- Section 2.17 -->
<h2 id="2.17">&sect; 2.17: Bell series and Dirichlet multiplication<a href="#2.17"></a></h2>
<h3 id="power-series-multiplication">Power series multiplication<a href="#power-series-multiplication"></a></h3>
<p>
  \[
    A(x) = \sum_{n=0}^{\infty} a(n) x^n, \quad
    B(x) = \sum_{n=0}^{\infty} b(n) x^n, \quad
    A(x) B(x) = \sum_{n=0}^{\infty} \underbrace{\sum_{k=0}^n a(k) b(n - k)}_{c(n)} x^n.
  \]
</p>
<h3 id="relationship-with-dirichlet-multiplication">Relationship with Dirichlet multiplication<a href="#relationship-with-dirichlet-multiplication"></a></h3>
<p>
  \[
    (f * g)_p(x) = f_p(x) g_p(x).
  \]

  \[
    f_p(x) = \sum_{n=0}^{\infty} f(p^n) x^n, \quad
    g_p(x) = \sum_{n=0}^{\infty} g(p^n) x^n, \quad
    f_p(x) g_p(x) = \sum_{n=0}^{\infty} \sum_{k=0}^n f(p^k) g(p^{n-k}) x^n.
  \]

  \[
    h = f * g = \sum_{d \mid n} f(d) g\left( \frac{n}{d} \right).
  \]

  \[
    h_p(x)
    = \sum_{n=0}^{\infty} h(p^n) x^n
    = \sum_{n=0}^{\infty} \sum_{d \mid p^n} f(d) g\left( \frac{p^n}{d} \right) x^n
    = \sum_{n=0}^{\infty} \sum_{k=0}^{n} f(p^k) g(p^{n-k}) x^n.
  \]
</p>
<p>
  Some steps of Example 1:

  \[
    I(n)= \mu^2(n) * \lambda(n)
    \implies I_p(x) = \mu_p^2(x) \lambda_p(x)
  \]

  \[
    I_p(x) = \mu_p^2(x) \lambda_p(x)
    \iff 1 = \mu_p^2(x) \cdot \frac{1}{1 + x}
    \iff \mu_p^2(x) = 1 + x.
  \]
</p>
<p>
  Some steps of Example 2:

  \begin{align*}
    \frac{1}{1 - p^{\alpha}} \cdot \frac{1}{1 - x}
    &amp; = \frac{1}{1 - x - p^{\alpha}x + p^{\alpha}x^2} \\
    &amp; = \frac{1}{1 - (1 + p^{\alpha})x + p^{\alpha}x^2} \\
    &amp; = \frac{1}{1 - \sigma_{\alpha}(p)x + p^{\alpha}x^2}.
  \end{align*}

  Note that \( \sigma_{\alpha}(n) = \sum_{d\,\mid\,n} d^{\alpha}, \) so

  \[
    \sigma_{\alpha}(p)
    = \sum_{d\,\mid\,p} d^{\alpha}
    = 1^{\alpha} + p^{\alpha}
    = 1 + p^{\alpha}.
  \]
</p>
<p>
  Some steps of Example 3: Showing That \( f(n) = 2^{\nu(n)} \) is
  multiplicative:

  \[
    f(n) = 2^{\nu(n)}.
  \]

  \[
    f(p_1^{\alpha_1} p_2^{\alpha_2} \dots p_k^{\alpha_k})
    = 2^{\nu(p_1^{\alpha_1} p_2^{\alpha_2} \dots p_k^{\alpha_k})} = 2^k.
  \]

  \[
    f(p_1^{\alpha_1}) f(p_2^{\alpha_2}) \dots f(p_k^{\alpha_k})
    = 2^{\nu(p_1^{\alpha_1})} 2^{\nu(p_2^{\alpha_2})} \dots 2^{\nu(p_k^{\alpha_k})}
    = \underbrace{2 \cdot 2 \cdot \dots \cdot 2}_{k \text{ times}}.
    = 2^k.
  \]
</p>

<!-- Section 2.18 -->
<h2 id="2.18">&sect; 2.18: Derivatives of arithmetical functions<a href="#2.18"></a></h2>
<h3 id="ordinary-derivative">Ordinary derivative<a href="#ordinary-derivative"></a></h3>
<p>
  \[
    (f + g)' = f' + g'.
  \]

  \[
    (fg)' = f'g + fg'.
  \]

  \[
    \left( f^{-1} \right)' = \frac{-f'}{f^2} = -f' \cdot (f \cdot f)^{-1}.
  \]
</p>
<h3 id="similarities-in-special-derivative">Similarities in special derivative<a href="#similarities-in-special-derivative"></a></h3>
<p>
  \[
    f'(n) = f(n) \log n.
  \]

  \[
    (f + g)' = f' + g'.
  \]

  \[
    (f * g)' = f' * g + f * g'.
  \]

  \[
    \left( f^{-1} \right)' = -f' * (f * f)^{-1}.
  \]
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/cc/iant/ch02.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Notes on Introduction to Analytic Number Theory</title>
<link>https://susam.net/cc/iant/notes.html</link>
<guid isPermaLink="false">idkhz</guid>
<pubDate>Sun, 07 Mar 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h1>Notes on Introduction to Analytic Number Theory</h1>
<p>
  This page contains an archive of notes from the book
  <em>Introduction to Analytic Number Theory</em> by Tom M. Apostol
  (1976).
</p>
<p>
  Note that this set of notes is not meant to be a systematic
  exposition of analytic number theory.  Instead this is just a
  collection of examples that illustrate some of the theorems in the
  reference textbook and intermediate steps that are not explicitly
  expressed in the book.  These boards were used to aid the
  discussions during book discussion meetings.  As a result, the
  content of these boards is informal in nature and is not intended to
  be a substitute for the book or the actual discussion meetings.
</p>
<p>
  If you find any mistakes in the content of the board files, please
  <a href="https://github.com/susam/susam.net/issues/new">create a new
  issue</a> or <a href="https://github.com/susam/susam.net">send a
  pull request</a>.
</p>
<ul>
  <li>
    <a href="ch02.html">Notes on Chapter 2: Arithmetical Functions and Dirichlet Multiplication</a>
  </li>
  <!--
  <li>
    <a href="ch03.html">Notes on Chapter 3: Averages of Arithmetical Functions</a>
  </li>
  <li>
    <a href="ch04.html">Notes on Chapter 4: Some Elementary Theorems on the Distribution of Prime Numbers</a>
  </li>
  <li>
    <a href="ch05.html">Notes on Chapter 5: Congruences</a>
  </li>
  <li>
    <a href="ch06.html">Notes on Chapter 6: Finite Abelian Groups and Their Characters</a>
  </li>
  <li>
    <a href="ch07.html">Notes on Chapter 7: Dirichlet's Theorem on Primes in Arithmetical Progressions</a>
  </li>
  <li>
    <a href="ch08.html">Notes on Chapter 8: Periodic Arithmetical Functions and Gauss Sums</a>
  </li>
  <li>
    <a href="ch09.html">Notes on Chapter 9: Quadratic Residues and the Quadratic Reciprocity Law</a>
  </li>
  <li>
    <a href="ch10.html">Notes on Chapter 10: Primitive Roots</a>
  </li>
  <li>
    <a href="ch11.html">Notes on Chapter 11: Dirichlet Series and Euler Products</a>
  </li>
  <li>
    <a href="ch12.html">Notes on Chapter 12: The Functions and \( \zeta(s) \) and \( L(s, \chi) \)</a>
  </li>
  <li>
    <a href="ch13.html">Notes on Chapter 13: Analytic Proof of Prime Number Theorem</a>
  </li>
  <li>
    <a href="ch14.html">Notes on Chapter 14: Partitions</a>
  </li>
  -->
</ul>
<p>
  More notes coming soon!  We have all the meeting notes safely
  archived.  Just need to format them and publish them here.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/cc/iant/notes.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Introduction to Analytic Number Theory Book Discussions</title>
<link>https://susam.net/cc/iant/</link>
<guid isPermaLink="false">trkge</guid>
<pubDate>Fri, 05 Mar 2021 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h1>Introduction to Analytic Number Theory Book Discussions</h1>
<div class="highlight">
  This meeting series is complete!  There are no new meetings
  happening for this book.  Go to the meetings
  <a href="../">main page</a> to find out about current
  active meetings.
</div>
<p>
  The following content on this page is an archive of the content as
  it appeared on the last day of meeting for this book.
</p>
<hr>
<div class="highlight">
  <p>
    Meeting time: 17:00 UTC from Tuesday to Friday,
    usually.<sup>&dagger;</sup>
  </p>
  <p>
    Meeting duration: 40 minutes.
  </p>
  <p>
    Meeting link: <a href="https://bit.ly/spzoom2">bit.ly/spzoom2</a>
  </p>
  <p>
    Meeting log: <a href="log.html">120 meetings</a>
  </p>
  <p>
    Reference Book: <a href="https://www.springer.com/gp/book/9780387901633"><em>Introduction
    to Analytic Number Theory</em></a> by Tom M. Apostol (1976)
  </p>
  <p>
    Chapter notes: <a href="notes.html">Notes</a>
  </p>
  <p>
    Started: 05 Mar 2021
  </p>
  <p>
    Ended: 01 Oct 2021
  </p>
</div>
<p>
  <small>&dagger; There are some exceptions to this schedule
  occasionally.  <a href="../#join">Join our channel</a> to
  receive schedule updates.</small>
</p>
<p>
  The primary reference book for these meetings is
  <em>Introduction to Analytic Number Theory</em> written by Tom
  M. Apostol.  Admittedly, the book is quite expensive but you may
  find a relatively cheap paperback (softcover) copy on some websites.
</p>
<p>
  These meetings are hosted by Susam and attended by some members of
  <code>#math</code> and <code>#algorithms</code> channels of Libera
  IRC network as well as by some members
  from <a href="https://news.ycombinator.com/">Hacker News</a>.
</p>
<p>
  You are welcome to join these meetings anytime.  If you are
  concerned that the meetings may not make sense if you join when we
  are in the middle of a chapter, please free to talk to us about it
  in the <a href="../#join">group channel</a>.  I can
  recommend the next best time to begin joining the meetings.
  Usually, it would be when we begin reading a new section or chapter
  that is fairly self-contained and does not depend a lot on material
  we have read previously.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/cc/iant/">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/number-theory.html">#number-theory</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/meetup.html">#meetup</a>
</p>
]]>
</description>
</item>
<item>
<title>Grothendieck Prime</title>
<link>https://susam.net/grothendieck-prime.html</link>
<guid isPermaLink="false">bijxn</guid>
<pubDate>Fri, 28 Aug 2020 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Quoting from the article
  <a href="https://www.ams.org/notices/200410/fea-grothendieck-part2.pdf">Comme
  Appel du Nant&mdash;As If Summoned from the Void: The Life of
  Alexandre Grothendieck</a> by Allyn Jackson published in <em>Notices
  of the AMS, Volume 51, Number 10</em>:
</p>
<blockquote>
  One striking characteristic of Grothendieck's mode of thinking is
  that it seemed to rely so little on examples.  This can be seen in
  the legend of the so-called "Grothendieck prime".  In a mathematical
  conversation, someone suggested to Grothendieck that they should
  consider a particular prime number.  "You mean an actual number?"
  Grothendieck asked.  The other person replied, yes, an actual prime
  number.  Grothendieck suggested, "All right, take 57."
</blockquote>
<!-- ### -->
<p>
  <a href="https://susam.net/grothendieck-prime.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/quote.html">#quote</a>
</p>
]]>
</description>
</item>
<item>
<title>Leap Year Test in K&amp;R</title>
<link>https://susam.net/leap-year-test-in-knr.html</link>
<guid isPermaLink="false">tzjpk</guid>
<pubDate>Sat, 29 Feb 2020 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  About 18 years ago, while learning to program a computer using C, I
  learnt the following test for leap year from the book <em>The C
  Programming Language, 2nd ed.</em> (K&amp;R) written by Brian
  Kernighan and Dennis Ritchie.  Section 2.5 (Arithmetic Operators) of
  the book uses the following test:
</p>
<pre>
<code>(year % 4 == 0 &amp;&amp; year % 100 != 0) || year % 400 == 0</code>
</pre>
<p>
  It came as a surprise to me.  Prior to reading this, I did not know
  that centurial years are not leap years except for those centurial
  years that are also divisible by 400.  Until then, I always
  incorrectly thought that all years divisible by 4 are leap years.  I
  have witnessed only one centurial year, namely the year 2000, which
  happens to be divisible by 400.  As a result, the year 2000 proved
  to be a leap year and my misconception remained unchallenged for
  another few years until I finally came across the above test in
  K&amp;R.
</p>
<p>
  Now that I understand that centurial years are not leap years unless
  divisible by 400, it is easy to confirm this with the
  Unix <code>cal</code> command.  Enter <code>cal 1800</code>
  or <code>cal 1900</code> and we see calendars of non-leap years.
  But enter <code>cal 2000</code> and we see the calendar of a leap
  year.
</p>
<p>
  By the way, the following leap year test is equally effective:
</p>
<pre>
<code>year % 4 == 0 &amp;&amp; (year % 100 != 0 || year % 400 == 0)</code>
</pre>
<hr>
<p>
  <strong>Update:</strong> In the
  <a href="comments/leap-year-test-in-knr.html">comments section</a>,
  Thaumasiotes explains why both tests work.  Let me take the liberty
  of elaborating that comment further with a truth table.  We use the
  notation <code>A</code>, <code>B</code> and <code>C</code>
  respectively, for the three comparisons in the above expressions.
  Then the two tests above can be expressed as the following boolean
  expressions:
</p>
<ul>
  <li><code>(A &amp;&amp; B) || C</code></li>
  <li><code>A &amp;&amp; (B || C)</code></li>
</ul>
<p>
  Now normally these two boolean expressions are not equivalent.  The
  truth table below shows this:
</p>
<table class="grid center textcenter">
  <tr>
    <th><code>A</code></th>
    <th><code>B</code></th>
    <th><code>C</code></th>
    <th><code>(A &amp;&amp; B) || C</code></th>
    <th><code>A &amp;&amp; (B || C)</code></th>
  </tr>
  <tr>
    <td>F</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
  </tr>
  <tr>
    <td>F</td>
    <td>F</td>
    <td>T</td>
    <td>T</td>
    <td>F</td>
  </tr>
  <tr>
    <td>F</td>
    <td>T</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
  </tr>
  <tr>
    <td>F</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
    <td>F</td>
  </tr>
  <tr>
    <td>T</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
    <td>F</td>
  </tr>
  <tr>
    <td>T</td>
    <td>F</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
  </tr>
  <tr>
    <td>T</td>
    <td>T</td>
    <td>F</td>
    <td>T</td>
    <td>T</td>
  </tr>
  <tr>
    <td>T</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
    <td>T</td>
  </tr>
</table>
<p>
  We see that there are two cases where the last two columns differ.
  This confirms that the two boolean expressions are not equivalent.
  The two cases where the boolean expressions yield different results
  occur when <code>A</code> is false and <code>C</code> is true.  But
  these cases are impossible!  If <code>A</code> is false
  and <code>C</code> is true, it means we have <code>year % 4 !=
  0</code> and <code>year % 400 == 0</code> which is impossible.
</p>
<p>
  If <code>year % 400 == 0</code> is true, then <code>year % 4 ==
  0</code> must also hold true.  In other words, if <code>C</code> is
  true, <code>A</code> must also be true.  Therefore, the two cases
  where the last two columns differ cannot occur and may be ignored.
  The last two columns are equal in all other cases and that is why
  the two tests we have are equivalent.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/leap-year-test-in-knr.html">Read on website</a> |
  <a href="https://susam.net/tag/c.html">#c</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>From Vector Spaces to Periodic Functions</title>
<link>https://susam.net/from-vector-spaces-to-periodic-functions.html</link>
<guid isPermaLink="false">btfyp</guid>
<pubDate>Wed, 30 Jan 2019 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="vector-spaces">Vector Spaces<a href="#vector-spaces"></a></h2>
<p>
  A fascinating result that appears in linear algebra is the fact that
  the set of real numbers \( \mathbb{R} \) is a vector space over the
  set of rational numbers \( \mathbb{Q}.  \)  This may appear
  surprising at first but it is easy to show that it is indeed so by
  checking that all eight axioms of vector spaces hold good:
</p>
<ol>
  <li>
    <p>
      Commutativity of vector addition:<br>
      \( x + y = y + x \) for all \( x, y \in \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Associativity of vector addition:<br>
      \( x + (y + z) = (x + y) + z \) for all \( x, y, z \in
      \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Existence of additive identity vector:<br>
      We have \( 0 \in \mathbb{R} \) such that \( x + 0 = x \) for all
      \( x \in \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Existence of additive inverse vectors:<br>
      There exists \( -x \in \mathbb{R} \) for every \( x \in
      \mathbb{R} \) such that \( x + (-x) = 0.  \)
    </p>
  </li>
  <li>
    <p>
      Associativity of scalar multiplication:<br>
      \( a(bx) = (ab)x \) for all \( a, b \in \mathbb{Q} \) and all \(
      x \in \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Distributivity of scalar multiplication over vector
      addition:<br>
      \( a(x + y) = ax + by \) for all \( a \in \mathbb{Q} \) and all
      \( x, y \in \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Distributivity of scalar multiplication over scalar
      addition:<br>
      \( (a + b)x = ax + bx \) for all \( a, b \in \mathbb{Q} \) and
      all \( x \in \mathbb{R}.  \)
    </p>
  </li>
  <li>
    <p>
      Existence of scalar multiplicative identity:<br>
      We have \( 1 \in \mathbb{Q} \) such that \( 1 \cdot x = x \) for
      all \( x \in \mathbb{R}.  \)
    </p>
  </li>
</ol>
<p>
  This shows that the set of real numbers \( \mathbb{R} \) forms a
  vector space over the field of rational numbers \( \mathbb{Q}.  \)
  Another quick way to arrive at this fact is to observe that \(
  \mathbb{Q} \subseteq \mathbb{R}, \) that is, \( \mathbb{Q} \) is a
  subfield of \( \mathbb{R}.  \)  Any field is a vector space over any
  of its subfields, so \( \mathbb{R} \) must be a vector space over \(
  \mathbb{Q}.  \)
</p>
<p>
  We can also show that \( \mathbb{R} \) is an infinite dimensional
  vector space over \( \mathbb{Q}.  \)  Let us assume the opposite,
  i.e. \( \mathbb{R} \) is finite dimensional.  Let \( r_1, \dots, r_n
  \) be the basis for this vector space.  Therefore for each \( r \in
  \mathbb{R}, \) we have unique \( q_1, \dots, q_n \in \mathbb{Q} \)
  such that \( r = q_1 r_1 + \dots + q_n r_n.  \)  Thus there is a
  bijection between \( \mathbb{Q}^n \) and \( \mathbb{R}.  \)  This is
  a contradiction because \( \mathbb{Q}^n \) is countable whereas \(
  \mathbb{R} \) is uncountable.  Therefore \( \mathbb{R} \) must be an
  infinite dimensional vector space over \( \mathbb{Q}.  \)
</p>
<h2 id="problem">Problem<a href="#problem"></a></h2>
<p>
  Here is an interesting problem related to vector spaces that I came
  across recently:
</p>
<div class="highlight">
  <p>
    Define two periodic functions \( f \) and \( g \) from \(
    \mathbb{R} \) to \( \mathbb{R} \) such that their sum \( f + g \)
    is the identity function.  The axiom of choice is allowed.
  </p>
  <p>
    A function \( f \) is periodic if there exists \( p \gt 0 \) such
    that \( f(x + p) = f(x) \) for all \( x \) in the domain.
  </p>
</div>
<p>
  <em>If you want to think about this problem, this is a good time to
  pause and think about it.  There are spoilers ahead.</em>
</p>
<h2 id="solution">Solution<a href="#solution"></a></h2>
<p>
  The axiom of choice is equivalent to the statement that every vector
  space has a basis.  Since the set of real numbers \( \mathbb{R} \)
  is a vector space over the set of rational numbers \( \mathbb{Q}, \)
  there must be a basis \( \mathcal{H} \subseteq \mathbb{R} \) such
  that every real number \( x \) can be written uniquely as a finite
  linear combination of elements of \( \mathcal{H} \) with rational
  coefficients, that is,

  \[
    x = \sum_{a \in \mathcal{H}} x_a a
  \]

  where each \( x_a \in \mathbb{Q} \) and \( \{ a \in \mathcal{H} \mid
  x_a \ne 0 \} \) is finite.  The set \( \mathcal{H} \) is also known
  as the Hamel basis.
</p>
<p>
  In the above expansion of \( x, \) we use the notation \( x_a \) to
  denote the rational number that appears as the coefficient of the
  basis vector \( a.  \)  Therefore \( (x + y)_{a} = x_a + y_a \) for
  all \( x, y \in \mathbb{R} \) and all \( a \in \mathcal{H}.  \)
</p>
<p>
  We know that \( b_a = 0 \) for distinct \( a, b \in \mathcal{H} \)
  because \( a \) and \( b \) are basis vectors.  Thus \( (x + b)_{a}
  = x_a + b_a = x_a + 0 = x_a \) for all \( x \in \mathbb{R} \) and
  distinct \( a, b \in \mathcal{H}.  \)  This shows that a function \(
  f(x) = x_a \) is a periodic function with period \( b \) for any \(
  a \in \mathcal{H} \) and any \( b \in \mathcal{H} \setminus \{ a \}.  \)
</p>
<p>
  Let us define two functions:

  \begin{align*}
    g(x) &amp; = \sum_{a \in \mathcal{H} \setminus \{ b \}} x_a a,
    &amp;
    h(x) &amp; = x_b b.
  \end{align*}

  where \( b \in \mathcal{H} \) and \( x \in \mathbb{R}.  \)  Now \(
  g(x) \) is a periodic function with period \( b \) for any \( b \in
  \mathcal{H} \) and \( h(x) \) is a periodic function with period \(
  c \) for any \( c \in \mathcal{H} \setminus \{ b \}.  \)  Further,

  \[
    g(x) + h(x)
    = \left( \sum_{a \in \mathcal{H} \setminus \{ b \}} x_a a \right) + x_b b
    = \sum_{a \in \mathcal{H}} x_a a
    = x.
  \]

  Thus \( g(x) \) and \( h(x) \) are two periodic functions such that
  their sum is the identity function.
</p>
<h2 id="references">References<a href="#references"></a></h2>
<ul>
  <li>
    <a href="https://mathworld.wolfram.com/VectorSpace.html">Vector Space</a>
    by Eric W. Weisstein
  </li>
  <li>
    <a href="https://web.archive.org/web/20141026224511/https://drexel28.wordpress.com/2010/10/22/the-dimension-of-r-over-q/">The Dimension of R over Q</a>
    by Alex Youcis
  </li>
  <li>
    <a href="https://mathblag.wordpress.com/2013/09/01/sums-of-periodic-functions/">Sums of Periodic Functions</a>
    by David Radcliffe
  </li>
</ul>
<!-- ### -->
<p>
  <a href="https://susam.net/from-vector-spaces-to-periodic-functions.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Temperature Conversion</title>
<link>https://susam.net/temperature-conversion.html</link>
<guid isPermaLink="false">wztkz</guid>
<pubDate>Sun, 05 Jan 2014 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="approximation-problem">Approximation Problem<a href="#approximation-problem"></a></h2>
<p>
  Everytime I travel to the US, one thing that troubles me a little is
  having to convert temperature from the Celsius scale to the
  Fahrenheit scale and vice versa.  The exact conversion formulas are:

  \begin{align*}
    f &amp; = \frac{9c}{5} + 32, \\
    c &amp; = \frac{5(f - 32)}{9}
  \end{align*}

  where \( f \) represents the temperature value in Fahrenheit and
  \( c \) represents the temperature value in Celsius.
</p>
<p>
  While the formulas above are accurate, they are not very convenient
  to mentally figure what I need to set a room thermostat in
  Fahrenheit scale to if I want to keep the room at, say,
  25&nbsp;&deg;C.  For this particular case, I have memorised that
  25&nbsp;&deg;C is 77&nbsp;&deg;F.  This combined with the fact that
  every 5&nbsp;&deg;C interval corresponds to an interval of
  9&nbsp;&deg;F, it is easy to mentally compute that 20&nbsp;&deg;C is
  68&nbsp;&deg;F or 22.5&nbsp;&deg;C is 72.5&nbsp;&deg;F.  It would
  still be nice to find an easy way to mentally convert any arbitrary
  temperature in one scale to the other scale.
</p>
<p>
  In my last trip to the US, I decided to devise a few approximation
  methods to convert temperature from the Fahrenheit scale to the
  Celsius scale and vice versa.  I arrived at two methods: one to
  convert temperature value in Fahrenheit to Celsius and another to
  convert from Fahrenheit to Celsius.  Both these methods are based on
  the exact conversion formulas but they sacrifice accuracy a little
  bit in favour of simplifying the computations, so that they can be
  performed mentally.
</p>
<h2 id="crude-approximation-methods">Crude Approximation Methods<a href="#crude-approximation-methods"></a></h2>
<p>
  Before we dive into the
  <a href="#refined-approximation-methods">refined approximation
  methods</a> I have arrived at, let us first see a very popular
  method that obtains a crude approximation of the result of
  temperature conversion from &deg;C to &deg;F and vice versa pretty
  quickly.
</p>
<p>
  To go from &deg;C to &deg;F, we perform the following two steps:
</p>
<ol>
  <li>Double the value in Celsius.</li>
  <li>Add 30 to the previous result.</li>
</ol>
<p>
  To go from &deg;F to &deg;C, we perform the inverse:
</p>
<ol>
  <li>Subtract 30 from the value in Fahrenheit.</li>
  <li>Halve the result.</li>
</ol>
<p>
  We arrive at the above methods by approximating 9/5 and 32 in the
  exact conversion formulas with 2 and 30 respectively.  These methods
  can be performed mentally quite fast but this speed of mental
  calculation comes at the cost of accuracy.  That's why I call them
  crude approximation methods.
</p>
<p>
  The first method converts 10&nbsp;&deg;C exactly to 50&nbsp;&deg;F
  without any error.  But then it introduces an error of 1&nbsp;&deg;F
  for every 5&nbsp;&deg;C interval.  For example, the error is
  3&nbsp;&deg;F for 25&nbsp;&deg;C and 18&nbsp;&deg;F for
  100&nbsp;&deg;C.
</p>
<p>
  Similarly, the second method converts 50&nbsp;&deg;F exactly to
  10&nbsp;&deg;C without any error.  But it introduces an error of
  0.5&nbsp;&deg;C for every 9&nbsp;&deg;F interval.  For example, the
  error is 1.5&nbsp;&deg;C for 77&nbsp;&deg;C and 9&nbsp;&deg;C for
  212&nbsp;&deg;F.
</p>
<h3 id="examples-for-crude-method">Examples<a href="#examples-for-crude-method"></a></h3>
<p>
  Let us do a few examples to see how well the crude approximation
  methods work.  Let us say, we want to convert 24&nbsp;&deg;C to
  &deg;F.
</p>
<ol>
  <li>Double 24.  We get 48.</li>
  <li>Add 30 to it.  We get 78.</li>
</ol>
<p>
  The exact value for 24&nbsp;&deg;C is 75.2&nbsp;&deg;F.  This
  approximation method overestimated the actual temperature in
  Fahrenheit by 2.8&nbsp;&deg;F.
</p>
<p>
  Let us now convert 75&nbsp;&deg;F to &deg;C.
</p>
<ol>
  <li>Subtract 30.  We get 45.</li>
  <li>Divide by 2.  We get 22.5.</li>
</ol>
<p>
  The exact value for 75&nbsp;&deg;F is 23.89&nbsp;&deg;C.  This
  approximation method underestimated the actual temperature in
  Celsius by 1.39&nbsp;&deg;C.
</p>
<p>
  Can we do better?
</p>
<h2 id="refined-approximation-methods">Refined Approximation Methods<a href="#refined-approximation-methods"></a></h2>
<p>
  This section presents the refined approximation methods that I have
  arrived at.  They are a little slower to perform mentally than the
  crude approximation methods but they are more accurate.
</p>
<p>
  To keep the methods convenient enough to perform mentally, we work
  with integers only.  We always start with an integer value in
  Celsius or Fahrenheit.  The result of conversion is also an integer.
  If a fraction arises in an intermediate step, we discard the
  fractional part.  For example, if a step requires us to calculate
  one-tenth of a number, say, 25, we consider the result to be 2.
  Similarly, if a step requires us to halve the number 25, we consider
  the result to be 12.  This is also known as <em>truncated
  division</em> or <em>integer division</em>.
</p>
<p>
  To go from &deg;C to &deg;F, here is my quick three-step
  approximation method:
</p>
<ol>
  <li>
    Subtract one-tenth of the value in Celsius from itself.
  </li>
  <li>
    Double the previous result.
  </li>
  <li>
    Add 31 to the previous result.
  </li>
</ol>
<p>
  The approximation error due to this method does not exceed
  1&nbsp;&deg;F in magnitude.  In terms of Celsius, the approximation
  error does not exceed 0.56&nbsp;&deg;C.  I believe this is pretty
  good if we are talking about setting the thermostat temperature.
</p>
<p>
  To go from &deg;F to &deg;C, we perform a rough inverse of the above
  steps:
</p>
<ol>
  <li>
    Subtract 31 from the value in Fahrenheit.
  </li>
  <li>
    Halve the result.
  </li>
  <li>
    Add one-tenth of the previous result to itself.
  </li>
</ol>
<p>
  In fact, for integer temperature values between 32&nbsp;&deg;F
  (0&nbsp;&deg;C) and 86&nbsp;&deg;F (30&nbsp;&deg;C), the
  approximation error due to this method does not exceed
  1.12&nbsp;&deg;C.  Further, for integer temperature values between
  &minus;148&nbsp;&deg;F (&minus;100&nbsp;&deg;C) and 212&nbsp;&deg;F
  (100&nbsp;&deg;C), the approximation error does not exceed
  1.89&nbsp;&deg;C.  This is pretty good if we are talking about the
  weather.
</p>
<h3 id="examples-for-refined-method">Examples<a href="#examples-for-refined-method"></a></h3>
<p>
  Let us do a few examples to see how well the three-step methods
  above work.  Let us say, we want to convert 24&nbsp;&deg;C to
  &deg;F.
</p>
<ol>
  <li>
    Subtract one-tenth of 24 from itself, i.e. subtract 2 from 24.  We
    get 22.
  </li>
  <li>
    Double 22.  We get 44.
  </li>
  <li>
    Add 31 to 44.  We get 75.
  </li>
</ol>
<p>
  The exact value for 22&nbsp;&deg;C is 75.2&nbsp;&deg;F.  The
  approximation method has underestimated the actual temperature in
  Fahrenheit by only 0.2&nbsp;&deg;F.
</p>
<p>
  Let us now try to convert 75&nbsp;&deg;F to &deg;C.
</p>
<ol>
  <li>
    Subtract 31 from 75.  We get 44.
  </li>
  <li>
    Halve 44.  We get 22.
  </li>
  <li>
    Add one-tenth of 22 to itself, i.e. add 2 to 22.  We get 24.
  </li>
</ol>
<p>
  The exact value for 75&nbsp;&deg;F is 23.89&nbsp;&deg;C, so this
  approximation method overestimated the actual temperature in Celsius
  by 0.11&nbsp;&deg;C only.
</p>
<p>
  If you were looking only for quick methods to convert temperature
  values in Fahrenheit to Celsius and vice versa, this is all you need
  to know.  You may skip the remaining post unless you want to know
  why these methods work.
</p>
<h3 id="analysis">Analysis<a href="#analysis"></a></h3>
<p>
  In this section, we will see why the refined approximation methods
  work so well.  Throughout this section we will use the notation \(
  [x] \) to mean the integer part of \( x.  \)  For example, \( [10.2]
  = [-10.2] = 10.  \)
</p>
<h4 id="celsius-to-fahrenheit-conversion">Celsius to Fahrenheit Conversion<a href="#celsius-to-fahrenheit-conversion"></a></h4>
<p>
  The method to convert temperature value from Celsius to Fahrenheit
  is equivalent to

  \[
    \overset{\approx}{f} =
    2 \left(c - \left[ \frac{c}{10} \right] \right) + 31
  \]

  where \( c \) is the temperature value in Celsius and \(
  \overset{\approx}{f} \) is the approximate temperature value in
  Fahrenheit.
</p>
<p>
  Here is a brief justification for this:
</p>
<ol>
  <li>
    Subtracting one-tenth (truncated division by 10) of \( c \) from
    itself gives us \( c - \left[ \frac{c}{10} \right].  \)
  </li>
  <li>
    Doubling the previous result gives us \( 2 \left(c - \left[
    \frac{c}{10} \right] \right).  \)
  </li>
  <li>
    Adding \( 31 \) to the previous result gives us \( 2 \left(c -
    \left[ \frac{c}{10} \right] \right) + 31.  \)
  </li>
</ol>
<p>
  Now let us see how we arrive at the above approximate conversion
  formula.  It's not too different from the exact conversion formula.
  The exact formula to convert temperature from Celsius to Fahrenheit
  is

  \[
    f  = \frac{9c}{5} + 32.
  \]

  This can be rewritten as

  \[
    f = 2 \left(c - \frac{c}{10} \right) + 32.
  \]

  We don't want to deal with fractions, so we decide to approximate \(
  \frac{c}{10} \) in the above formula with \( \left[
  \frac{c}{10} \right] \) and get

  \[
    \overset{\sim}{f} =
    2 \left(c - \left[ \frac{c}{10} \right] \right) + 32.
  \]

  where \( \overset{\sim}{f} \) is an approximation of the value in
  Fahrenheit.  The truncated division has the effect of potentially
  overestimating the final result by a value that is less than \( 2
 .  \)  This is the approximation error.
</p>
<p>
  If we define the approximation error as \( \overset{\sim}{f} - f, \)
  then the approximation error lies in the half-open interval \( [0,
  2).  \)  To ensure that the magnitude of the error never exceeds \( 1
 , \) i.e. to make the approximation error lie in the half-open
  interval \( [-1, 1), \) we subtract \( 1 \) from the above formula
  and get

  \[
    \overset{\approx}{f} =
    2 \left(c - \left[ \frac{c}{10} \right] \right) + 31.
  \]

  This is the formula that the three-step method to convert
  temperature from Celsius to Fahrenheit is equivalent to.
</p>
<h4 id="fahrenheit-to-celsius-conversion">Fahrenheit to Celsius Conversion<a href="#fahrenheit-to-celsius-conversion"></a></h4>
<p>
  The inverse method to convert temperature value from Fahrenheit to
  Celsius amounts to this formula:

  \[
    \overset{\approx}{c} =
    \left[ \frac{f - 31}{2} \right] +
    \left[ \frac{f - 31}{20} \right]
  \]

  where \( f \) is the temperature value in Fahrenheit and \(
  \overset{\approx}{c} \) is the approximate temperature value in
  Celsius.
</p>
<p>
  Here is a brief justification for this:
</p>
<ol>
  <li>
    Subtracting \( 31 \) from \( f \) gives us \( f - 31.  \)
  </li>
  <li>
    Halving (truncated division by \( 2 \)) the previous result gives
    us \( \left[ \frac{f - 31}{2} \right].  \)
  </li>
  <li>
    Adding one-tenth (truncated division by \( 10 \)) of the previous
    result to itself gives us \( \left[ \frac{f - 31}{2} \right] +
    \left[ \frac{1}{10} \left[ \frac{f - 31}{2} \right] \right] =
    \left[ \frac{f - 31}{2} \right] + \left[ \frac{f - 31}{20} \right].  \)
  </li>
</ol>
<p>
  This is roughly an inverse of all the steps for converting a
  temperature value from Celsius to Fahrenheit.  Let us see if this is
  close to the exact conversion formula

  \[
    c = \frac{5(f - 32)}{9}.
  \]
</p>
<p>
  It turns out that it is in fact close to the exact conversion formula as
  follows:

  \begin{align*}
    c &amp; =       \frac{5(f - 32)}{9} \\
      &amp; \approx 0.55 (f - 31) \\
      &amp; =       \frac{11 (f - 31)}{20} \\
      &amp; =       \frac{f - 31}{2} + \frac{f - 31}{20} \\
      &amp; \approx \left[ \frac{f - 31}{2} \right] +
                \left[ \frac{f - 31}{20} \right]
      = \overset{\approx}{c}
  \end{align*}
</p>
<p>
  Like we discussed earlier, the magnitude of the approximation error
  does not exceed 1.89&nbsp;&deg;C for integer values between
  &minus;148&nbsp;&deg;F and 212&nbsp;&deg;F.  The error here is a
  little bit more than the previous approximation method to convert
  temperature in Celsius to Fahrenheit but it is still small enough to
  give us a reasonably good estimate of what a temperature value in
  Fahrenheit would look like in Celsius when we are talking about the
  weather.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/temperature-conversion.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Loopy C Puzzle</title>
<link>https://susam.net/loopy-c-puzzle.html</link>
<guid isPermaLink="false">yuzqb</guid>
<pubDate>Sat, 01 Oct 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="integer-underflow">Integer Underflow<a href="#integer-underflow"></a></h2>
<p>
  Let us talk a little bit about integer underflow and undefined
  behaviour in C before we discuss the puzzle I want to share in this
  post.
</p>
<pre>
<code>#include &lt;stdio.h&gt;

int main()
{
    int i;
    for (i = 0; i &lt; 6; i--) {
        printf(".");
    }
    return 0;
}</code>
</pre>
<p>
  This code invokes undefined behaviour.  The value in variable
  <code>i</code> decrements to <code>INT_MIN</code> after
  <code>|INT_MIN|</code> iterations.  In the next iteration, there is a
  negative overflow which is undefined for signed integers in C.  On
  many implementations though, <code>INT_MIN - 1</code> wraps around
  to <code>INT_MAX</code>.  Since <code>INT_MAX</code> is not less than
  <code>6</code>, the loop terminates.  With such implementations, this
  code prints print <code>|INT_MIN| + 1</code> dots.  With 32-bit integers,
  that amounts to 2147483649 dots.  Here is one such example output:
</p>
<pre>
<samp>$ <kbd>gcc -std=c89 -Wall -Wextra -pedantic foo.c &amp;&amp; ./a.out | wc -c</kbd>
2147483649</samp>
</pre>
<p>
  It is worth noting that the above behaviour is only one of the many
  possible ones.  The code invokes undefined behaviour and the ISO
  standard imposes no requirements on a specific implementation of the
  compiler regarding what the behaviour of such code should be.  For
  example, an implementation could also exploit the undefined
  behaviour to turn the loop into an infinite loop.  In fact, GCC does
  optimise it to an infinite loop if we compile the code with
  the <code>-O2</code> option.
</p>
<pre>
<samp><kbd># This never terminates!</kbd>
$ <kbd>gcc -O2 -std=c89 -Wall -Wextra -pedantic foo.c &amp;&amp; ./a.out</kbd></samp>
</pre>
<h2 id="puzzle">Puzzle<a href="#puzzle"></a></h2>
<p>
  Let us take a look at the puzzle now.
</p>
<div class="highlight">
<p>
  Add or modify exactly one operator in the following code such that
  it prints exactly 6 dots.
</p>
<pre>
<code>for (i = 0; i &lt; 6; i--) {
    printf(".");
}</code>
</pre>
</div>
<p>
  An obvious solution is to change <code>i--</code>
  to <code>i++</code>.
</p>
<pre>
<code>for (i = 0; i &lt; 6; i++) {
    printf(".");
}</code>
</pre>
<p>
  There are a few more solutions to this puzzle.  One of the solutions
  is very interesting.  We will discuss the interesting solution in
  detail below.
</p>
<h2 id="solutions">Solutions<a href="#solutions"></a></h2>
<p>
  <em><strong>Update on 02 Oct 2011:</strong> The puzzle has been
  solved in the <a href="comments/loopy-c-puzzle.html">comments</a>
  section.  We will discuss the solutions now.  If you want to think
  about the problem before you see the solutions, this is a good time
  to pause and think about it.  There are spoilers ahead.</em>
</p>
<p>
  Here is a list of some solutions:
</p>
<ul>
  <li>
    <code>for (i = 0; i &lt; 6; i++)</code>
  </li>
  <li>
    <code>for (i = 0; i &lt; 6; ++i)</code>
  </li>
  <li>
    <code>for (i = 0; -i &lt; 6; i--)</code>
  </li>
  <li>
    <code>for (i = 0; i + 6; i--)</code>
  </li>
  <li>
    <code>for (i = 0; i ^= 6; i--)</code>
  </li>
</ul>
<p>
  The last solution involving the bitwise XOR operation is not
  immediately obvious.  A little analysis is required to understand
  why it works.
</p>
<h2 id="generalisation">Generalisation<a href="#generalisation"></a></h2>
<p>
  Let us generalise the puzzle by replacing \( 6 \) in the loop with
  an arbitrary positive integer \( n.  \)  The loop in the last
  solution now becomes:
</p>
<pre>
<code>for (i = 0; i ^= n; i--) {
    printf(".");
}</code>
</pre>
<p>
  If we denote the value of the variable <code>i</code> set by the
  execution of <code>i ^= n</code> after \( k \) dots are printed as
  \( f(k), \) then

  \[
    f(k) =
      \begin{cases}
        0                       &amp; \text{if } k = 0, \\
        n \oplus (f(k - 1) - 1) &amp; \text{if } k \gt 1
      \end{cases}
  \]

  where \( k \) is a nonnegative integer, \( n \) is a positive
  integer and the symbol \( \oplus \) denotes bitwise XOR operation on
  two nonnegative integers.
</p>
<p>
  Note that \( f(0) \) represents the value of <code>i</code> set by
  the execution of <code>i ^= n</code> when no dots have been printed
  yet.
</p>
<p>
  If we can show that \( n \) is the least value of \( k \) for which
  \( f(k) = 0, \) it would prove that the loop terminates after
  printing \( n \) dots.
</p>
<p>
  We will see in the next section that for odd values of \( n, \)

  \[
    f(k) =
      \begin{cases}
        n &amp; \text{if } k \text{ is even}, \\
        1 &amp; \text{if } k \text{ is odd}.
      \end{cases}
  \]

  Therefore there is no value of \( k \) for which \( f(k) = 0 \) when
  \( n \) is odd.  As a result, the loop never terminates when \( n \)
  is odd.
</p>
<p>
  We will then see that for even values of \( n \) and \( 0 \leq k
  \leq n, \)

  \[
    f(k) = 0 \iff k = n.
  \]

  Therefore the loop terminates after printing \( n \) dots when
  \( n \) is even.
</p>
<h2 id="lemmas">Lemmas<a href="#lemmas"></a></h2>
<p>
  We will first prove a few lemmas about some interesting properties
  of the bitwise XOR operation.  We will then use it to prove the
  claims made in the previous section.
</p>
<!-- Lemma 1 -->
<p>
<strong>Lemma 1.</strong>
<em>
  For an odd positive integer \( n, \)

  \[
    n \oplus (n - 1) = 1
  \]

  where the symbol \( \oplus \) denotes bitwise XOR operation on two
  nonnegative integers.
</em>
</p>
<p>
  <em>Proof.</em>  Let the binary representation of \( n \) be \( b_m
  \dots b_1 b_0 \) where \( m \) is a nonnegative integer and
  \( b_m \) represents the most significant nonzero bit of \( n.  \)
  Since \( n \) is an odd number, \( b_0 = 1.  \)

  Thus \( n \) may be written as

  \[
    b_m \dots b_1 1.
  \]

  As a result \( n - 1 \) may be written as

  \[
    b_m \dots b_1 0.
  \]

  The bitwise XOR of both binary representations is \( 1.  \)
</p>
<!-- Lemma 2 -->
<p>
  <strong>Lemma 2.</strong>
  <em>
    For a nonnegative integer \( n, \)

    \[
      n \oplus 1 =
      \begin{cases}
      n + 1 &amp; \text{if } n \text{ is even}, \\
      n - 1 &amp; \text{if } n \text{ is odd}.
      \end{cases}
    \]

    where the symbol \( \oplus \) denotes bitwise XOR operation on two
    nonnegative integers.
  </em>
</p>
<p>
  <em>Proof.</em>  Let the binary representation of \( n \) be \( b_m
  \dots b_1 b_0 \) where \( m \) is a nonnegative integer and
  \( b_m \) represents the most significant nonzero bit of \( n.  \)
</p>
<p>
  If \( n \) is even, \( b_0 = 0.  \)  In this case, \( n \) may be
  written as \( b_m \dots b_1 0.  \)  Thus \( n \oplus 1 \) may be
  written as \( b_m \dots b_1 1.  \)  Therefore \( n \oplus 1 = n + 1.  \)
</p>
<p>
  If \( n \) is odd, \( b_0 = 1.  \)  In this case, \( n \) may be
  written as \( b_m \dots b_1 1.  \)  Thus \( n \oplus 1 \) may be
  written as \( b_m \dots b_1 0.  \)  Therefore \( n \oplus 1 = n - 1.  \)
</p>
<p>
  Note that for odd \( n, \) lemma 1 can also be derived as a
  corollary of lemma 2 in this manner:

  \[
    k \oplus (k - 1)
    = k \oplus (k \oplus 1)
    = (k \oplus k) \oplus 1
    = 0 \oplus 1
    = 1.
  \]
</p>
<!-- Lemma 3 -->
<p>
  <strong>Lemma 3.</strong>
  <em>
    If \( x \) is an even nonnegative integer and \( y \) is an odd
    positive integer, then \( x \oplus y \) is odd, where the symbol
    \( \oplus \) denotes bitwise XOR operation on two nonnegative
    integers.
  </em>
</p>
<p>
  <em>Proof.</em>  Let the binary representation of \( x \) be \(
  b_{xm_x} \dots b_{x1} b_{x0} \) and that of \( y \) be \( b_{ym_y}
  \dots b_{y1} b_{y0} \) where \( m_x \) and \( m_y \) are nonnegative
  integers and \( b_{xm_x} \) and \( b_{xm_y} \) represent the most
  significant nonzero bits of \( x \) and \( y \) respectively.
</p>
<p>
  Since \( x \) is even, \( b_{x0} = 0.  \)  Since \( y \) is odd, \(
  b_{y0} = 1.  \)
</p>
<p>
  Let \( z = x \oplus y \) with a binary representation of \( b_{zm_z}
  \dots b_{z1} b_{z0} \) where \( m_{zm_z} \) is a nonnegative integer
  and \( b_{zm_z} \) is the most significant nonzero bit of \( z.  \)
</p>
<p>
  We get \( b_{z0} = b_{x0} \oplus b_{y0} = 0 \oplus 1 = 1.  \)
  Therefore \( z \) is odd.
</p>
<h2 id="theorems">Theorems<a href="#theorems"></a></h2>
<!-- Theorem 1 -->
<p>
<strong>Theorem 1.</strong>
<em>
  Let \( \oplus \) denote bitwise XOR operation on two nonnegative
  integers and

  \[
    f(k) =
    \begin{cases}
    n                        &amp; \text{if } n = 0, \\
    n \oplus (f(n - 1) - 1)  &amp; \text{if } n \gt 1.
    \end{cases}
  \]

  where \( k \) is a nonnegative integer and \( n \) is an odd
  positive integer.  Then

  \[
    f(k) =
    \begin{cases}
    n &amp; \text{if } k \text{ is even}, \\
    1 &amp; \text{if } k \text{ is odd}.
    \end{cases}
  \]
</em>
</p>
<p>
  <em>Proof.</em>  This is a proof by mathematical induction.  We have
  \( f(0) = n \) by definition.  Therefore the base case holds good.
</p>
<p>
  Let us assume that \( f(k) = n \) for any even \( k \) (induction
  hypothesis).  Let \( k' = k + 1 \) and \( k'' = k + 2.  \)
</p>
<p>
  If \( k \) is even, we get

  \begin{align*}
    f(k')  &amp; = n \oplus (f(k) - 1)  &amp;&amp; \text{(by definition)} \\
           &amp; = n \oplus (n - 1)     &amp;&amp; \text{(by induction hypothesis)} \\
           &amp; = 1                    &amp;&amp; \text{(by lemma 1)},\\
    f(k'') &amp; = n \oplus (f(k') - 1) &amp;&amp; \text{(by definition)} \\
           &amp; = n \oplus (1 - 1)     &amp;&amp; \text{(since \( f(k') = 1 \))} \\
           &amp; = n \oplus 0 \\
           &amp; = n.
  \end{align*}
</p>
<p>
  Since \( f(k'') = n \) and \( k'' \) is the next even number after
  \( k, \) the induction step is complete.  The induction step shows
  that for every even \( k, \) \( f(k) = n \) holds good.  It also
  shows that as a result of \( f(k) = n \) for every even \( k, \) we
  get \( f(k') = 1 \) for every odd \( k'.  \)
</p>
<!-- Theorem 2 -->
<p>
  <strong>Theorem 2.</strong>
  <em>
    Let \( \oplus \) denote bitwise XOR operation on two nonnegative
    integers and

    \[
      f(k) =
        \begin{cases}
          n                        &amp; \text{if } n = 0, \\
          n \oplus (f(n - 1) - 1)  &amp; \text{if } n \gt 1.
        \end{cases}
    \]

    where \( k \) is a nonnegative integer, \( n \) is an even
    positive integer and \( 0 \leq k \leq n.  \)  Then

   \[
     f(k) = 0 \iff k = n.
   \]
</em>
</p>
<p>
  <em>Proof.</em>  We will first show by the principle of mathematical
  induction that for even \( k, \) \( f(k) = n - k.  \)  We have \(
  f(0) = n \) by definition, so the base case holds good.  Now let us
  assume that \( f(k) = n - k \) holds good for any even \( k \) where
  \( 0 \leq k \leq n \) (induction hypothesis).
</p>
<p>
  Since \( n \) is even (by definition) and \( k \) is even (by
  induction hypothesis), \( f(k) = n - k \) is even.  As a result, \(
  f(k) - 1 \) is odd.  By lemma 3, we conclude that \( f(k + 1) = n
  \oplus (f(k) - 1) \) is odd.
</p>
<p>
  Now we perform the induction step as follows:

  \begin{align*}
    f(k + 2) &amp; = n \oplus (f(k + 1) - 1)
                     &amp;&amp; \text{(by definition)} \\
             &amp; = n \oplus (f(k + 1) \oplus 1)
                     &amp;&amp; \text{(by lemma 2 for odd \( n \))} \\
             &amp; = n \oplus ((n \oplus (f(k) - 1)) \oplus 1)
                     &amp;&amp; \text{(by definition)} \\
             &amp; = (n \oplus n ) \oplus ((f(k) - 1) \oplus 1)
                     &amp;&amp; \text{(by associativity of XOR)} \\
             &amp; = 0 \oplus ((f(k) - 1) \oplus 1) \\
             &amp; = (f(k) - 1) \oplus 1 \\
             &amp; = (f(k) - 1) - 1
                     &amp;&amp; \text{(from lemma 2 for odd \( n \))} \\
             &amp; = f(k) - 2 \\
             &amp; = n - k - 2
                     &amp;&amp; \text{(by induction hypothesis).}
  \end{align*}

  This completes the induction step and proves that \( f(k) = n - k \)
  for even \( k \) where \( 0 \leq k \leq n.  \)
</p>
<p>
  We have shown above that \( f(k) \) is even for every even \( k \)
  where \( 0 \leq k \leq n \) which results in \( f(k + 1) \) as odd
  for every odd \( k + 1.  \)  This means that \( f(k) \) cannot be \(
  0 \) for any odd \( k.  \)  Therefore \( f(k) = 0 \) is possible only
  even \( k.  \)  Solving \( f(k) = n - k = 0, \) we conclude that \(
  f(k) = 0 \) if and only if \( k = n.  \)
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/loopy-c-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/c.html">#c</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>From Tower of Hanoi to Counting Bits</title>
<link>https://susam.net/from-tower-of-hanoi-to-counting-bits.html</link>
<guid isPermaLink="false">xfjry</guid>
<pubDate>Sun, 25 Sep 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[

<h2 id="tower-of-hanoi">Tower of Hanoi<a href="#tower-of-hanoi"></a></h2>
<p>
  A few weeks ago, I watched
  <a href="http://www.imdb.com/title/tt1318514/">Rise of the Planet of
  the Apes</a>.  The movie showed a genetically engineered chimpanzee
  trying to solve a puzzle involving four discs, initially stacked in
  ascending order of size on one of three pegs.  The chimpanzee was
  supposed to transfer the entire stack to one of the other pegs,
  moving only one disc at a time and never placing a larger disc on a
  smaller one.
</p>
<p>
  The problem was called the <em>Lucas' Tower</em> in the movie.  I
  have always known this problem as the <em>Tower of Hanoi</em>
  puzzle.  The minimum number of moves required to solve the problem
  is \( 2^n - 1 \) where \( n \) is the number of discs.  In the
  movie, the chimpanzee solved the problem in 15 moves, the minimum
  number of moves required when there are 4 discs.
</p>
<p>
  Referring to the problem as the Lucas' Tower made me wonder why it
  was called so instead of calling it the Tower of Hanoi.  I guessed
  it was probably because the puzzle was invented by the French
  mathematician &Eacute;douard Lucas.  Later when I checked
  the <a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi">Wikipedia
  article on this topic</a>, I realised I was right about this.  In
  fact, the article mentioned that there is another version of this
  problem known as <em>the Tower of Brahma</em> that involves 64 discs
  made of pure gold and three diamond needles.  According to a legend,
  a group of Brahmin priests are working at the problem and the world
  will end when the last move of the puzzle is completed.  Now, even
  if they make one move every second, it'll take
  18&#8239;446&#8239;744&#8239;073&#8239;709&#8239;551&#8239;615
  seconds to complete the puzzle.  That's about 585 billion years.
  The article also had this nice animation of a solution involving
  four discs.
</p>
<figure>
  <img src="files/blog/tower-of-hanoi-animation.gif"
       alt="Animated solution of the Tower of Hanoi puzzle">
  <figcaption>
    Animated solution of the Tower of Hanoi puzzle created by
    Andr&eacute; Karwath
    (<a
    href="https://commons.wikimedia.org/wiki/File:Tower_of_Hanoi_4.gif">original
    source</a>)
  </figcaption>
</figure>
<p>
  I'll not discuss the solution of this puzzle in this blog post.
  There are plenty of articles on the web including the Wikpedia
  article that describes why it takes a minimum of \( 2^n - 1 \) moves
  to solve the puzzle when there are \( n \) discs involved.  In this
  post, I'll talk about an interesting result I discovered while
  playing with this puzzle one afternoon.
</p>
<h2 id="binary-numbers">Binary Numbers<a href="#binary-numbers"></a></h2>
<p>
  If we denote the minimum number of moves required to solve the Tower
  of Hanoi puzzle as \( T_n, \) then \( T_n \) when expressed in
  binary is the largest possible \( n \)-bit integer.  For example, \(
  T_4 = 15_{10} = 1111_{2}.  \)  That makes sense because \( T_n = 2^n
  - 1 \) indeed represents the maximum possible \( n \)-bit integer
  where all \( n \) bits are set to \( 1.  \)
</p>
<p>
  While playing with different values of \( T_n \) for different
  values of \( n, \) I stumbled upon an interesting result which I
  will pose as a problem in a later section below.
</p>
<h2 id="assumptions">Assumptions<a href="#assumptions"></a></h2>
<p>
  Before proceeding to the problem, let us define the bit-length of an
  integer to eliminate any possibility of ambiguity:
</p>
<ul>
  <li>
    A positive integer \( x \) is said to be an \( n \)-bit integer if
    and only if the minimum number of bits required to express the
    integer is \( n; \) or equivalently, \( \lfloor \log_2 x \rfloor +
    1 = n.  \)
  </li>
</ul>
<p>
  We will be dealing with arbitrary precision integers (bignums) in
  the problem, so let us also make a few assumptions:
</p>
<ul>
  <li>
    Addition or subtraction of an \( m \)-bit integer and an \( n \)-bit
    integer (\( m \le n \)) takes \( O(n) \) time.
  </li>
  <li>
    Counting the number of \( 1 \)-bits in an \( n \)-bit integer takes
    \( O(n) \) time.
  </li>
</ul>
<p>
  The definition along with the assumptions lead to the following
  conclusions:
</p>
<ul>
  <li>
    Adding or subtracting two integers \( a \) and \( b \) takes \(
    O(\log(\max(a, b))) \) time.
  </li>
  <li>
    Counting the number of \( 1 \)-bits in an integer \( a \) takes
    \( O(\log(a)) \) time.
  </li>
</ul>
<h2 id="binary-puzzle">Binary Puzzle<a href="#binary-puzzle"></a></h2>
<div class="highlight">
  What is the most efficient way to compute the number of \( 1 \)-bits
  in

  \[
    T_1 + T_2 + \dots + T_n
  \]

  where \( n \) is a positive integer, each \( T_i = 2^i - 1 \) for
  integers \( 1 \le i \le n \) and efficiency is measured in terms of
  time and space complexity?
</div>
<p>
  The naive approach involves adding all the \( n \) integers and
  counting the number of \( 1 \)-bits in the sum.  It takes
  \( O(n^2) \) time to add the \( n \) integers.  The sum is an \( (n
  + 1) \)-bit integer, so it takes \( O(n) \) time to count the number
  of \( 1 \)-bits in the sum.  Since the sum is \( (n + 1) \)-bit
  long, it takes \( O(n) \) memory to store the sum.  If \( n \) is as
  large as, say, \( 2^{64}, \) it takes 2 exbibytes plus one more bit
  of memory to store the sum.
</p>
<p>
  We can arrive at a much more efficient solution if we look at what
  the binary representation of the sum looks like.  We first arrive at
  a closed-form expression for the sum:

  \begin{align*}
    T_1 + T_2 + \dots + T_n
    &amp; = (2 - 1) + (2^2 - 1) + \dots + (2^n - 1) \\
    &amp; = (2 + 2^2 + \dots + 2^n) - n \\
    &amp; = (2^{n + 1} - 2) - n \\
    &amp; = (2^{n + 1} - 1) - (n + 1).
  \end{align*}

  Now \( 2^{n + 1} - 1 \) is an \( (n + 1) \)-bit number with all its
  bits set to \( 1.  \)  Subtracting \( n + 1 \) from it is equivalent
  to performing the following operation with their binary
  representations: for each \( 1 \)-bit in \( (n + 1), \) set the
  corresponding bit in \( (2^{n + 1} - 1) \) to \( 0.  \)
</p>
<p>
  If we use the notation \( \text{bitcount}(n) \) to represent the
  number of \( 1 \)-bits in the binary representation of a positive
  integer \( n, \) then we get

  \[
    \text{bitcount}(T_1 + T_2 + \dots + T_n)
    = (n + 1) - \text{bitcount}(n + 1).
  \]

  Now the computation involves counting the number of \( 1 \)-bits in
  \( n + 1 \) which takes \( O(\log n) \) and subtracting this count
  from \( n + 1 \) which also takes \( O(\log n) \) time.  Further,
  the largest number that we keep in memory is \( n + 1 \) which
  occupies \( O(\log n) \) space.  Therefore, the entire problem can
  be solved in \( O(\log n) \) time with \( O(\log n) \) space.
</p>
<p>
  What would have taken 2 exbibytes and 1 bit of memory with the naive
  approach requires 8 bytes and 1 bit of memory now.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/from-tower-of-hanoi-to-counting-bits.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Langford Pairing</title>
<link>https://susam.net/langford-pairing.html</link>
<guid isPermaLink="false">dobze</guid>
<pubDate>Sat, 17 Sep 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="permutation-problem">Permutation Problem<a href="#permutation-problem"></a></h2>
<p>
  A few days ago, I came across this problem:
</p>
<div class="highlight">
  <p>
    There is a sequence of \( 2n \) numbers where each natural number
    from \( 1 \) to \( n \) is repeated twice, i.e.

    \[
      (1, 1, 2, 2, \dots, n, n).
    \]

    Find a permutation of this sequence such that for each \( k \)
    where \( 1 \le k \le n, \) there are \( k \) numbers between two
    occurrences of \( k \) in the permutation.
  </p>
</div>
<h2 id="getting-started">Getting Started<a href="#getting-started"></a></h2>
<p>
  In combinatorics, this problem has a name: <em>Langford's
  problem</em>.  A permutation of \( (1, 1, 2, 2, \dots, n, n) \) that
  satisfies the condition given in the probem is known as
  a <em>Langford pairing</em> or <em>Langford sequence</em>.
</p>
<p>
  For small \( n, \) say \( n = 4, \) Langford pairings can be
  obtained easily by trial and error: \( (4, 1, 3, 1, 2, 4, 3, 2).  \)
  What if \( n \) is large?  We need an algorithm to find a
  permutation that solves the problem in that case.
</p>
<p>
  There is another question to consider: Is there a solution for every
  possible \( n?  \)  One can easily see that there are no Langford
  pairings for \( n = 1 \) and \( n = 2, \) i.e. the sequences \( (1,
  1) \) and \( (1, 1, 2, 2) \) have no Langford pairings.
</p>
<p>
  We need to understand two things:
</p>
<ol>
  <li>
    For what values of \( n \) do Langford pairings exist?
  </li>
  <li>
    For the values of \( n \) for which Langford pairings exist, how
    do we find the Langford pairings?
  </li>
</ol>
<p>
  A simple Python 3 program I wrote to find Langford pairings for
  small values of \( n \) offers some clues.  Here is the program:
</p>
<pre>
<code>def find_solutions(n, s=None):
    # If called from top-level (s=None), create a list of 2n zero
    # values.  Zeroes represent unoccupied cells.
    if s is None:
        s = [0] * (2 * n)

    # Next number to be placed.
    m = max(s) + 1

    # For each i, try to place m at s[i] and s[i + m + 1].
    for i in range(2 * n - m - 1):

        # If s[i] and s[i + m + 1] are unoccupied, ...
        if s[i] == s[i + m + 1] == 0:

            # first place m at s[i] and s[i + m + 1].
            s[i] = s[i + m + 1] = m

            # If m is the last number to be placed, ...
            if m == n:
                # then a solution has been found; yield it.
                yield s[:]
            else:
                # else try to place the next number.
                yield from find_solutions(n, s)

            # Undo placement of m.
            s[i] = s[i + m + 1] = 0


# Count solutions for 1 &lt;= n &lt;= 12.
for n in range(1, 13):
    count = sum(1 for s in find_solutions(n))
    print('n = {:2} =&gt; {:6} solutions'.format(n, count))</code>
</pre>
<p>
  It takes a few minutes for this program to run.  Here is the output
  of this program:
</p>
<pre>
<samp>$ <kbd>python3 langford.py</kbd>
n =  1 =&gt;      0 solutions
n =  2 =&gt;      0 solutions
n =  3 =&gt;      2 solutions
n =  4 =&gt;      2 solutions
n =  5 =&gt;      0 solutions
n =  6 =&gt;      0 solutions
n =  7 =&gt;     52 solutions
n =  8 =&gt;    300 solutions
n =  9 =&gt;      0 solutions
n = 10 =&gt;      0 solutions
n = 11 =&gt;  35584 solutions
n = 12 =&gt; 216288 solutions</samp>
</pre>
<p>
  Note that we always talk about Langford pairings in plural in this
  post.  That's because either a sequence has no Langford pairings or
  it has two or more Langford pairings.  There is never a sequence
  that has only one Langford pairing.  That's because if we find at
  least one Langford pairing for a sequence, the reverse of that
  Langford pairing is also a Langford pairing.  Therefore, when
  Langford pairings exist for a sequence, they must at least be two in
  number.  Since they occur in pairs, they are always even in number.
  This is why we don't have to write "one or more Langford pairings"
  in this post.  We can always write "Langford pairings" instead.
</p>
<h2 id="conjecture">Conjecture<a href="#conjecture"></a></h2>
<p>
  From the output above, we can form a conjecture:
</p>
<div class="highlight">
  The sequence \( (1, 1, 2, 2, \dots, n, n) \) has Langford pairings
  if and only if \( n \equiv 0 \pmod{4} \) or
  \( n \equiv 3 \pmod{4}.  \)
</div>
<p>
  For convenience, let us denote the sequence \( (1, 1, 2, 2, \dots,
  n, n) \) as \( S_n.  \)  We will now prove the above conjecture in two
  parts:
</p>
<ol>
  <li>
    We will first show that the condition that either \( n \equiv 0
    \pmod{4} \) or \( n \equiv 3 \pmod{4} \) must hold is a
    <em>necessary</em> condition for the sequence \( S_n \) to have
    Langford pairings.
  </li>
  <li>
    We will then show that the condition that either \( n \equiv 0
    \pmod{4} \) or \( n \equiv 3 \pmod{4} \) must hold is a
    <em>sufficient</em> condition for the sequence \( S_n \) to have
    Langford pairings.
  </li>
</ol>
<h2 id="necessity">Necessity<a href="#necessity"></a></h2>
<p>
  Let \( S_n = (1, 1, 2, 2, \dots, n, n) \) be a sequence such that it
  has Langford pairings.  Let us pick an arbitrary Langford pairing \(
  s \) of \( S_n \) and split this Langford pairing \( s \) into two
  mutually exclusive subsequences \( s_1 \) and \( s_2 \) such that:
</p>
<ul>
  <li>
    \( s_1 \) contains all numbers in odd numbered positions and
  </li>
  <li>
    \( s_2 \) contains all numbers in even numbered positions.
  </li>
</ul>
<p>
  For example, if we pick \( s = (1, 7, 1, 2, 5, 6, 2, 3, 4, 7, 5, 3,
  6, 4) \) which is a Langford pairing of \( S_7, \) we split \( s \)
  into

  \begin{align*}
    s_1 &amp; = (1, 1, 5, 2, 4, 5, 6), \\
    s_2 &amp; = (7, 2, 6, 3, 7, 3, 4).
  \end{align*}
</p>
<p>
  We can make a few observations:
</p>
<ol>
  <li>
    Both occurrences of an even number do not occur in the same
    subsequence.
  </li>
  <li>
    There are \( \left\lfloor \frac{n}{2} \right\rfloor \) even
    numbers in each subsequence.
  </li>
  <li>
    Both occurrences of an odd number occur in the same subsequence.
  </li>
  <li>
    There are \( \left\lceil \frac{n}{2} \right\rceil \) odd numbers
    in each subsequence.
  </li>
</ol>
<p>
  Do these observations hold good for every Langford pairing of any
  aribrary \( S_n \) for every positive integer value of \( n?  \)
  Yes, they do.  We will now prove them one by one:
</p>
<ol>
  <li>
    <p>
      Let us consider an even number \( k \) from a Langford pairing.
      If the first occurrence of \( k \) lies at the \( i \)th
      position in the pairing, then its second occurrence lies at the
      \( (i + k + 1) \)th position.  Since \( k \) is even, \( i \)
      and \( i + k + 1 \) have different parities, i.e. if \( i \) is
      odd, then \( i + k + 1 \) is even and vice versa.  Therefore, if
      the first occurrence of \( k \) lies at an odd numbered
      position, its second occurrence must lie at an even numbered
      position and vice versa.  Thus one occurrence of \( k \) must
      belong to \( s_1 \) and the other must belong to \( s_2.  \)
      This proves the first observation.
    </p>
  </li>
  <li>
    <p>
      The number of even numbers between \( 1 \) and \( n, \)
      inclusive, is \( \left\lfloor \frac{n}{2} \right\rfloor.  \)
      Each of these even numbers has been split equally between
      \( s_1 \) and \( s_2.  \)  This proves the second observation.
    </p>
  </li>
  <li>
    <p>
      Now let us consider an odd number \( k \) from a Langford
      pairing.  If the first occurrence of \( k \) lies at the
      \( i \)th position in the pairing, then its second occurrence lies at
      the \( (i + k + 1) \)th position.  Since \( k \) is odd, \( i \)
      and \( i + k + 1 \) have the same parity.  Therefore, either
      both occurrences of \( k \) belong to \( s_1 \) or both belong
      to \( s_2.  \)  This proves the third observation.
    </p>
  </li>
  <li>
    <p>
      Each subsequence, \( s_1 \) or \( s_2 \) has \( n \) numbers
      because we split a Langford pairing \( s \) with \( 2n \)
      numbers equally between the two subsequences.  We have shown
      that each subsequence has \( \left\lfloor \frac{n}{2}
      \right\rfloor \) even numbers.  Therefore the number of odd
      numbers in each subsequence is \( n - \left\lfloor \frac{n}{2}
      \right\rfloor = \left\lceil \frac{n}{2} \right\rceil.  \)
    </p>
  </li>
</ol>
<p>
  From the third observation, we know that the odd numbers always
  occur in pairs in each subsequence because both occurrences of an
  odd number occur together in the same subsequence.  Therefore, the
  number of odd numbers in each subsequence must be even.  Since the
  number of odd numbers in each subsequence is \( \left\lceil
  \frac{n}{2} \right\rceil \) as proven for the fourth observation, we
  conclude that \( \left\lceil \frac{n}{2} \right\rceil \) must be
  even.
</p>
<p>
  Now let us see what must \( n \) be like so that \( \left\lceil
  \frac{n}{2} \right\rceil \) is even.
</p>
<p>
  Let us express \( n \) as \( 4q + r \) where \( q \) is a
  nonnegative integer and \( r \in \{0, 1, 2, 3\}.  \)
</p>
<ul>
  <li>
    If \( n = 4q + 0, \) then\( \left\lceil \frac{n}{2} \right\rceil =
    \left\lceil \frac{4q}{2} \right\rceil = 2q.  \)
  </li>
  <li>
    If \( n = 4q + 1, \) then\( \left\lceil \frac{n}{2} \right\rceil =
    \left\lceil \frac{4q + 1}{2} \right\rceil = 2q + 1.  \)
  </li>
  <li>
    If \( n = 4q + 2, \) then\( \left\lceil \frac{n}{2} \right\rceil =
    \left\lceil \frac{4q + 2}{2} \right\rceil = 2q + 1.  \)
  </li>
  <li>
    If \( n = 4q + 3, \) then\( \left\lceil \frac{n}{2} \right\rceil =
    \left\lceil \frac{4q + 3}{2} \right\rceil = 2q + 2.  \)
  </li>
</ul>
<p>
  We see that \( \left\lceil \frac{n}{2} \right\rceil \) is even if
  and only if either \( n \equiv 0 \pmod{4} \) or \( n \equiv 3
  \pmod{4} \) holds good.
</p>
<p>
  We have shown that if a sequence \( S_n \) has Langford pairings,
  then either \( n \equiv 0 \pmod{4} \) or \( n \equiv 3 \pmod{4}.  \)
  This proves the necessity of the condition.
</p>
<h2 id="sufficiency">Sufficiency<a href="#sufficiency"></a></h2>
<p>
  If we can show that we can construct a Langford pairing for \( (1,
  1, 2, 2, \dots, n, n ) \) for both cases, i.e. \( n \equiv 0
  \pmod{4} \) as well as \( n \equiv 3 \pmod{4}, \) then it would
  complete the proof.
</p>
<h3 id="notation">Notation<a href="#notation"></a></h3>
<p>
  Let us define some notation to make it easier to write sequences we
  will use in the construction of a Langford pairing:
</p>
<ul>
  <li>
    <p>
      \( (i \dots j)_{even} \) denotes a sequence of even positive
      integers from \( i \) to \( j, \) exclusive, arranged in
      ascending order.
    </p>
    <p>
      For example, \( (1 \dots 8)_{even} = (2, 4, 6) \) and \( (1
      \dots 2)_{even} = ().  \)
    </p>
  </li>
  <li>
    <p>
      \( (i \dots j)_{odd} \) denotes a sequence of odd positive
      integers from \( i \) to \( j, \) exclusive, arranged in
      ascending order.
    </p>
    <p>
      For example, \( (1 \dots 8)_{odd} = (3, 5, 7) \) and \( (1 \dots
      3)_{odd} = ().  \)
    </p>
  </li>
  <li>
    <p>
      \( s' \) denotes the reverse of the sequence \( s.  \)
    </p>
    <p>
      For example, for a sequence \( s = (2, 3, 4, 5), \) we have \(
      s' = (2, 3, 4, 5)' = (5, 4, 3, 2).  \)
    </p>
  </li>
  <li>
    <p>
      \( s \cdot t \) denotes the concatenation of sequences \( s \)
      and \( t.  \)
    </p>
    <p>
      For example, for sequences \( s = (1, 2, 3) \) and
      \( t = (4, 5), \) we have \( s \cdot t = (1, 2, 3) \cdot (4, 5)
      = (1, 2, 3, 4, 5).  \)
    </p>
  </li>
</ul>
<p>
  Let \( x = \left\lceil \frac{n}{4} \right\rceil.  \)  Therefore,

  \[
    x =
    \begin{cases}
      \frac{n}{4}     &amp; \text{if } n \equiv 0 \pmod{4}, \\
      \frac{n + 1}{4} &amp; \text{if } n \equiv 3 \pmod{4}.
    \end{cases}
  \]

  Let us now define the following eight sequences:

  \begin{align*}
    a &amp; = (2x - 1), \\
    b &amp; = (4x - 2), \\
    c &amp; = (4x - 1), \\
    d &amp; = (4x), \\
    p &amp; = (0 \dots a)_{odd}, \\
    q &amp; = (0 \dots a)_{even}, \\
    r &amp; = (a \dots b)_{odd}, \\
    s &amp; = (a \dots b)_{even}.
  \end{align*}

  Now let us construct a Langford pairing for both cases: \( n \equiv
  0 \pmod{4} \) and \( n \equiv 3 \pmod{4}.  \)  We will do this case by
  case.
</p>
<h3 id="case-1">Case \( n \equiv 0 \pmod{4} \)<a href="#case-1"></a></h3>
<p>
  If \( n \equiv 0 \pmod{4}, \) we construct a Langford pairing with
  the following concatenation:

  \[
    s' \cdot
    p' \cdot
    b  \cdot
    p  \cdot
    c  \cdot
    s  \cdot
    d  \cdot
    r' \cdot
    q' \cdot
    b  \cdot
    a  \cdot
    q  \cdot
    c  \cdot
    r  \cdot
    a  \cdot
    d.
  \]

  Let us do an example with \( n = 12.  \)
</p>
<p>
  For \( n = 12, \) we get \( x = \frac{n}{4} = 3.  \)  Therefore,

  \begin{alignat*}{2}
    a &amp; = (2x - 1)           &amp;&amp; = (5), \\
    b &amp; = (4x - 2)           &amp;&amp; = (10), \\
    c &amp; = (4x - 1)           &amp;&amp; = (11), \\
    d &amp; = (4x)               &amp;&amp; = (12), \\
    p &amp; = (0 \dots a)_{odd}  &amp;&amp; = (1, 3), \\
    q &amp; = (0 \dots a)_{even} &amp;&amp; = (2, 4), \\
    r &amp; = (a \dots b)_{odd}  &amp;&amp; = (7, 9), \\
    s &amp; = (a \dots b)_{even} &amp;&amp; = (6, 8).
  \end{alignat*}

  After performing the specified concatenation, we get the following
  Langford pairing:

  \[
    (
      8, 6, 3, 1, 10, 1, 3, 11, 6, 8, 12, 9,
      7, 4, 2, 10, 5, 2, 4, 11, 7, 9, 5, 12
    ).
  \]

  Let us now show that any construction of a sequence as per this
  specified concatenation always leads to a Langford pairing.
</p>
<p>
  Each sequence \( a, \) \( b, \) \( c \) and \( d \) has one number
  each.  Each sequence \( p, \) \( q, \) \( r \) and \( s \) has \( x
  - 1 \) numbers each.
</p>
<p>
  The two occurrences of \( a \) have \( q, \) \( c \) and \( r \) in
  between, i.e.

  \[
    (x - 1) + 1 + (x - 1) = 2x - 1 = a
  \]

  numbers in between.  Similarly, we can check that the two
  occurrences of \( b \) have \( b \) numbers in between; likewise for
  \( c \) and \( d.  \)
</p>
<p>
  The two occurrences of \( 1 \) belong to \( p \) and \( p'.  \)
  Between these two occurrences of \( 1, \) we have only one element
  of \( b.  \)
</p>
<p>
  We now show that for each \( k \) in \( p, \) there are \( k \)
  numbers in between.  For any \( k \) in \( p, \) there is the
  sequence \( (0..k)'_{odd} \cdot b \cdot (0..k)_{odd} \) in between
  the two occurrences of \( k, \) i.e, there are \( \frac{k - 1}{2} +
  1 + \frac{k - 1}{2} = k \) numbers in between.  Similarly, we can
  check that for each \( k \) in \( q, \) there are \( k \) numbers in
  between.
</p>
<p>
  Finally, we show that for each \( k \) in \( r, \) there are \( k \)
  numbers in between.  For any \( k \) in \( r, \) there is the
  sequence \( (a..k)'_{odd} \cdot q' \cdot b \cdot a \cdot q \cdot c
  \cdot (a..k)_{odd} \) in between the two occurrences of \( k.  \)
  Note that \( a \) is odd, so the number of integers in this sequence
  is

  \[
    \frac{k - a - 2}{2} + (x - 1) + 1 + 1 + (x - 1) + 1 + \frac{k - a - 2}{2}.
  \]

  Simplifying the above expression and then substituting
  \( a = 2x - 1, \) we get

  \[
    k - a - 2 + 2x + 1 = k.
  \]

  Similarly, we can check that for each \( k \) in \( s, \) there are
  \( k \) numbers in between.
</p>
<h3 id="case-2">Case \( n \equiv 3 \pmod{4} \)<a href="#case-2"></a></h3>
<p>
  If \( n \equiv 3 \pmod{4}, \) we construct a Langford pairing with
  the following concatenation:

  \[
    s' \cdot
    p' \cdot
    b  \cdot
    p  \cdot
    c  \cdot
    s  \cdot
    a  \cdot
    r' \cdot
    q' \cdot
    b  \cdot
    a  \cdot
    q  \cdot
    c  \cdot
    r.
  \]

  Note that this concatenation of sequences is almost the same as the
  concatenation in the previous section with the following two
  differences:
</p>
<ul>
  <li>
    The sequence \( d \) is not used here.
  </li>
  <li>
    The sequence \( a \) in the end has moved to replace the sequence
    \( d \) in the middle of the concatenation.
  </li>
</ul>
<p>
  Let us do an example with \( n = 11.  \)  For \( n = 12, \) we get \(
  x = \frac{n + 1}{4} = 3.  \)  Therefore, the sequences \( a, \) \(
  b, \) \( c, \) \( p, \) \( q, \) \( r \) and \( s \) are same as
  those in the last example in the previous section.  After performing
  the specified concatenation, we get the following Langford pairing:

  \[
    (
      8, 6, 3, 1, 10, 1, 3, 11, 6, 8, 5,
      9, 7, 4, 2, 10, 5, 2, 4, 11, 7, 9
    ).
  \]

  We can verify that for every \( k \) in a Langford pairing
  constructed in this manner, there are \( k \) numbers in between.
  The verification steps are similar to what we did in the previous
  section.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/langford-pairing.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/combinatorics.html">#combinatorics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Magical Chameleons Puzzle</title>
<link>https://susam.net/magical-chameleons-puzzle.html</link>
<guid isPermaLink="false">fuoog</guid>
<pubDate>Sun, 19 Jun 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  An island contained chameleons of three different colours: red,
  green and blue.  The chameleons were studied by some biologists and
  they found that when two chameleons of different colours met they
  changed their colours to the third one.  They found that there were
  2000 red chameleons and 3000 green ones on the day they counted
  them.  They didn't get time to count the number of blue chameleons.
</p>
<p>
  When the biologists returned to the island two months later they
  found that all chameleons were red in colour.  They were certain
  that no chameleons died because they did not find dead remains of
  any chameleon.  What does it say about the number of blue chameleons
  on the day the biologists counted the number of red and green
  chameleons?
</p>
<p>
  See the comments page for the solution.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/magical-chameleons-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Calendar Cubes Puzzle</title>
<link>https://susam.net/calendar-cubes-puzzle.html</link>
<guid isPermaLink="false">yoksw</guid>
<pubDate>Sun, 12 Jun 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  How many different ways are there to assign the ten digits of Arabic
  numerals (0 to 9) to each face of two cubes to ensure that we can
  arrange both cubes on any day such that the front faces of the cubes
  show the current day of the month?
</p>
<p>
  For example, on February 9 the cubes would be placed side by side
  such that the front face of the cube on the left side shows 0 and
  that of the one on the right side shows 9.
</p>
<p>
  Two ways of assigning the digits to the faces of the cubes are
  considered different if and only if it is not possible to get one
  assignment from the other by performing one or more of the following
  operrations:
</p>
<ol>
  <li>
    Rotating (reorienting) the digits with respect to the faces they
    belong to.
  </li>
  <li>
    Rotating the cubes.
  </li>
  <li>
    Swapping the cubes.
  </li>
</ol>
<p>
  See the comments page for the solution.
</p>

<!-- ### -->
<p>
  <a href="https://susam.net/calendar-cubes-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Mathematics Storm</title>
<link>https://susam.net/mathematics-storm.html</link>
<guid isPermaLink="false">fkkxg</guid>
<pubDate>Sun, 15 May 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  From Freenode IRC <code>#math</code> channel this morning:
</p>
<pre>
<samp>&lt;oops&gt; I have a confusion.  the calculation that was giving
       me 2 bits earlier is not giving me 2 bits now.  :(  please help.
&lt;oops&gt; 4 equally probable symbols: so 4 * (1/4) * log(1 / 1/4), right?
&lt;antonfire&gt; yeah math changes sometimes.
&lt;antonfire&gt; probably a math storm
&lt;antonfire&gt; wait a few minutes and try again
&lt;_Ray_&gt; try logging out and back in
&lt;oops&gt; so, so 1 * log(4) = 2
&lt;oops&gt; oh it is giving me 2 bits again
&lt;mariano|syzygy&gt; hmmm, actually he was not his advisor
&lt;oops&gt; thanks, nvm.
&lt;sig^&gt; try switching math off and on again
&lt;thermoplyae&gt; haha
&lt;_Ray_&gt; yeah, it was the router</samp>
</pre>
<!-- ### -->
<p>
  <a href="https://susam.net/mathematics-storm.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/irc.html">#irc</a> |
  <a href="https://susam.net/tag/humour.html">#humour</a>
</p>
]]>
</description>
</item>
<item>
<title>Missing Digit Puzzle</title>
<link>https://susam.net/missing-digit-puzzle.html</link>
<guid isPermaLink="false">fthed</guid>
<pubDate>Sat, 14 May 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  There is a 10-digit multiple of 234.  9 of its digits in ascending
  order are: 0, 1, 1, 2, 3, 4, 5, 7 and 9.  What is the missing digit?
</p>
<p>
  See the comments page for the solution.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/missing-digit-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>From Out Shuffles to Multiplicative Order</title>
<link>https://susam.net/from-out-shuffles-to-multiplicative-order.html</link>
<guid isPermaLink="false">vnpml</guid>
<pubDate>Wed, 23 Mar 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<div style="display: none">
  \( \gdef\ord{\operatorname{ord}} \)
</div>
<h2 id="out-shuffles">Out Shuffles<a href="#out-shuffles"></a></h2>
<p>
  A <em>perfect riffle shuffle</em> of a deck of cards involves
  splitting the deck into two halves (one in each hand) and then
  alternately dropping one card from each half till all cards have
  fallen.  If the shuffling is done in such a manner that the bottom
  and the top cards remain preserved at their positions, then it is
  an <em>out shuffle</em>.
</p>
<h2 id="problem">Problem<a href="#problem"></a></h2>
<p>
  Here is a problem that a colleague asked me recently while
  discussing shuffling techniques:
</p>
<div class="highlight">
  How many out shuffles do we need to perform on a deck of 52 cards to
  return the deck to its initial state?
</div>
<h2 id="solution">Solution<a href="#solution"></a></h2>
<p>
  The solution to this problem is rather long, so it has been split
  into three parts below.
</p>
<h3 id="part-1">Part 1: Congruence Relation<a href="#part-1"></a></h3>
<p>
  Let us assume that there are \( n \) cards where \( n \) is a
  positive even integer.  Let us denote these cards as \( c_0, c_1,
  \dots, c_{n-1} \) where \( c_0 \) is the card at index \( 0 \) (top
  of the deck), \( c_{n - 1} \) is the card at index \( n - 1 \)
  (bottom of the deck) and \( c_i \) is the card at index \( i \)
  where \( 0 \le i \le n - 1.  \)
</p>
<p>
  For example, if we have \( 8 \) cards, then the cards are denoted as

  \[
    c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7.
  \]

  After the first out shuffle, these cards are now in this order:

  \[
    c_0, c_4, c_1, c_5, c_2, c_6, c_3, c_7.
  \]
</p>
<p>
  From the problem description and the example above, we see that
  after a single out shuffle, a card at index \( i \) moves to index
  \( 2i \bmod (n - 1) \) for \( 0 \le i \lt n - 1 \) and the card at
  index \( n - 1 \) remains at the same place.
</p>
<p>
  After \( m \) shuffles a card at index \( i \) moves to index \(
  (2^m i) \bmod (n - 1) \) for \( 0 \le i \lt n - 1.  \)  The card at
  index \( n - 1 \) always remains at the same place.
</p>
<p>
  The solution to the problem then is the smallest positive integer \(
  m \) such that

  \[
    2^m i \equiv i \pmod{n - 1}.
  \]

  for all integers \( i \) that satisfy the inequality \( 0 \le i \lt
  n - 1.  \)
</p>
<p>
  In modular arithmetic, we know that

  \[
    ac \equiv bc \pmod{n}
    \iff
    a \equiv b \pmod{n / \gcd(c, n)}.
  \]

  where \( a, \) \( b, \) \( c \) and \( n \) are integers.  Therefore

  \[
    2^m i \equiv i \pmod{n - 1}
    \iff
    2^m \equiv 1 \pmod{(n - 1) / \gcd(i, n - 1)}.
  \]

  Let \( F_{ni} = \frac{n - 1}{\gcd(i, n - 1)} \) where \( 0 \le i \lt
  n - 1.  \)  Now the above congruence relation can be written as

  \[
    2^m \equiv 1 \pmod{F_{ni}}.
  \]
</p>
<h3 id="part-2">Part 2: Multiplicative Order<a href="#part-2"></a></h3>
<p>
  The smallest positive integer \( m \) that satisfies the above
  congruence relation is known as the multiplicative order of \( 2 \)
  modulo \( F_{ni}.  \)  It is denoted as \( \ord_{F_{ni}}(2).  \)
</p>
<p>
  In general, given an integer \( a \) and a positive integer \( n \)
  with \( \gcd(a, n) = 1, \) the multiplicative order of \( a \)
  modulo \( n, \) denoted as \( \ord_{n}(a), \) is the smallest
  positive integer \( k \) such that

  \[
    a^k \equiv 1 \pmod{n}.
  \]

  In this problem, \( n \) is even, so \( n - 1 \) is odd as a result
  of which \( F_{ni} \) is also odd.  As a result, \( \gcd(2, F_{ni}) =
  1.  \)  Therefore, the smallest positive integer \( m \) that
  satisfies the congruence relation \( 2^m \equiv 1 \pmod{F_{ni}} \)
  is denoted as \( \ord_{F_{ni}}(2).  \)
</p>
<p>
  If \( n = 2, \) \( F_{n0} = F_{n1} = 1, \) therefore \( 2^m \equiv 1
  \pmod{F_{ni}} \) for \( 0 \le i \lt n - 1 \) and all positive
  integers \( m.  \)  This proves the trivial observation that when
  there are only two cards \( c_0 \) and \( c_1, \) they remain at
  index \( 0 \) and index \( 1 \) respectively, after any number of
  out shuffles, i.e. their positions do not change with out shuffles.
</p>
<h4 id="case-1">Case \( n \ge 4 \)<a href="#case-1"></a></h4>
<p>
  If \( n \ge 4, \) we know that there exists at least one integer
  between \( 1 \) and \( n - 1 \) that is coprime to \( n - 1 \)
  because \( \varphi(n) \ge 2 \) for \( n \ge 4 \) where
  \( \varphi(n) \) represents Euler's totient of \( n.  \)
</p>
<h5 id="subcase-1-1">Subcase \( i \) is coprime to \( n - 1 \)<a href="#subcase-1-1"></a></h5>
<p>
  For any arbitrary \( n \ge 4, \) let \( i \) be an integer that is
  coprime to \( n - 1 \) such that \( 1 \lt i \lt n - 1.  \)  Now \(
  \gcd(i, n - 1 ) = 1, \) so \( F_{ni} = \frac{n - 1}{\gcd(i, n - 1)}
  = n - 1.  \)  As a result, \( \ord_{F_{ni}}(2) = \ord_{n -
  1}({2}).  \)  This shows that any card at index \( i \) such that \(
  i \) is coprime to \( n - 1 \) requires a minimum of \( \ord_{n -
  1}(2) \) out shuffles to return to its initial place.
</p>
<h5 id="subcase-1-2">Subcase \( i \) is <em>not</em> coprime to \( n - 1 \)<a href="#subcase-1-2"></a></h5>
<p>
  For any arbitrary \( n \ge 4, \) now let us consider the case of a
  card at index \( i \) such that \( i \) is not coprime to
  \( n - 1.  \)  Since \( n - 1 \) is odd, we have \( \gcd(2, n - 1) =
  1.  \)  Therefore, by definition of multiplicative order,

  \[
    2^{\ord_{n - 1}(2)} \equiv 1 \pmod{n - 1}.
  \]

  Since \( F_{ni} \mid n - 1, \) we get,

  \[
    2^{\ord_{n - 1}(2)} \equiv 1 \pmod{F_{ni}}.
  \]

  This shows that a card at index \( i \) such that \( i \) is not
  coprime to \( n - 1 \) also return to its initial place after \(
  \ord_{n - 1}(2) \) out shuffles.  Actually, it takes only \(
  \ord_{F_{ni}}(2) \) out shuffles to return the card to its initial
  place.  But \( \ord_{F_{ni}}(2) \mid \ord_{n - 1}(2), \) so \(
  \ord_{n - 1}(2) = c \ord_{F_{ni}}(2) \) for some positive integer \(
  c.  \)  Therefore performing \( \ord_{n - 1}(2) \) out shuffles is
  same as repeating \( \ord_{F_{ni}}(2) \) out shuffles \( c \) times.
  Every \( \ord_{F_{ni}}(2) \) brings back the card to its initial
  position, so repeating it \( c \) times also brings it back to its
  initial position.
</p>
<p>
  We have shown that a card at index \( i \) such that \( i \) is
  coprime to \( n - 1 \) takes a minimum of \( \ord_{n - 1}(2) \) out
  shuffles to return to its initial place.  We have also shown that a
  card at other indices also return to its initial place after the
  same number of out shuffles.  Therefore, it takes a minimum of \(
  \ord_{n - 1}{2} \) out shuffles to return the deck of cards to its
  initial state.
</p>
<h4 id="case-2">Case \( n = 2 \)<a href="#case-2"></a></h4>
<p>
  When there are only \( 2 \) cards in the deck, every out shuffle
  trivially returns the deck to its initial state.  In other words, it
  takes only \( 1 \) out shuffle to return the deck to its initial
  state.  Indeed \( \ord_{n - 1}(2) = \ord_{1}(2) = 1.  \)
</p>
<p>
  We have now shown that for any positive even integer \( n, \) a deck
  of \( n \) cards returns to its initial state after \( \ord_{n -
  1}(2) \) out shuffles.
</p>
<h3 id="part-3">Part 3: Computing Multiplicative Order<a href="#part-3"></a></h3>
<p>
  For a deck of \( 52 \) cards, we have \( n = 52.  \)  A minimum of \(
  \ord_{51}(2) \) out shuffles are required to return the deck to its
  initial state.  To compute \( \ord_{51}(2) \) we first enumerate the
  powers of \( 2 \) modulo \( 51 \):

  \begin{alignat*}{2}
    2^0 &amp; \equiv 1  &amp;&amp; \pmod{51}, \\
    2^1 &amp; \equiv 2  &amp;&amp; \pmod{51}, \\
    2^2 &amp; \equiv 4  &amp;&amp; \pmod{51}, \\
    2^3 &amp; \equiv 8  &amp;&amp; \pmod{51}, \\
    2^4 &amp; \equiv 16 &amp;&amp; \pmod{51}, \\
    2^5 &amp; \equiv 32 &amp;&amp; \pmod{51}, \\
    2^6 &amp; \equiv 13 &amp;&amp; \pmod{51}, \\
    2^7 &amp; \equiv 26 &amp;&amp; \pmod{51}, \\
    2^8 &amp; \equiv 1  &amp;&amp; \pmod{51}.
  \end{alignat*}

  The smallest positive integer \( k \) such that \( 2^k \equiv 1
  \pmod{51} \) is 8, so \( \ord_51(2) = 8.  \)  We need \( 8 \) out
  shuffles to return a deck of \( 52 \) cards to its initial state.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/from-out-shuffles-to-multiplicative-order.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/combinatorics.html">#combinatorics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Polar Bear Puzzle</title>
<link>https://susam.net/polar-bear-puzzle.html</link>
<guid isPermaLink="false">hrcyc</guid>
<pubDate>Wed, 09 Mar 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Alice asked Bob, "A bear walked 1 km south, then 1 km west, then 1
  km north and it was back at the point from where it started.  What
  colour was the bear most likely to be?"
</p>
<p>
  Bob thought for a while, could not arrive at an answer and gave up.
  Alice explained, "Well, the answer is white.  It is a polar bear.
  It is only when you start from the North Pole that after travelling
  1 km south, 1 km west and 1 km north you would end up at the point
  where you started."
</p>
<p>
  Bob replied, "That is an interesting solution.  Now that I
  understand your solution, I realise that there are other starting
  points apart from the North Pole where one could walk 1 km south,
  then 1 km west and then 1 km north to return to the starting point."
</p>
<p>
  Can you find all the other such starting points that Bob is talking
  about?
</p>
<p>
  See the comments page for the solution.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/polar-bear-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Shrinking List Puzzle</title>
<link>https://susam.net/shrinking-list-puzzle.html</link>
<guid isPermaLink="false">ysrvs</guid>
<pubDate>Thu, 17 Feb 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  The first \( 9 \) natural numbers are given in a list.  You are
  supposed to select two numbers randomly from the list, call them \(
  x \) and \( y, \) remove them from the list and insert \( x + y + xy \)
  into the list.  You keep repeating this until you are left with only
  one number in the list.  Find the final number that is left in the
  list.
</p>
<p>
  See the comments page for the solution.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/shrinking-list-puzzle.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>From Diophantine Equation to Fermat's Last Theorem</title>
<link>https://susam.net/from-diophantine-equation-to-fermats-last-theorem.html</link>
<guid isPermaLink="false">sfcma</guid>
<pubDate>Wed, 12 Jan 2011 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Here is a puzzle I created recently for my friends who love to
  indulge in recreational mathematics:
</p>
<div class="highlight">
  Find all integer solutions to the equation

  \[
    y^2 + 3 = \frac{x^3}{18}.
  \]
</div>
<p>
  <em>If you want to think about this puzzle, this is a good time to
  pause and think about it.  There are spoilers ahead.</em>
</p>
<p>
  It does not take long to realise that this is a Diophantine equation
  of the form \( a^n + b^n = c^n.  \)  Here is how the equation looks
  after rearranging the terms:

  \[
    x^3 = 18y^2 + 54.
  \]
</p>
<p>
  The right hand side is positive, so any \( x \) that satisfies this
  equation must also be positive, i.e. \( x \gt 0 \) must hold good
  for any solution \( x \) and \( y.  \)
</p>
<p>
  Also, if some \( y \) satisfies the equation, then \( -y \) also
  satisfies the equation because the right hand side value remains the
  same for both \( y \) and \( -y.  \)
</p>
<p>
  The right hand side is \( 2(9y^2 + 3^3).  \)  This is of the form \(
  2(3a^2b + b^3) \) where \( a = y \) and \( b = 3.  \)  Now \( 2(3a^2b
  + b^3) = (a + b)^3 - (a - b)^3.  \)  Using these details, we get

  \[
    x^3 = 18y^2 + 54 = 2(9y^2 + 3^3) = (y + 3)^3 - (y - 3)^3.
  \]

  Rearranging the terms, we get

  \[
    x^3 + (y - 3)^3 = (y + 3)^3.
  \]

  From Fermat's Last Theorem, we know that an equation of the form \(
  a^n + b^n = c^n \) does not have any solution for positive integers
  \( a, \) \( b, \) \( c \) and positive integer \( n \gt 2.  \)  We
  know that \( x \gt 0.  \)  Therefore \( y \gt 3 \) contradicts
  Fermat's Last Theorem.  Thus the inequality \( y \le 3 \) must hold
  good.  Further since for every solution \( x \) and \( y, \) there
  is also a solution \( x \) and \( -y, \) the inequality \( -y \le
  3 \) must also hold good.  Therefore values of \( x \) and \( y \)
  that satisfy the above equation must satisfy the following
  inequalities:

  \begin{align*}
    x \gt 0, \\
    -3 \le y \le 3.
  \end{align*}

  Since \( y \) must be one of the seven integers between \( -3 \) and
  \( 3, \) inclusive, we can try solving for \( x \) with each of
  these seven values of \( y.  \)  When we do so, we find that there
  are only two values of \( y \) for which we get integer solutions
  for \( x.  \)  They are \( y = 3 \) and \( y = -3.  \)  In both cases,
  we get \( x = 6.  \)  Therefore, the solutions to the given equation
  are:

  \[
    x = 6, \qquad y = \pm 3.
  \]
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/from-diophantine-equation-to-fermats-last-theorem.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/combinatorics.html">#combinatorics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>Squaring Numbers That Begin or End With 5</title>
<link>https://susam.net/squaring-numbers-that-begin-or-end-with-5.html</link>
<guid isPermaLink="false">caort</guid>
<pubDate>Tue, 21 Dec 2010 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  In this post, I will discuss some simple tricks I use to square
  numbers that begin or end with the digit \( 5.  \)  We will first see
  a few illustrations of each trick.  Then we will generalise the
  tricks for any number that begins or ends with the digit \( 5.  \)
</p>
<h2 id="squaring-a-2-digit-number-that-ends-with-5">Squaring a 2-Digit Number That Ends With 5<a href="#squaring-a-2-digit-number-that-ends-with-5"></a></h2>
<p>
  I learnt this from an arithmetic book during my childhood days.  If
  the first digit of a 2-digit number is \( a \) and the second digit
  is \( 5 \) in decimal representation, then its square can then be
  written as the result of \( a \times (a + 1) \) followed by \( 25 \)
  in decimal representation, i.e. the first few digits of the square
  is given by \( a \times (a + 1) \) and the last two digits are \(
  25.  \)  Here are some examples:
</p>
<ul>
  <li>\( 25^2 = 625.  \)  (since \( 2 \times 3 = 6 \))</li>
  <li>\( 85^2 = 7225.  \)  (since \( 8 \times 9 = 72 \))</li>
</ul>
<h2 id="squaring-a-2-digit-number-that-begins-with-5">Squaring a 2-Digit Number That Begins With 5<a href="#squaring-a-2-digit-number-that-begins-with-5"></a></h2>
<p>
  After learning the previous trick, I wondered if I could make more
  such tricks for myself.  This is the first one I could come up with.
  If the first digit of a 2-digit number is \( 5 \) and the second
  digit is \( a, \) then its square can be written as the result of \(
  25 + a \) followed by \( a^2.  \)  In other words, the first two
  digits of the square are obtained from the result of \( 25 + a \)
  and the last two digits are obtained from the result of \( a^2.  \)
  Here are some examples:
</p>
<ul>
  <li>\( 52^2 = 2704.  \)  (since \( 25 + 2 = 27 \) and \( 2^2 = 4 \))</li>
  <li>\( 57^2 = 3249.  \)  (since \( 25 + 7 = 32 \) and \( 7^2 = 49 \))</li>
</ul>
<h2 id="squaring-any-number-that-ends-with-5">Squaring Any Number That Ends with 5<a href="#squaring-any-number-that-ends-with-5"></a></h2>
<p>
  Let us represent all digits except the last one as \( a, \) e.g. if
  we are given the number \( 115.  \)  We say, \( a = 11.  \)  Then we
  can express the given number algebraically as \( 10a + 5.  \)  Note
  that the square of this number is

  \[
  (10a + 5)^2 = 100a(a + 1) + 25.
  \]

  In decimal representation, this amounts to writing the result of \(
  a(a + 1) \) followed by \( 25.  \)  Here are some examples:
</p>
<ul>
  <li>\( 115^2 = 13225.  \)  (since \( 11 \times 12 = 132 \))</li>
  <li>\( 9995^2 = 99900025.  \)  (since \( 999 \times 1000 = 999000 \))</li>
</ul>
<h2 id="squaring-any-number-that-begins-with-5">Squaring Any Number That Begins With 5<a href="#squaring-any-number-that-begins-with-5"></a></h2>
<p>
  Let us represent all digits except the first one as \( a, \) e.g.
  if we are given the number \( 512, \) we say, \( a = 12.  \)  Then we
  can express the given number algebraically as \( 5 \times 10^n +
  a \) where \( n \) is the number of digits in \( a.  \)  Note that

  \[
  (5 \times 10^n + a)^2 = 25 \times 10^{2n} + 10^{n + 1} a + a^2.
  \]

  In decimal reprensetation, this amounts to performing the following
  steps:
</p>
<ol>
  <li>
    Write \( 25 \) as the first two digits.
  </li>
  <li>
    Then write \( a^2 \) as a \( 2n \)-digit number immediately after
    \( 25.  \)  Prefix \( a^2 \) with appropriate number of zeros so
    that \( a^2 \) is written with \( 2n \) digits.
  </li>
  <li>
    Write the \( + \)-sign directly below the first digit, that is,
    write the \( + \)-sign directly before the first \( 2.  \)
  </li>
  <li>
    Write every digit of \( a \) including any preceding zeros
    immediately after the \( + \)-sign.
  </li>
  <li>
    Finally add the numbers in both rows column by column performing
    the carrying operation whenever necessary.
  </li>
</ol>
<p>
  Here are some examples:

  \[
  502^2 = \\
  \left\{ \begin{array}{cccccc}
  2 &amp; 5 &amp; 0 &amp; 0 &amp; 0 &amp; 4 \\
  + &amp; 0 &amp; 2 \\
  \hline
  2 &amp; 5 &amp; 2 &amp; 0 &amp; 0 &amp; 4
  \end{array} \right\} = 252004.
  \]

  \[
  512^2 = \\
  \left\{ \begin{array}{cccccc}
  2 &amp; 5 &amp; 0 &amp; 1 &amp; 1 &amp; 4 \\
  + &amp; 1 &amp; 2 \\
  \hline
  2 &amp; 6 &amp; 2 &amp; 1 &amp; 1 &amp; 4
  \end{array} \right\} = 262114.
  \]

  \[
  564^2 = \\
  \left\{ \begin{array}{cccccc}
  2 &amp; 5 &amp; 4 &amp; 0 &amp; 9 &amp; 6 \\
  + &amp; 6 &amp; 4 \\
  \hline
  3 &amp; 1 &amp; 8 &amp; 0 &amp; 9 &amp; 6 \\
  \end{array} \right\} = 318096.
  \]
</p>
<h2 id="applying-both-tricks-together">Applying Both Tricks Together<a href="#applying-both-tricks-together"></a></h2>
<p>
  Let us now see an example where we use both the tricks together.
  Let us find \( 5195^2.  \)  This is a number that begins with the
  digit \( 5 \) as well as ends with the digit \( 5.  \)  We need to
  use the second trick to find \( 5195^2.  \)  But the second trick
  begins with writing \( 25 \) immediately followed by the result of
  \( 195^2.  \)  So we use the first trick to calculate \( 195^2.  \)
</p>
<p>
  To write the result of \( 195^2, \) we first write \( 380 \) which
  we obtain as the result of \( 19 \times 20 \) and then we write \(
  25 \) immediately after it.  Thus \( 195^2 = 38025.  \)  Now we
  perform the second trick as follows:

  \begin{align*}
  5195^2 =
  \left\{ \begin{array}{cccccccc}
  2 &amp; 5 &amp; 0 &amp; 3 &amp; 8 &amp; 0 &amp; 2 &amp; 5 \\
  + &amp; 1 &amp; 9 &amp; 5 \\
  \hline
  2 &amp; 6 &amp; 9 &amp; 8 &amp; 8 &amp; 0 &amp; 2 &amp; 5
  \end{array} \right\} = 26988025.
  \end{align*}
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/squaring-numbers-that-begin-or-end-with-5.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Clumsy Equations</title>
<link>https://susam.net/clumsy-equations.html</link>
<guid isPermaLink="false">isdgu</guid>
<pubDate>Thu, 28 Oct 2010 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="less-clumsy-equations">Less Clumsy Equation<a href="#less-clumsy-equations"></a></h2>
<p>
  At work today, while calculating optimal parameters for a data
  structure, I arrived at this equation:

  \[
    x e^{-x} + (1 - e^{-x}) \ln (1 - e^{-x}) = 0.
  \]

  This equation is a little clumsy to deal with.  Let us make this neat
  by substituting \( e^{-x} \) with \( t.  \)  Now we get

  \[
    -t \ln t + (1 - t) \ln (1 - t) = 0.
  \]

  If we rearrange this equation as

  \[
    t \ln t = (1 - t) \ln (1 - t),
  \]

  it is easy to see that one of the solutions is \( t = 1 - t, \) thus
  \( t = \frac{1}{2}.  \)  Are there more solutions to this equation?
  The answer turns out to be: <em>no</em>.
</p>
<figure class="soft">
  <img src="files/blog/clumsy-equations-1.png" alt="Graph">
  <figcaption>
    Graph of the function \( f(t) = -t \ln t + (1 - t) \ln (1 - t) \)
  </figcaption>
</figure>
<p>
  In this problem, \( x \) is a real number and so is \( t.  \)  In the
  domain of real numbers, \( \ln t \) is undefined for \( t \le 0.  \)
  As a result, \( \ln (1 - t) \) is undefined for \( t \ge 1.  \)
  Therefore we need to look for values of \( t \) in the open interval
  \( (0, 1) \) that could be solutions to the equation.
</p>
<p>
  The function \( \ln t \) is a strictly increasing function for \( t
  \gt 0.  \)  This is easy to show from the derivative of \( \ln t.  \)
  The derivative of \( \ln t \) is \( \frac{1}{t} \) which is strictly
  positive for \( t \gt 0.  \)
</p>
<p>
  Since \( \ln t \) is a strictly increasing function for \( t \gt
  0, \) we can make the following conclusions:
</p>
<ul>
  <li>
    For \( 0 \lt t \lt \frac{1}{2}, \) \( t \lt (1 - t) \implies t \ln
    t \lt (1 - t) \ln (1 - t).  \)
  </li>
  <li>
    For \( \frac{1}{2} \lt t \lt 1, \) \( t \gt (1 - t) \implies t \ln
    t \gt (1 - t) \ln (1 - t).  \)
  </li>
</ul>
<p>
  This proves that there are no values of \( t \) in the open
  intervals \( \left( 0, \frac{1}{2} \right) \) and \( \left(
  \frac{1}{2}, 1 \right) \) that could be solutions to the equation.
  Therefore \( t = \frac{1}{2} \) is the only solution to the
  equation.
</p>
<p>
  Since \( t = e^{-x}, \) we now arrive at the solution to the
  original equation expressed in terms of \( x \) as follows:

  \begin{align*}
    t = \frac{1}{2} &amp; \iff e^{-x} = \frac{1}{2} \\
                    &amp; \iff -x = \ln \frac{1}{2} \\
                    &amp; \iff x = \ln 2 \approx 0.6931.
  \end{align*}
</p>
<figure class="soft">
  <img src="files/blog/clumsy-equations-2.png" alt="Graph">
  <figcaption>
    Graph of the function
    \( f(x) = x e^{-x} + (1 - e^{-x}) \ln (1 - e^{-x}) \)
  </figcaption>
</figure>
<h2 id="more-clumsy-equations">More Clumsy Equation<a href="#more-clumsy-equations"></a></h2>
<p>
  Here is another equation that I had to solve a couple of years ago:

  \[
    \ln x - x^{0.03} = 0.
  \]

  There is no simple way to solve this equation.  I had to resort to
  using the
  <a href="https://en.wikipedia.org/wiki/Lambert_W_function">Lambert W
  function</a> to solve this.  To use this function, we first need to
  bring our equation to the form \( f(x) e^{f(x)} = Y \) where
  \( f(x) \) is a function of \( x \) and \( Y \) is an expression
  that does not contain \( x.  \)  The solution of
  \( f(x) e^{f(x)} = Y \) is \( f(x) = W(Y).  \)  To summarise:

  \[
    f(x) e^{f(x)} = Y \iff f(x) = W(Y).
  \]

  Applying this technique to our equation, we get

  \begin{align*}
    \ln x - x^{0.03} = 0
    &amp; \iff \ln x = x^{0.03} \\
    &amp; \iff \ln x = e^{0.03 \ln x} \\
    &amp; \iff (\ln x)(e^{-0.03 \ln x}) = 1 \\
    &amp; \iff (-0.03 \ln x)(e^{-0.03 \ln x}) = -0.03.  \\
  \end{align*}

  We have brought the equation to the desired \( f(x) e^{f(x)} = Y \)
  form where \( f(x) = -0.03 \ln x \) and \( Y = -0.03.  \)  Therefore
  the solution to this equation is

  \[
    -0.03 \ln x = W(-0.03).
  \]

  Simplifying this further, we get

  \[
    x = e^{\frac{W(-0.03)}{-0.03}}.
  \]

  The Maclaurin series of \( W(x) \) is given by

  \[
    W(x)
    = \sum_{n=1}^\infty \frac{(-n)^{n-1}}{n!} x^n
    = x - x^2 + \frac{3}{2} x^3 - \frac{8}{3} x^4 +
      \frac{125}{24} x^5 - \dots
  \]

  Using the Maclaurin series of \( W(x), \) we get

  \[
    W(-0.03) \approx -0.0309428.
  \]

  Therefore

  \[
    x = e^{\frac{W(-0.03)}{-0.03}}
      \approx e^{\frac{-0.0309428}{-0.03}}
      \approx 2.80506.
  \]
</p>
<figure class="soft">
  <img src="files/blog/clumsy-equations-3.png" alt="Graph">
  <figcaption>
    Graph of the function
    \( f(x) = \ln x - x^{0.03} \)
  </figcaption>
</figure>
<!-- ### -->
<p>
  <a href="https://susam.net/clumsy-equations.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Lost Solutions</title>
<link>https://susam.net/lost-solutions.html</link>
<guid isPermaLink="false">yzmto</guid>
<pubDate>Sun, 05 Sep 2010 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="algebraic-equation">Algebraic Equation<a href="#algebraic-equation"></a></h2>
<p>
  Let me start this post with a simple equation:

  \[
    x(x - 2) = 3x.
  \]

  Let us first solve it in a <em>flawed</em> manner.  Dividing both
  sides by \( x \) we get

  \[
    x - 2 = 3.
  \]

  Adding \( 2 \) to both sides we get

  \[
    x = 5.
  \]

  Indeed, \( x = 5 \) is a solution of this equation.  If we
  substitute \( x \) with \( 5 \) in the equation, we get \( 15 \) on
  both sides, so it confirms that this solution holds good.
</p>
<p>
  However, \( x = 0 \) is also a solution of this equation.  If we
  substitute \( x \) with \( 0 \) in the equation, we get \( 0 \) on
  both sides.  But we did not get this solution when we solved the
  equation in the flawed manner above.  Why did we lose this solution?
</p>
<p>
  We lost this solution in the step where we divided both sides by \(
  x.  \)  In doing so, we made the wrong assumption that
  \( x \ne 0.  \)  If we want to consider the possibility of
  \( x = 0 \) as a solution, then we cannot divide both sides of the
  equation by \( x.  \)  Division by \( 0 \) is undefined in
  arithmetic.  This is why the typical way to solve such an equation
  is to bring all terms involving \( x \) to one side of the equation
  and then solve it.  For example,

  \begin{align*}
    x(x - 2) = 3x
    &amp; \iff x(x - 2) - 3x = 0 \\
    &amp; \iff x^2 - 2x - 3x = 0 \\
    &amp; \iff x(x - 5) = 0 \\
    &amp; \iff x \in \{ 0, 5 \}.
  \end{align*}
</p>
<h2 id="mathematical-fallacy">Mathematical Fallacy<a href="#mathematical-fallacy"></a></h2>
<p>
  Often fallacies or spurious proofs of contradictions is formed by
  sneaking such a division by \( 0 \) into the proof.  Here is one I
  such spurious proof:
</p>
<p>
  Let \( a = b.  \)  Then

  \begin{align*}
    a + b = 2a
    &amp; \iff a + b - 2b = 2a - 2b \\
    &amp; \iff a - b = 2(a - b).
  \end{align*}

  Dividing both sides by \( a - b, \) we get

  \[
    1 = 2.
  \]

  We arrived at this contradiction because we incorrectly divided both
  sides of the equation by \( a - b \) even though \( a - b = 0.  \)
</p>
<h2 id="differential-equation">Differential Equation<a href="#differential-equation"></a></h2>
<p>
  We need to be careful about division by zero while solving
  differential equations too.
</p>
<p>
  Consider the differential equation

  \[
    y' = x^2 y^2.
  \]

  Dividing both sides by \( y^2, \) the variables separate and we get

  \[
    y^{-2} y' = x^2.
  \]

  Integrating both sides with respect to \( x, \) we get

  \[
    -y^{-1} = \frac{x^3}{3} + c
  \]

  where \( c \) is the constant of integration.  Taking the negative
  reciprocal of both sides, we get

  \[
    y = \frac{-3}{x^3 + 3c} = \frac{-3}{x^3 + d}.
  \]

  where \( d = 3c.  \)
</p>
<p>
  This is indeed a valid solution of the differential equation.
  However, we have lost a solution.  The lost solution is
  \( y = 0.  \)  We can verify that with \( y = 0, \) we get \( 0 \) on
  both sides of the differential equation.  We lost this solution by
  dividing both sides of the differential equation by \( y^2.  \)
</p>
<p>
  Whenever we need to divide both sides of a differential equation by
  a function to separate the variables, we also need to perform an
  additional step of verifying whether the zero function is a solution
  of the equation.  In this case \( y = 0 \) is indeed a solution of
  the differential equation.
</p>
<p>
  In general, when we need to divide both sides of the equation by a
  function of \( y, \) say \( f(y), \) to separate the variables, we
  need to perform an additional step of verifying whether
  \( f(y) = 0 \) is a solution of the differential equation.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/lost-solutions.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>Coefficient of Variation Function in PostgreSQL</title>
<link>https://susam.net/coefficient-of-variation-function-in-postgresql.html</link>
<guid isPermaLink="false">doyiz</guid>
<pubDate>Thu, 13 May 2010 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Today I learnt how to create an aggregate function in PostgreSQL
  function that wraps the functionality provided by other aggregate
  functions in PosgreSQL.  In this experiment, I created
  a <code>cv()</code> function that calculates the coefficient of
  variation.  The function <code>cv(x)</code> is equivalent
  to <code>stddev(x)</code> / <code>avg(x)</code> where <code>x</code>
  represents the list of data points.
</p>
<h2 id="example-data-table">Example Data Table<a href="#example-data-table"></a></h2>
<pre>
<samp>$ <kbd>cat perf.sql</kbd>
CREATE TABLE performance
(
    name VARCHAR,
    duration DOUBLE PRECISION
);

INSERT INTO performance VALUES ('RAND', 101.0);
INSERT INTO performance VALUES ('ZERO', 157.0);
INSERT INTO performance VALUES ('NONE', 209.0);
INSERT INTO performance VALUES ('TEST', 176.0);
INSERT INTO performance VALUES ('UNIT', 197.0);
INSERT INTO performance VALUES ('LOAD', 193.0);
INSERT INTO performance VALUES ('FREE', 198.0);
$ <kbd>psql statistics</kbd>
psql (8.4.3)
Type "help" for help.
statistics=# <kbd>\i perf.sql</kbd>
DROP TABLE
CREATE TABLE
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
INSERT 0 1
statistics=# <kbd>select * from performance;</kbd>
 name | duration
------+----------
 RAND |      101
 ZERO |      157
 NONE |      209
 TEST |      176
 UNIT |      197
 LOAD |      193
 FREE |      198
(7 rows)

statistics=#</samp>
</pre>
<h2 id="useful-details-to-create-our-function">Useful Details to Create Our Function<a href="#useful-details-to-create-our-function"></a></h2>
<pre>
<samp>statistics=# <kbd>SELECT aggtransfn, aggfinalfn, aggtranstype::regtype, agginitval</kbd>
statistics-# <kbd>FROM pg_aggregate</kbd>
statistics-# <kbd>WHERE aggfnoid='stddev(double precision)'::regprocedure;</kbd>
  aggtransfn  |     aggfinalfn     |    aggtranstype    | agginitval
--------------+--------------------+--------------------+------------
 float8_accum | float8_stddev_samp | double precision[] | {0,0,0}
(1 row)

statistics=# <kbd>SELECT aggtransfn, aggfinalfn, aggtranstype::regtype, agginitval</kbd>
statistics-# <kbd>FROM pg_aggregate</kbd>
statistics-# <kbd>WHERE aggfnoid='avg(double precision)'::regprocedure;</kbd>
  aggtransfn  | aggfinalfn |    aggtranstype    | agginitval
--------------+------------+--------------------+------------
 float8_accum | float8_avg | double precision[] | {0,0,0}
(1 row)

statistics=#</samp>
</pre>
<h2 id="function-definition">Function Definition<a href="#function-definition"></a></h2>
<pre>
<samp>$ <kbd>cat cv.sql</kbd>
CREATE OR REPLACE FUNCTION finalcv(double precision[])
RETURNS double precision
AS $$
    SELECT float8_stddev_samp($1) / float8_avg($1);
$$ LANGUAGE SQL;

CREATE AGGREGATE cv(double precision)
(
    sfunc = float8_accum,
    stype = double precision[],
    finalfunc = finalcv,
    initcond = '{0, 0, 0}'
);</samp>
</pre>
<h2 id="usage">Usage<a href="#usage"></a></h2>
<pre>
<samp>$ <kbd>psql statistics</kbd>
psql (8.4.3)
Type "help" for help.

statistics=# <kbd>select stddev(duration), avg(duration) from performance;</kbd>
      stddev      |       avg
------------------+------------------
 37.1682147873178 | 175.857142857143
(1 row)

statistics=# <kbd>select stddev(duration) / avg(duration) as cv from performance;</kbd>
        cv
-------------------
 0.211354592616754
(1 row)

statistics=# <kbd>\i cv.sql</kbd>
CREATE FUNCTION
CREATE AGGREGATE
statistics=# <kbd>select cv(duration) from performance;</kbd>
        cv
-------------------
 0.211354592616754
(1 row)

statistics=#</samp>
</pre>
<h2 id="bessel-correction">Bessel's Correction<a href="#bessel-correction"></a></h2>
<p>
  Checked whether
  <a href="http://en.wikipedia.org/wiki/Bessel's_correction">Bessel's
  correction</a> was used in the <code>stddev()</code> function of
  PostgreSQL.  Yes, it was used.
</p>
<pre>
<samp>$ <kbd>octave -q</kbd>
octave:1&gt; <kbd>std([101, 157, 209, 176, 197, 193, 198], 0)</kbd>
ans =  37.168
octave:2&gt; <kbd>std([101, 157, 209, 176, 197, 193, 198], 1)</kbd>
ans =  34.411
octave:3&gt;</samp>
</pre>
<p>
  The <code>std()</code> function in MATLAB and GNU Octave applies
  Bessel's correction when invoked with the second argument
  as <code>0</code>.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/coefficient-of-variation-function-in-postgresql.html">Read on website</a> |
  <a href="https://susam.net/tag/sql.html">#sql</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a>
</p>
]]>
</description>
</item>
<item>
<title>Combinatorics Coincidence</title>
<link>https://susam.net/combinatorics-coincidence.html</link>
<guid isPermaLink="false">gfhlc</guid>
<pubDate>Sun, 14 Sep 2008 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<h2 id="combinatorics-for-fun">Combinatorics for Fun<a href="#combinatorics-for-fun"></a></h2>
<p>
  At my current workplace, there are several engineers who have an
  affinity for combinatorics.  As a result, discussions about
  combinatorics problems often occur at the cafeteria.  Probability
  theory is another popular topic of discussion.  Of course, often
  combinatorics and probability theory go hand in hand.
</p>
<h2 id="recurrence-relation">Recurrence Relation<a href="#recurrence-relation"></a></h2>
<p>
  At the cafeteria one day, I joined in on a conversation about
  combinatorics problems.  During the conversation, I happened to
  share the following problem:
</p>
<div class="highlight">
  <p>
    For integers \( n \ge 1 \) and \( k \ge 1, \)

    \[
      f_k(n) =
      \begin{cases}
        n                       &amp; \text{if } k = 1, \\
        \sum_{i=1}^n f_{k-1}(i) &amp; \text{if } k \ge 2.
      \end{cases}
    \]

    Find a closed-form expression for \( f_k(n).  \)
  </p>
</div>
<h2 id="nested-loops">Nested Loops<a href="#nested-loops"></a></h2>
<p>
  Soon after I shared the above problem, a colleague of mine shared
  this problem with me:
</p>
<div class="highlight">
  <p>
    Consider the following pseudocode with <code>k</code> nested
    loops:
  </p>
<pre>
<code>x = 0
for c<sub>1</sub> in 0 to (n - 1):
    for c<sub>2</sub> in 0 to (c<sub>1</sub> - 1):
        ...
            for c<sub>k</sub> in 0 to (c<sub>k-1</sub> - 1):
                x = x + 1</code>
</pre>
  <p>
    What is the final value of <code>x</code> after the outermost loop
    terminates?
  </p>
</div>
<h2 id="coincidence">Coincidence<a href="#coincidence"></a></h2>
<p>
  With one problem each, we went back to our desks.  As I began
  solving the <em>nested loops</em> problem shared by my colleague, I
  realised that the solution to his problem led me to
  the <em>recurrence relation</em> in the problem I shared with him.
</p>
<p>
  In the <em>nested loops</em> problem, if \( k = 1, \) the final
  value of \( x \) after the loop terminates is \( x = n.  \)  This is
  also the value of \( f_1(n).  \)
</p>
<p>
  If \( k = 2, \) the inner loop with counter \( c_2 \) runs once when
  \( c_1 = 0, \) twice when \( c_1 = 1 \) and so on.  When the loop
  terminates, \( x = 1 + 2 + \dots + n.  \)  Note that this series is
  same as \( f_2(n) = f_1(1) + f_1(2) + \dots + f_1(n).  \)
</p>
<p>
  Extending this argument, we now see that for any \( k \ge 1, \) the
  final value of \( x \) is

  \[
    f_k(n) = f_{k-1}(1) + f_{k-1}(2) + \dots + f_{k-1}(n).
  \]

  In other words, the solution to his <em>nested loops</em> problem is
  the solution to my <em>recurrence relation</em> problem.  It was an
  interesting coincidence that the problems we shared with each other
  had the same solution.
</p>
<h2 id="closed-form-expression">Closed-Form Expression<a href="#closed-form-expression"></a></h2>
<p>
  The closed form expression for the recurrence relation is

  \[
    f_k(n) = \binom{n + k - 1}{k}.
  \]

  It is quite easy to prove this using the principle of mathematical
  induction.  Since we know that this is also the result of
  the <em>nested loops</em> problem, we can also arrive at this result
  by another way.
</p>
<p>
  In the <em>nested loops</em> problem, the following inequalities are
  always met due to the loop conditions:

  \[
    n - 1 \ge c_1 \ge c_2 \ge \dots \ge c_k \ge 0.
  \]

  The variables \( c_1, c_2, \dots, c_k \) take all possible
  arrangements of integer values that satisfy the above inequalities.
  If we find out how many such arrangements are there, we will know
  how many times the variable \( x \) is incremented.
</p>
<p>
  Let us consider \( n - 1 \) similar balls and \( k \) similar
  sticks.  For every possible permutation of these balls and sticks,
  if we count the number of balls to the right of the \( i \)th stick
  where \( 1 \le i \le k, \) we get a number that the variable \( c_i \)
  holds in some iteration of the \( i \)th loop.  Therefore the
  variable \( c_i \) is represented as the number of balls lying on
  the right side of the \( i \)th stick.
</p>
<p>
  The above argument holds good because the number of balls on the
  right side of the first stick does not exceed \( n - 1, \) the
  number of balls on the right side of the second stick does not
  exceed the number of balls on the right side of the first stick and
  so on.  Thus the inequalities mentioned earlier are always
  satisfied.  Also, any set of valid values for \( c_1, c_2, \dots,
  c_k \) can be represented as an arrangement of these sticks and
  balls.
</p>
<p>
  The number of permutations of \( n - 1 \) similar balls and \( k \)
  similar sticks is

  \[
    \frac{(n + k - 1)!}{(n - 1)! \, k!} = \binom{n + k - 1}{k}.
  \]

  This closed-form expression is the solution to both the
  <em>recurrence relation</em> problem and the <em>nested loops</em>
  problem.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/combinatorics-coincidence.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/combinatorics.html">#combinatorics</a> |
  <a href="https://susam.net/tag/puzzle.html">#puzzle</a>
</p>
]]>
</description>
</item>
<item>
<title>From Perl to Pi</title>
<link>https://susam.net/from-perl-to-pi.html</link>
<guid isPermaLink="false">jelap</guid>
<pubDate>Tue, 15 Apr 2008 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  I was learning Perl last weekend from the book <em>Learning
  Perl</em>, 3rd Edition by Randal L. Schwartz and Tom Phoenix.  While
  reading the book, I came across these lines:
</p>
<blockquote>
  It's easier to type <code>$pi</code> than \( \pi, \) especially if
  you don't have Unicode.  And it will be easy to maintain the program
  in case the value of \( \pi \) ever changes.<sup>379</sup>
</blockquote>
<p>
  The corresponding footnote mentions:
</p>
<blockquote>
  <em>
    <sup>379</sup> It nearly did change by a legislative act in the
    state of Indiana.
    <a href="http://www.urbanlegends.com/legal/pi_indiana.htm">http://www.urbanlegends.com/legal/pi_indiana.htm</a>
  </em>
</blockquote>
<p>
  I searched the web and found that the original urbanlegends.com
  website is no longer there.  However while searching for it, I came
  across this brilliant piece of humour archived in the article titled
  <a href="https://www.snopes.com/fact-check/alabamas-slice-of-pi/">Alabama's
  Slice of Pi</a> on Snopes.  It is a fictitious report on a state
  legislature redefining the value of \( \pi \) to 3.
</p>
<p>
  The Snopes article mentions that this piece of humour was first
  posted in a newsgroup.  Then people started circulating it as hoax
  from there.  Here are some of the intriguing and funny bits from it:
</p>
<blockquote>
  Lawson called into question the usefulness of any number that cannot
  be calculated exactly, and suggested that never knowing the exact
  answer could harm students' self-esteem.
</blockquote>
<blockquote>
  Scientists have arbitrarily assumed that space is Euclidean, he
  says.  He points out that a circle drawn on a spherical surface has
  a different value for the ratio of circumfence to diameter.
</blockquote>
<p>
  In fact, with a little geometry we can see that if a
  <a href="https://en.wikipedia.org/wiki/Flatland">flatlander</a>
  living on a globe with diameter \( D \) draws a circle of diameter
  \( d \) assuming that he is on a flat surface, then the ratio of the
  circumference \( c \) to the diameter \( d \) is

  \[
    \frac{c}{d} = \frac{\pi D}{d} \sin{\frac{d}{D}}.
  \]

  Here are a few more excerpts from the Snopes article:
</p>
<blockquote>
  "These nabobs waltzed into the capital with an arrogance that was
  breathtaking," Learned said.  "Their prefatorial deficit resulted in
  a polemical stance at absolute contraposition to the legislature's
  puissance."
</blockquote>
<blockquote>
  One member of the state school board, Lily Ponja, is anxious to get
  the new value of pi into the state's math textbooks, but thinks that
  the old value should be retained as an alternative.  She said, "As
  far as I am concerned, the value of pi is only a theory, and we
  should be open to all interpretations."  She looks forward to
  students having the freedom to decide for themselves what value pi
  should have.
</blockquote>
<p>
  By the way, the real event that the footnote in the
  <em>Learning Perl</em> book mentioned is
  the <a href="http://en.wikipedia.org/wiki/Indiana_Pi_Bill">Indiana
  Pi Bill</a>.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/from-perl-to-pi.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a> |
  <a href="https://susam.net/tag/book.html">#book</a> |
  <a href="https://susam.net/tag/programming.html">#programming</a> |
  <a href="https://susam.net/tag/technology.html">#technology</a> |
  <a href="https://susam.net/tag/quote.html">#quote</a>
</p>
]]>
</description>
</item>
<item>
<title>Irrational Base</title>
<link>https://susam.net/irrational-base.html</link>
<guid isPermaLink="false">hphlo</guid>
<pubDate>Tue, 11 Sep 2007 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  Yesterday, I was going through
  <a href="https://googleblog.blogspot.com/2007/09/our-plans-for-code-jam.html">Google's
  blog on their plans for Code Jam</a>.  I found an error in a
  document linked to from their blog post.  I wrote an email to inform
  them about it.  A few hours later, I got an amusing reply from them.
  Read the conversation below.
</p>
<div class="box">
  <p>
    <strong>From:</strong> "Susam Pal"<br>
    <strong>To:</strong> Programmingcontest-feedback [at] google [dot] com<br>
    <strong>Date:</strong> Tue, Sep 11, 2007, 11:45 AM IST<br>
    <strong>Subject:</strong> Sample output of Problem A probably incorrect
  </p>
  <p>
    I was going through this PDF file:
    <a href="http://services.google.com/blog_resources/Google_CodeJam_Practice.pdf">http://services.google.com/blog_resources/Google_CodeJam_Practice.pdf</a>.
  </p>
  <p>
    Shouldn't the sample output for Problem A be 17 &times; 3 = 51?
    The sample output is mentioned as 120 which is incorrect.  Please
    let me know what you feel.
  </p>
  <p>
    Regards,<br>
    Susam Pal
  </p>
</div>
<div class="box">
  <p>
    <strong>From:</strong> "Bartholomew Furrow"<br>
    <strong>To:</strong> "Susam Pal"<br>
    <strong>Cc:</strong> Programmingcontest-feedback [at] google [dot] com<br>
    <strong>Date:</strong> Tue, Sep 11, 2007, 11:29 PM IST<br>
    <strong>Subject:</strong> Re: Sample output of Problem A probably incorrect
  </p>
  <p>
    Susam,
  </p>
  <p>
    If you write the answer in base 6.21110255, I think you'll find
    that 120 is correct.
  </p>
  <p>
    You're quite right: that was the answer to another example (one
    that we did not use).  We will push a fix shortly to the PDF, and
    we will have a much more careful review process for actual
    competitions.
  </p>
  <p>
    Thanks for your careful attention,<br>
    Bartholomew
  </p>
</div>
<div class="box">
<p>
  <strong>From:</strong> "Susam Pal"<br>
  <strong>To:</strong> "Bartholomew Furrow"<br>
  <strong>Cc:</strong> Programmingcontest-feedback [at] google [dot] com<br>
  <strong>Date:</strong> Wed, Sep 12, 2007, 12:04 AM IST<br>
  <strong>Subject:</strong> Re: Sample output of Problem A probably incorrect
</p>
<p>
  Thanks for the quick response.
</p>
<p>
  &gt; If you write the answer in base 6.21110255, I think you'll find that
  &gt; 120 is correct.
</p>
<p>
  :-) Or base -8.2111025509.  :-P
</p>
<p>
  Regards,<br>
  Susam Pal
</p>
</div>
<p>
  If you are wondering, how weird the numeral system must be in an
  irrational base or negative base, here's a surprise: Donald Knuth
  has worked on transcendental base and imaginary base too.
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/irrational-base.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>
<item>
<title>0.999...</title>
<link>https://susam.net/one.html</link>
<guid isPermaLink="false">rfdyx</guid>
<pubDate>Sun, 09 Dec 2001 00:00:00 +0000</pubDate>
<description>
<![CDATA[
<p>
  What is the difference between \( 0.999\ldots \) and \( 1?  \)  Note
  that the ellipsis denotes a never-ending sequence of 9s.  Is the
  difference a small but positive real number?  If so, what is that
  number?  Or is the difference simply \( 0?  \)  It turns out that the
  difference is indeed \( 0.  \)  In fact, we can write

  \[
    1 - 0.999\ldots = 0.
  \]

  Intuitively, it may feel like the number \( 0.999\ldots \) gets
  closer and closer to \( 1 \) without ever reaching \( 1.  \)  This
  intuition is common.  After all, adding each new digit brings the
  number closer to 1.  There is a precise way to define such
  behaviour, using the concept of limits.  When we do so, we will find
  that

  \[
    0.999\ldots = 1.
  \]

  Both numbers are exactly equal and their difference is exactly
  \( 0.  \)
</p>
<h2 id="contents">Contents<a href="#contents"></a></h2>
<ul>
  <li><a href="#algebraic-illustration">Algebraic Illustration</a></li>
  <li><a href="#another-algebraic-illustration">Another Algebraic Illustration</a></li>
  <li><a href="#three-equalities">Three Equalities</a>
    <ul>
      <li><a href="#sum-of-fractions">Sum of Fractions</a></li>
      <li><a href="#limiting-sum">Limiting Sum</a></li>
      <li><a href="#computing-the-limit">Computing the Limit</a></li>
    </ul>
  </li>
  <li><a href="#tying-it-together">Tying It Together</a></li>
  <li><a href="#conclusion">Conclusion</a></li>
</ul>
<h2 id="algebraic-illustration">Algebraic Illustration<a href="#algebraic-illustration"></a></h2>
<p>
  Before we do a formal analysis, it might be worthwhile demonstrating
  the equality above algebraically.  Let us first write

  \[
    0.333\ldots = \frac{1}{3}.
  \]

  Multiplying both sides by \( 3, \) we obtain

  \[
    0.999\ldots = 1.
  \]

  That was quite straightforward, though not rigorous.  We will
  return to this point shortly.
</p>
<h2 id="another-algebraic-illustration">Another Algebraic Illustration<a href="#another-algebraic-illustration"></a></h2>
<p>
  Let us see another similar demonstration.  This time we follow a
  popular method for finding a fraction that represents a repeating
  decimal number.  Let

  \[
    x = 0.999\ldots
  \]

  Multiplying both sides by \( 10 \) gives us

  \[
    10x = 9.999\ldots
  \]

  Subtracting \( x = 0.999\ldots \) from both sides, we get

  \[
    9x = 9.
  \]

  Dividing both sides by \( 9 \) gives us

  \[
    x = 1.
  \]

  Since \( x = 0.999\ldots, \) the above equation is equivalent to

  \[
    0.999\ldots = 1.
  \]
</p>
<p>
  It is important to note that these algebraic demonstrations do not
  constitute rigorous proofs because we have not shown that the
  elementary rules for addition and multiplication extend to repeating
  decimal numbers too.  They do, but we have not demonstrated that
  here.  Nonetheless, they provide a useful way to challenge any
  faulty intuition that might have led one to doubt the above
  equality.  While there are many ways to prove this equality, one
  easy way is to rely on the formula for the infinite geometric
  series, which has a sound basis in real analysis.
</p>
<h2 id="three-equalities">Three Equalities<a href="#three-equalities"></a></h2>
<p>
  We now prove that \( 0.999\ldots = 1 \) using the following sequence
  of equalities:

  \[
    0.999\ldots
    = \frac{9}{10} + \frac{9}{100} + \frac{9}{1000} + \dots
    = \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k}
    = 1.
  \]

  Now let us take a close look at each equality.
</p>
<h3 id="sum-of-fractions">Sum of Fractions<a href="#sum-of-fractions"></a></h3>
<p>
  The first equality is in fact the definition of the number \(
  0.999\ldots.  \)  Every real number has a decimal expansion, which
  can be expressed as a finite or infinite sum of fractions.  The
  number \( 0.999\ldots, \) when expressed as a sum of fractions, can
  be written as

  \begin{equation}
    0.999\ldots
    = \frac{9}{10} + \frac{9}{100} + \frac{9}{1000} + \dots
    \label{eq-fractions}
  \end{equation}

  The left-hand side (LHS) and the right-hand side (RHS) represent the
  same real number, expressed in two different notations.  The LHS
  uses decimal notation, while the RHS uses series notation.
</p>
<h3 id="limiting-sum">Limiting Sum<a href="#limiting-sum"></a></h3>
<p>
  The RHS of the equation above appears to involve the sum of an
  infinite number of terms.  But we cannot literally add an infinite
  number of terms together, can we?  <em>Infinity is not a
  number.</em>  What does the RHS mean then?
</p>
<p>
  Let \( (a_n) \) denote a sequence of real numbers \( a_1, a_2,
  \dots.  \)  We say that the limit of the sequence \( (a_n) \) is \(
  L \) and write

  \[
    \lim_{n \to \infty} a_n = L
  \]

  if, for every real number \( \epsilon \gt 0, \) there exists a
  positive integer \( N \) such that for all \( n \ge N, \) we have \(
  \lvert a_n - L \rvert \lt \epsilon.  \)  In symbolic form, this
  definition can be expressed as:

  \[
    \forall \varepsilon \gt 0, \;
      \exists N \in \mathbb{N} \;
      \text{such that} \;
      \forall n \ge N, \;
      \lvert a_n - L \rvert
      \lt \varepsilon.
  \]

  In simple terms, saying that the limit of the sequence \( (a_n) \)
  converges to a limit \( L \) means that no matter how small a
  positive real number we choose for \( \epsilon, \) we can find a
  large enough threshold for \( n \) such that for all values of
  \( n \) equal to or above that threshold, the difference between \(
  a_n \) and \( L \) is smaller than \( \epsilon.  \)  In other words,
  the difference can be made as small as we like simply by considering
  terms that are farther and farther out.
</p>
<p>
  Now consider the sequence

  \[
    \frac{9}{10}, \quad
    \frac{9}{10} + \frac{9}{10^2}, \quad
    \frac{9}{10} + \frac{9}{10^2} + \frac{9}{10^3}, \quad
    \dots
  \]

  The \( n \)th term has the form

  \[
    s_n = \sum_{k=1}^n \frac{9}{10^k}.
  \]

  where \( n \) is a positive integer.  We say that

  \[
    \frac{9}{10} + \frac{9}{10^2} + \frac{9}{10^3} + \dots
    = \lim_{n \to \infty} s_n.
  \]

  Or if we write the expansion of the RHS, then

  \begin{equation}
    \frac{9}{10} + \frac{9}{10^2} + \frac{9}{10^3} + \dots
    = \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k}.
    \label{eq-limit-sum}
  \end{equation}

  Again, this is a definition.  It does not feel like we are proving
  anything.  We are merely saying one thing is the same as another
  thing because we named them so.  It may not feel satisfying at
  first, but definitions lay the groundwork for meaningful results.
  After all, to get the result \( 1 + 1 = 2, \) we first need to
  define \( 2 \) as a successor of \( 1.  \)  We must assign a meaning
  to \( 2 \) before we can get any interesting results about it.
</p>
<p>
  Similarly, we must assign a meaning to \( 0.999\ldots \) before we
  can find interesting results about it.  Equation \(
  \eqnref{eq-fractions}{1} \) in the previous section is how we choose
  to define \( 0.999\ldots.  \)  If you choose to define this number
  differently, in a way that is inconsistent with the definition here,
  that's fine, but then you and I will be working with different
  mathematical systems.  The onus will be on you to demonstrate that
  your mathematical system is useful and consistent.  In this article,
  however, we will work with the above definitions, which are widely
  accepted and known to be internally consistent.
</p>
<h3 id="computing-the-limit">Computing the Limit<a href="#computing-the-limit"></a></h3>
<p>
  We now evaluate the RHS of equation \( \eqnref{eq-limit-sum}{2}.  \)
  It is an infinite geometric series.  From real analysis, we know
  that for real numbers \( a \) and \( r \) with \( \lvert r \rvert
  \lt 1, \)

  \[
    \lim_{n \to \infty} \sum_{k=0}^n ar^k = \frac{a}{1 - r}
  \]

  Substituting \( a = 9/10 \) and \( r = 1/10, \) we obtain

  \[
    \lim_{n \to \infty}
    \sum_{k=0}^n \left(
      \frac{9}{10} \cdot \frac{1}{10^k}
    \right)
    = \frac{\frac{9}{10}}{1 - \frac{1}{10}}.
  \]

  Simplifying both sides, we get

  \begin{equation}
    \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k} = 1.
    \label{eq-limit-value}
  \end{equation}

  We have not proven the formula for geometric series here.  The point
  of this article is not to establish the basic theorems of real
  analysis from scratch, but instead to use them to explore what \(
  0.999\ldots \) is.  Relying on the geometric series formula gives us
  a convenient starting point without compromising on rigour.  The
  analytic proof of the formula depends on the convergence of the
  geometric series when \( \lvert r \rvert \lt 1.  \)  The proof of
  convergence can be found in any introductory book on real analysis.
</p>
<p>
  This limit tells us that no matter how small a positive real number
  we choose for \( \epsilon, \) we can find a large enough threshold
  \( N \) such that for all values of \( n \ge N, \) the difference
  between \( \sum_{k=1}^n \frac{9}{10^k} \) and \( 1 \) is smaller
  than \( \epsilon.  \)  In other words, the difference can be made as
  small as we like simply by adding sufficiently many terms.
</p>
<h2 id="tying-it-together">Tying It Together<a href="#tying-it-together"></a></h2>
<p>
  Equations \( \eqnref{eq-fractions}{1}, \) \(
  \eqnref{eq-limit-sum}{2} \) and \( \eqnref{eq-limit-value}{3} \) can
  be summarised as follows:

  \[
    0.999\ldots
    = \frac{9}{10} + \frac{9}{100} + \frac{9}{1000} + \dots
    = \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k}
    = 1.
  \]

  The first two equalities follow from definitions; the last follows
  from real analysis.  If this still seems unconvincing, we must ask:
  what is the difference between \( 0.999\ldots \) and \( 1?  \)  It
  must be either zero or non-zero.  Suppose the difference is
  non-zero.  Let

  \[
    \lvert 0.999\ldots - 1 \rvert = \delta
  \]

  where \( \delta \) is a positive real number.  Therefore

  \begin{equation}
    \left\lvert
      \left(
        \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k}
      \right) - 1
    \right\rvert = \delta.
    \label{eq-delta}
  \end{equation}

  But we have already shown that

  \[
    \lim_{n \to \infty} \sum_{k=1}^n \frac{9}{10^k} = 1
  \]

  By the definition of limit, it means that no matter how small a
  positive real number we choose for \( \epsilon, \) there is a
  positive integer \( N \) such that for all \( n \ge N, \) we have

  \[
    \left\lvert
      \left(
        \sum_{k=1}^n \frac{9}{10^k}
      \right) - 1
    \right\rvert
    \lt \epsilon.
  \]

  So if we choose \( \epsilon = \delta \) (the very number we assumed
  was the difference between \( 0.999\ldots \) and \( 1 \)), we arrive
  at

  \[
    \left\lvert
      \left(
        \sum_{k=1}^n \frac{9}{10^k}
      \right) - 1
    \right\rvert
    \lt \delta.
  \]

  for all \( n \ge N.  \)  This contradicts \( \eqnref{eq-delta}{4}.  \)
  Therefore the difference between \( 0.999\ldots \) and \( 1 \)
  cannot be non-zero.  So the only possibility is that the difference
  is zero, meaning \( 0.999\ldots = 1.  \)
</p>
<h2 id="conclusion">Conclusion<a href="#conclusion"></a></h2>
<p>
  We have examined the equality \( 0.999\ldots = 1 \) from several
  perspectives: informal algebraic arguments, the geometric series
  formula and the rigour of limits in real analysis.  While the idea
  may initially seem counterintuitive, each approach confirms that \(
  0.999\ldots \) is exactly equal to \( 1.  \)  The difference between
  them is not a small number.  It is precisely zero.  This example is
  a useful reminder that initial mathematical intuition may need to be
  revised through formal reasoning.  Once that happens, the insight
  gained often becomes new intuition!
</p>
<!-- ### -->
<p>
  <a href="https://susam.net/one.html">Read on website</a> |
  <a href="https://susam.net/tag/mathematics.html">#mathematics</a>
</p>
]]>
</description>
</item>


</channel>
</rss>
